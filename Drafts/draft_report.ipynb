{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a54c8f7-aa1f-4305-bbf3-62fb05b75fbc",
   "metadata": {},
   "source": [
    "# A LYRICAL EVOLUTION: \n",
    "\n",
    "### An Investigation of the Cultural Lexicon & Historical Relevance of U.S. Popular Music from 1958 - Present"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49eb0c4-37d7-45cc-979b-57baaf3d0d2e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c102d20-adbd-44b0-a0fc-0062cacb04eb",
   "metadata": {},
   "source": [
    "**An NLP based Capstone Project & Final Report Created By:**\n",
    "\n",
    "Ben Smith, Chris Teceno, Jerry Nolf & Rachel Robbins-Mayhill\n",
    "Codeup   |   Innis Cohort   |   June 2022  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0330cf87-3d96-49e2-93f9-f638738c3096",
   "metadata": {},
   "source": [
    "<img src=\"dataset-cover.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819a8499-ba91-4a83-89c6-67dd43412f15",
   "metadata": {},
   "source": [
    "## Project Goal\n",
    "This project aimed to investigate the patterns of song lyrics across decades using Time Series Analysis and Natural Language Processing techniques including Topic Modeling, Sentiment Analysis, and Term Frequency.  The data used was collected from a Kaggle data set of the Billboard Top 100 Songs from 1958 to 2021 and lyrics pulled through web-scraping from the Genius.com API. We believe the lyrics of popular songs could be used for historical analysis using exploratory methods and hypothesis testing to identify changing societal trends in relationships, technology, sexuality, and vulgarity. Furthermore, we believe we can predict the decade the song first appeared on the Top 100 using features and machine learning methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae16ef11-eaa7-47c1-a858-84593c0dfa32",
   "metadata": {},
   "source": [
    "## Project Description\n",
    "\n",
    "Songs are powerful tokens: they can soothe, validate, ignite, confront, and educate us – among other things. Like time capsules, they are captured for eternity. The slang and language used are often indicative of the times, and you can probably recall exactly when a song was made based on what is mentioned. Arguably, music is a catalyst for societal and cultural evolution like no other art form. It has been causing controversy and societal upheaval for decades, and it seems with every generation there’s a new musical trend that has the older generations shaking their heads. \n",
    "\n",
    "For centuries, songs have been passed down through generations, being sung as oral histories. However, with advancements of the 20th century, technology has made the world of music a much smaller place and, thanks to cheap, widely-available audio equipment, songs are now distributed on a much larger scale, having a farther-reaching impact, and a more permanent place in history. \n",
    "\n",
    "This project aimed to combine the record of lyrical history and technological advancements to evaluate the changes in the cultural lexicon and societal evolution over the last 50+ years. Using machine learning and natural language processing methodologies we investigated the topics prevalent in songs of the past, predicted the decade in which they were written, and conducted historical analysis through exploration to identify changing societal trends in relationships, technology, sexuality, and vulgarity.\n",
    "\n",
    "<img src='Billboard.png' width=\"350\" height=\"350\" align=\"left\"/> To do this, we acquired a [Kaggle](https://www.kaggle.com/datasets/dhruvildave/billboard-the-hot-100-songs) data set of the Billboard Top 100 Songs from its inception in 1958 to present. We then utilized the [Genius.com](https://genius.com/) API and LyricGenius Library to conduct web scraping to pull the lyrics for the specified songs which became the corpus for this project. After acquiring and preparing the corpus, our team conducted natural language processing exploration utilizing methods such as topic modeling, word clouds, and bigrams. We employed multiclass classification methods to create multiple machine learning models. The end goal was to create an NLP model that accurately predicted the decade a song first appeared on the Billboard Top 100 chart, based on the words and word combinations found in the lyrics of the song.\n",
    "\n",
    "We choose the Billboard Hot 100 song list as a focus because it is the music industry standard record chart in the United States for song popularity, published weekly by Billboard magazine. It provides a window into popular culture at a given time, by providing chart rankings of songs that were trending on sales, airplay, and now streaming for that week in the United States. It is arguably the best historical record of the impact of specific popular songs over time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd55c6df-51d2-412e-adce-1daa9ca98119",
   "metadata": {},
   "source": [
    "## Initial Thoughts & Hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe434ce1-b1f0-4415-8403-23081cd5e590",
   "metadata": {},
   "source": [
    "The initial hypothesis of this project was that we could use the top songs of each decade in conjunction with topic modeling to identify unique words or topics which could be used as features to accurately predict the decade a song was on the Billboard Top 100 using machine learning. The thought behind this was that popular songs have been the historians of a unique lexicon, specific to their place in time. We believe the lyrics of popular songs could be analyzed through machine learning to identify societal trends in relationships, technology, sexuality, and vulgarity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c142d940-b385-4db4-b002-1d76ab647b73",
   "metadata": {},
   "source": [
    "## Initial Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ddad9a-cfe1-4f58-8b8b-2b1cc20cd755",
   "metadata": {},
   "source": [
    "The focus of this project is on identifying the decade a song first appeared on the Billboard Top 100. Below are some of the initial questions this project looked to answer throughout the Data Science Pipeline.\n",
    " \n",
    "##### Data-Focused Questions\n",
    "- What are the most frequently occuring words?\n",
    "- What are the most frequently occuring bigrams (pairs of words) by each decade?\n",
    "- What decade did the song first appear in the top 100?\n",
    "- What topics are most unique to each decade?\n",
    "- Is there a correlation between sentiment and decade?\n",
    "- How do topics, such as violence, sexual explicitness, technology references, or relationship references, change over time?\n",
    "- How does foreign language usage change over time?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68b0078-9645-445d-9dec-996ac6aa654e",
   "metadata": {},
   "source": [
    "## Key Findings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0373f8-c854-40d9-a4ba-805f656e987e",
   "metadata": {},
   "source": [
    "The key findings for this presentation are available in slide format by clicking on the [Final Slide Presentation](https://www.canva.com/design/DAFCXoeG7z0/jNCtQkQFqyOTWS5Ckg8Xuw/view?utm_content=DAFCXoeG7z0&utm_campaign=designshare&utm_medium=link2&utm_source=sharebutton).\n",
    "\n",
    "\n",
    "TBD......."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff086dd-8e1c-41b6-89a0-7f4795d490e3",
   "metadata": {},
   "source": [
    "=========================================================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f447ae0-d3ba-4e65-9e65-923a31f642cb",
   "metadata": {},
   "source": [
    "## I. ACQUIRE\n",
    "To acquire the data for this project, we utilized a [Kaggle](https://www.kaggle.com/datasets/dhruvildave/billboard-the-hot-100-songs) data set of the Billboard Top 100 Songs from its inception in 1958 to present. \n",
    "\n",
    "The dataset provided:\n",
    "- date song was on the Billboard Top 100 \n",
    "- rank of song  \n",
    "- title\n",
    "- artist name\n",
    "- rank of song the previous week\n",
    "- rank of song at it's peak week\n",
    "- number of weeks song was on the Top 100  \n",
    "\n",
    "We selected only unique artists and songs, to ensure there were no duplicates, keeping only the earliest appearance on the chart to standardize the selections in the event of multiple appearances. Following song selection with the Kaggle dataset, we then obtained an API token to utilize the [Genius.com](https://genius.com/) API and [LyricGenius Library](https://pypi.org/project/lyricsgenius/) to conduct web scraping to pull the lyrics for the specified songs which became the corpus for this project.\n",
    "\n",
    "The acquired data can be easily accessed via a [Google Drive .csv file](https://drive.google.com/file/d/1S0dJ7-5x8NIgt1LranE3UETgl_JvukGT/view). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac72be3-1478-4f30-8653-a08976cab5a6",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Note about imports: \n",
    "Imports for this project are added in the sections in which they are required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a7a8a76-8a52-468b-84d5-13ad0a952e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import for acquisition\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "import draft_prepare as prep\n",
    "import draft_explore as explore\n",
    "import draft_model as model\n",
    "\n",
    "\n",
    "# import for data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Dict, List, Optional, Union, cast\n",
    "\n",
    "# import to ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "292b3645-39b5-422e-8cf4-e884a2004574",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>artist</th>\n",
       "      <th>date</th>\n",
       "      <th>lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>#1</td>\n",
       "      <td>Nelly</td>\n",
       "      <td>2001-10-20</td>\n",
       "      <td>#1 LyricsUh uh uh I just gotta bring it to the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>#9 Dream</td>\n",
       "      <td>John Lennon</td>\n",
       "      <td>1974-12-21</td>\n",
       "      <td>#9 Dream Lyrics[Verse 1] So long ago Was it in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>#Beautiful</td>\n",
       "      <td>Mariah Carey Featuring Miguel</td>\n",
       "      <td>2013-05-25</td>\n",
       "      <td>#Beautiful Lyrics[Intro: Mariah Carey] Ah, ah,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>#SELFIE</td>\n",
       "      <td>The Chainsmokers</td>\n",
       "      <td>2014-03-15</td>\n",
       "      <td>#SELFIE Lyrics[Verse 1] When Jason was at the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>#thatPOWER</td>\n",
       "      <td>will.i.am Featuring Justin Bieber</td>\n",
       "      <td>2013-04-06</td>\n",
       "      <td>#thatPOWER Lyrics[Instrumental break]  [Pre-Ch...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0       title                             artist        date  \\\n",
       "0           2          #1                              Nelly  2001-10-20   \n",
       "1           4    #9 Dream                        John Lennon  1974-12-21   \n",
       "2           5  #Beautiful      Mariah Carey Featuring Miguel  2013-05-25   \n",
       "3           6     #SELFIE                   The Chainsmokers  2014-03-15   \n",
       "4           7  #thatPOWER  will.i.am Featuring Justin Bieber  2013-04-06   \n",
       "\n",
       "                                              lyrics  \n",
       "0  #1 LyricsUh uh uh I just gotta bring it to the...  \n",
       "1  #9 Dream Lyrics[Verse 1] So long ago Was it in...  \n",
       "2  #Beautiful Lyrics[Intro: Mariah Carey] Ah, ah,...  \n",
       "3  #SELFIE Lyrics[Verse 1] When Jason was at the ...  \n",
       "4  #thatPOWER Lyrics[Instrumental break]  [Pre-Ch...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# acquire data from .csv saved and processed using functions found in acquire.py\n",
    "df = pd.read_csv('songs_0526.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3880d05-efc6-46ab-972a-8c76a99d3a30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23762, 5)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# obtain number of columns and rows for original dataframe\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd63d50-ff49-481a-8a6e-ebefdec84ba3",
   "metadata": {},
   "source": [
    "### The Original DataFrame Size: \n",
    "- 23,762 rows, or documents, and 5 columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d076cd8f-b486-41d3-896d-9c2024733290",
   "metadata": {},
   "source": [
    "---- Draft Info for Acquire ----\n",
    "\n",
    "\n",
    "we started with 300k+ rows but it contain a lot of duplicates\n",
    "\n",
    "we then grouped by artist and song title with date.min() in order to remove duplicates and keep the earliest appearance in the top 100\n",
    "\n",
    "with this reduced dataset of unique songs we were able to use lyricsgenius to pull data from the Genius API to pull in lyrics\n",
    "\n",
    "Some string cleaning was performed to retrieve more results, (title and song formatting differed from the Kaggle dataset and genius.com)\n",
    "\n",
    "so the main challenges were:\n",
    "\n",
    "duplicated songs, and inconsistent naming, plus the Genius API sometimes returned incorrect lyrics.\n",
    "\n",
    "we are able to test for correct lyrics by doing a string comparison of song title and the first section of the returned lyrics\n",
    "\n",
    "all properly pulled lyrics contain the song title prior to the actual lyrics\n",
    "\n",
    "so\n",
    "\n",
    "song.title in song.lyrics gave us an accurate test\n",
    "\n",
    "however with regex, ben was able to be more specific\n",
    "\n",
    "song.title == song.title_from_lyrics\n",
    "\n",
    "I’d have to look back at his notebook to know what he called the variables\n",
    "\n",
    "but thats the gist of it\n",
    "\n",
    "also add in that it took 1000+ hours to run the initial acquire"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836ce5ea-d032-4f7d-8c2d-dc6b6375d8b8",
   "metadata": {},
   "source": [
    "=========================================================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0896e47-ee98-4192-9cb0-f79b187d5658",
   "metadata": {},
   "source": [
    "## II. PREPARE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9df564-01cc-463b-96a6-8536629a9e95",
   "metadata": {},
   "source": [
    "After data acquisition, the dataframe was analyzed and cleaned to facilitate functional exploration and clarify variable confusion. The preparation of this data can be replicated using the 'get_data' function saved within the prepare.py file inside the [Lyrical Evolution](https://github.com/CBRJ-Lyrical-Metrics/song-lyrics-capstone) repository on GitHub. The function takes in the original acquire dataframe and returns it with the changes noted below.\n",
    "\n",
    "**Steps Taken to Clean & Prepare Data:**\n",
    "\n",
    "- Cleaning: \n",
    "    - Make all text lowercase\n",
    "    - Normalize, encode, and decode to remove accented text and special characters\n",
    "    - Expand abbreviated contractions\n",
    "    - Lemmatize words to acquire base words\n",
    "    - Remove stopwords\n",
    "    - Convert date to DateTime format\n",
    "    - Remove song part identifiers ('lyrics' 'verse', 'chorus', 'hook', 'embed')\n",
    "    \n",
    "---   \n",
    "- Address missing values, data errors, unnecessary data, and unclear values:\n",
    "    - The dataset had no null values, therefore, there was no need to drop any observations for this reason.\n",
    "    - Data Errors: The API returned lyrics that were not the expected song's lyrics. This was addressed by:\n",
    "        1. Mannually checking some of the lyrics to identify the output pattern.\n",
    "        2. Coding to compare the title syntax, to the lyric output pattern. If they match after cleaning manipulation, they would be accepted as correct. If the title and lyric output syntax did not match, further cleaning would be conducted to correct the error, or the song would be dropped.  \n",
    "---    \n",
    "- Create feature engineered columns:\n",
    "    - Decade \n",
    "    - Chorus Count\n",
    "    - Verse Count\n",
    "    - Verse/Chorus Ratio\n",
    "    - Word Count\n",
    "    - Unique Words per Song\n",
    "    - Unique Words per Decade\n",
    "    - Bigrams\n",
    "    - Trigrams\n",
    "    \n",
    "- Apply Natural Language Processing (NLP Methods):\n",
    "    - Topic Modeling\n",
    "    - Sentiment Analysis\n",
    "    \n",
    "---\n",
    "- Split corpus into train, validate, and test samples \n",
    "\n",
    "**Note on Splitting Data:**\n",
    "The data was not split prior to Exploration because the features were not utilized in modeling, therefore there was no concern of data leakage. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451b9a01-205b-4f39-a1fc-48c028d1fba5",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b275169-a1af-49e5-9015-0253404eec84",
   "metadata": {},
   "source": [
    "### Specialized Preparation Steps\n",
    "After applying data pre-processing methods to conduct basic cleaning, two specialized preparation steps were taken specific to Natural Langauge Processing. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d44f477-d17f-4b5f-a2c1-4448d9837aed",
   "metadata": {},
   "source": [
    "### Topic Modeling\n",
    "\n",
    "The first specialized preparation step taken was Topic Modeling. Specifically, Latent Dirichlet Allocation was the Topic Modeling method utilized to create clustered groupings of labeled text, identifying the 20 most common topics found within the corpus. Those 20 topics were then manually reviewed for accuracy and theme, then decreased to 17 when it was determined there was overlap between a few of the topics. Following the identification of the 17 most common topics, those with similar themes were combined into two related groupings that would be investigated further in exploration:\n",
    "- **Vice Topics** - including the three separate topics of violence, sex, and money\n",
    "- **Relationship Topics** -  including the five separate topics of affection, breakups, heartache, jealousy, sex\n",
    "\n",
    "These labels were then added to the dataframe, identifying all documents that qualified as having the designated topic as a theme, or listing the theme as 'Other' if the Vice or Relationship topics were not applied. \n",
    "\n",
    "\n",
    "### Sentiment Analysis\n",
    "\n",
    "Secondly, Sentiment Analysis was also conducting in preparation as a speciaized approach, specific to Natural Language Pricessing and in conjunction with Time Series Analysis.  The sklearn.decomposition package was used to conduct Sentiment Analaysis, first examining the change in average sentiment score over time by looking at a rolling 5 year avereage and average by decade. Divided sentiment score into 5 categories _________. Looked at what portion of the total taken up by each category adn how it changed over time. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937b3c41-a336-4a8c-9777-c13ff07c28ef",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee80dea-08b3-4064-adcf-513ab910b2d5",
   "metadata": {},
   "source": [
    "## Results of Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d32c4894-1f7c-4684-8b22-81b119ad2ca3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import for prepare\n",
    "import draft_prepare\n",
    "import unicodedata\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from time import strftime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9f2991-5a51-464c-9aac-b7ccf3be680f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features added ******************\n",
      "fitting lda ***********\r"
     ]
    }
   ],
   "source": [
    "# apply the data preparation observations and tasks to clean the data using the prep_data function found in the prepare.py\n",
    "df = prep.get_data()\n",
    "df = prep.get_topics(df)\n",
    "# view first few rows of dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da756d4a-6454-4067-bbe0-d8b3eca0037c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073cf085-4b0b-44cc-a947-ada6d011536a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35f06a2-40f7-49ea-b3b2-502d3a90b253",
   "metadata": {},
   "source": [
    "## Prepared DataFrame Size: \n",
    "- 23,762 rows, or documents, and 23 columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad099ea-da58-4d5c-81d7-b391b4449579",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83fdb06a-f042-4b96-8fda-bf90f16f0227",
   "metadata": {},
   "source": [
    "### PREPARE - SPLIT  ( Adjustments will be made prior to the final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa98112-1a01-4d46-bfe5-70db5d26156e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import for split\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c20745-cd3b-4686-a502-99981315e1d0",
   "metadata": {},
   "source": [
    "After preparing the corpus, it was split into 3 samples; train, validate, and test using:\n",
    "\n",
    "- Random State: 42\n",
    "- Test = 20% of the original dataset\n",
    "- The remaining 80% of the dataset is divided between valiidate and train\n",
    "    - Validate (.30*.80) = 24% of the original dataset\n",
    "    - Train (.70*.80) = 56% of the original dataset\n",
    "    \n",
    "The split of this data can be replicated using the split_data function saved within the prepare.py file inside the [_____](_________) repository on GitHub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687cb248-9647-41c5-9d50-30a4afa44e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into train, validate, and test using the split_data function found in the prepare.py\n",
    "train, validate, test = prep.split_data(df)\n",
    "# obtain the number of rows and columns for the splits\n",
    "train.shape, validate.shape, test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c0e03e-2952-4eb6-9d44-54a5a04b8bf4",
   "metadata": {},
   "source": [
    "=========================================================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919c78fe-d835-4aa1-ab27-80c23b51faf5",
   "metadata": {},
   "source": [
    "## III. EXPLORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd7468a-7db3-4a6c-ba59-ca948831622c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import for data visualization\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.ticker import StrMethodFormatter\n",
    "from matplotlib import style\n",
    "from wordcloud import WordCloud\n",
    "import draft_explore as explore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e368cfc-c8db-4375-9436-087118ad07ca",
   "metadata": {},
   "source": [
    "After acquiring and preparing the corpus, exploration was conducted. All univariate exploration was completed on the entire cleaned corpus in the workbook for this project. For the purpose of the final report, only the target variable will be displayed in order to reduce noise and provide focused context for the project. Following univariate exploration, the split sets (train, validate, and test samples) were utilized thorugh modeling, where only the train set was used for bivariate and multivariate exploration to prevent data leakage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4497c72-55d0-49eb-be38-154f2edf0a13",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c18e44d-ab8b-4cd3-af33-6388dd07862f",
   "metadata": {},
   "source": [
    "### EXPLORATION QUESTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa631c3e-d91c-4717-b4cc-01f09def4632",
   "metadata": {},
   "source": [
    "All bivariate exploration was conducted on the train corpus to prevent data leakage. The initial questions and univariate exploration guided the bivariate exploration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9502d911-af97-483e-a1d5-349aea49a78b",
   "metadata": {},
   "source": [
    "#### EXPLORE QUESTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888ede95-8e60-4d69-a6b0-a2b317559668",
   "metadata": {},
   "source": [
    "### QUESTION 1: How has Sentiment Chaged Over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d59745-be33-4929-b07b-708ca0d68aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualization\n",
    "palette = [\n",
    "'#1f1e1b', #(black)\n",
    "'#fc9d1c', #(orange)\n",
    "'#ec1c34', #(red)\n",
    "'#69b138', #(green)\n",
    "'#2dace4', #(blue)\n",
    "'#fbdb08' #(yellow)\n",
    "]\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "sns.barplot(data=df, y='sentiment', x='decade', ci=None, ec='black',\n",
    "            palette=palette)\n",
    "plt.title('Average Sentiment by Decade', fontsize=16)\n",
    "plt.ylabel('Average Sentiment Score', fontsize=14)\n",
    "plt.xlabel(None)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f74f8a-4397-4cd6-a643-08f6b3b1c5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hypothesis Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d8992b-ef63-4cf4-849a-4b9039887562",
   "metadata": {},
   "source": [
    "#### ANSWER 1: \n",
    "Sentiment was fairly steady in the 60's and 70's, followed by a gradual downward trend which becomes sharper in the 2000's and 2010's. The downward trend is due to an increase in very negative sentiment and decrease in very positivesentiment while mid-range sentiment stays contstant. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503ce5b0-b5c9-4335-87a5-263e40c892d1",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c1d4c5-a1f7-4ae2-8afb-686121e70eab",
   "metadata": {},
   "source": [
    "### QUESTION 2: What Topics are Most Prevalent From ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3886c652-8c1c-4630-8679-32bad9768e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "explore.topic_popularity(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6526a5d-c90e-4f51-a476-f21670103591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hypothesis Testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a441ce9a-bf1e-4826-853e-bb0cd8c9f0b9",
   "metadata": {},
   "source": [
    "#### ANSWER 2: \n",
    "Breakups are by far the most popular topics in songs, followed by being lost in life, then affection, sex, and nature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670b60c6-1f78-43a7-8994-4db9ec8954da",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb530be-6211-44d0-a297-b3e23c62be54",
   "metadata": {},
   "source": [
    "### QUESTION 3: How Do Relationship Topics Change Over the Decades?   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39cb6f85-539d-400c-8f25-e74bc4adcbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization\n",
    "explore.relationship_line(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18cc4409-cb69-4c27-a3d0-ca53211624d5",
   "metadata": {},
   "source": [
    "Observation:\n",
    "There appears to be an inverse relationship between affection and sex, with the topic of affection decreaseing in prevalence over time, and the topic of sex increasing in usage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a030f994-c27a-481a-a42f-42b2ab6842ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization\n",
    "explore.touch_swarm(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a59f0df-6b92-4b36-82b1-6de684de1bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hypothesis Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ef1c5e-25ac-4926-bd7a-3ddd9e613b1b",
   "metadata": {},
   "source": [
    "#### ANSWER 3: \n",
    "While most relationship topics appear constant, affection and sex have an inverse relationship. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a1aeee-5f81-4b97-8993-1ccef77175ef",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fbe992-9a12-4057-b452-cd6ec3ac3717",
   "metadata": {},
   "source": [
    "### QUESTION 4: How Do Vice Topics Change Over the Decades?   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3efb1086-aca0-46da-b522-6ebfdd7b257b",
   "metadata": {},
   "outputs": [],
   "source": [
    "explore.vice_swarm(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9df5ec9-4f5d-403b-818e-65b5481f27d2",
   "metadata": {},
   "source": [
    "#### ANSWER 4: \n",
    "After 1990 sex became extremely popular in lyrics, then around 2015 violence and money exploded as well. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f18e4e0-6d19-433b-8296-2fa8b9695c33",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c19ac86-6cf9-4469-92c1-e20cdaa94d9e",
   "metadata": {},
   "source": [
    "### QUESTION 5: What Happened to the Love?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0422b0-39d2-4e55-be6f-ea3f751332be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "TBD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c39828b-52aa-44f4-9c6e-455795a6f6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hypothesis Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a20a59-8136-45f6-ba55-48c3f8dcdc83",
   "metadata": {},
   "source": [
    "#### ANSWER 5: \n",
    "Love went from most common word in the early decades, to lower in the top 5, then out of the top 5 and replaced with like. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac10a9de-95a8-4daf-b1e3-e4cf8ef4de83",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d6c456-2f86-4b58-9fe2-275219c4f04f",
   "metadata": {},
   "source": [
    "### EXPLORATION SUMMARY\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c456be1-eb72-47a7-b705-67edb07d26e4",
   "metadata": {},
   "source": [
    "=========================================================================================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15980c0-808c-448b-9a74-6fb25fff882e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### SPLIT???"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b8ebc6-10c5-4182-9d63-0e4d761931d7",
   "metadata": {},
   "source": [
    "## IV. MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df42ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4d6649-e624-4553-a888-6d541bc4abed",
   "metadata": {},
   "source": [
    "### Focus of Model Metrics\n",
    "The target variable, Decade, is a categorical variable, therefore classification machine learning algorithms were used to fit to the training corpus and the models were evaluated on the validate corpus. The metrics used for model evaluation was accuracy, due to the multi-class classification approach. In other words, the model was optimized for identifying true positives, false positive, true negatives, and false negatives, therefore we focused on creating a model with the highest accuracy score from train to validate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15bd6310",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get fresh data\n",
    "df = pd.read_csv('songs_0526.csv', index_col=0)\n",
    "# prep for model\n",
    "df = prep.model_clean(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca5a63a-4615-42df-adb8-657de6c546fe",
   "metadata": {},
   "source": [
    "### Set X & y\n",
    "As mentioned above, two different approaches were taken to prepare the data for modeling. Feature engineering was done for exploratory analysis and even more for modeling. This however did not result in a significant improvement in the accuracy of the model. Therefore, the data was prepared for modeling by using TF-IDF vectorization which takes into account the word count in each file vs word count in the entire corpus. Below is how this was performed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48eb07a5-1f03-4b2b-b503-a6aa67026c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make vectorizer\n",
    "tfidf = TfidfVectorizer()\n",
    "X = tfidf.fit_transform(df[\"lyrics\"])\n",
    "y = df[\"decade\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2447d7-1416-40b0-8640-d055c4149517",
   "metadata": {},
   "source": [
    "### Set Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f2bcd5-0951-420a-bf28-7bc695dd0851",
   "metadata": {},
   "source": [
    "A baseline prediction was set by using the mode for decade. This gave us a baseline accuracy of 20.6%. We will evaluate the accuracy of our models in comparrison to that baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59991093-1945-4841-9d86-697669b28795",
   "metadata": {},
   "source": [
    "### Condsider Feature Engineering\n",
    "First lets look at the models with the lower accuracy, this is the df using feature engineering not including TF-IDF. \n",
    "The following were adjustable:\n",
    "- scale or not scale\n",
    "- use only unique bigrams as features or use all numeric features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5905ad01-a1b6-44a8-8a7d-31c203270f34",
   "metadata": {},
   "source": [
    "### Observation of models with feature engineering:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02eb1514-a9cf-4014-a47b-e5f64e7292d0",
   "metadata": {},
   "source": [
    "### Consider TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4b11e5-7df4-4abf-a957-3ab1139f7ce9",
   "metadata": {},
   "source": [
    "#### The Type of Classification models built were \n",
    "- Decision Tree\n",
    "- Random Forest\n",
    "- Logistic Regression\n",
    "\n",
    "The models were run with many trials, adjusting parameters and algorithms to find the best performing model.  \n",
    "\n",
    "- All Logistic Regression models appeared to be overfit based upon their high performance on train accuracy compared to the significant drop off on validate accuracy.\n",
    "    - This is in part due to the use of TF-IDF which analyzes each word in the train corpus and does not remove attributes.\n",
    "- In general all models outperformed baseline, which had  ___ accuracy on train and ___ accuracy on validate.\n",
    "- The Logistic Regression Model that performed best had a c of 1000 and solver of 'lbfgs', with train accuracy of 98% and validate accuracy of 61% performing 19% better than baseline with validate. It was then applied to the un-seen test data with an accuracy of 56%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a947abac-40e4-4485-9584-0210964ad131",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c5c1da-b384-49cf-bc1a-ebc3f864aa52",
   "metadata": {},
   "source": [
    "### MODEL - DECISION TREE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726cb86d-0aa6-476e-827c-af30f91d0e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.run_decision_tree_models(df)\n",
    "results.head(1) # show baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52ae935",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.sort_values('validate_accuracy', ascending=False).head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e4f2e6-460f-47c0-bd50-384db7d781de",
   "metadata": {},
   "source": [
    "The Decision Tree model that performed the best on train & validate set had max_depth of 2, with 51% accuracy on train, and 45% accuracy on validate, so that model will be isolated below in the event it is the best performing model to be applied to the test (unseen) dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e24f001-7ba7-448d-b335-a45e55f27d79",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270052c1-bab2-47e8-b59a-5b71a6a86f7e",
   "metadata": {},
   "source": [
    "### Model - RANDOM FOREST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a52c2c-be2b-430c-b83b-1b006fe62513",
   "metadata": {},
   "outputs": [],
   "source": [
    "results2 = model.run_random_forest_models(df)\n",
    "results2.sort_values('validate_accuracy', ascending=False).head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10eab43c-070b-4722-aa80-f2e2cf34e497",
   "metadata": {},
   "source": [
    "The Random Forest model that performed the best on train & validate set had max_depth of 100 and min_sample_leaf of 1, with 99%  accuracy on train, and 48% accuracy on validate, so that model will be isolated below in the event it is the best performing model to be applied to the test (unseen) dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6066b3b-404f-4820-ae54-77ecd1370c61",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d8c6d5-67c9-4131-86ac-7c232895b562",
   "metadata": {},
   "source": [
    "### Model - LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41046d1-993b-40e7-be10-2beda9826b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "results3 = model.run_logistic_reg_models(df)\n",
    "results3.sort_values('validate_accuracy', ascending=False).head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caba7b5a-5cea-4ed8-91ca-6e93a8d9d0c7",
   "metadata": {},
   "source": [
    "Evaluating the model with the validate data set was done in the function above for comparrison. The Logistic Regression Model that performed best had a c-statistic of 1000 with a train accuracy of 99% and validate accuracy of 61% performing 19% better than baseline on unseen (validate) data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b2dbe2-e0d7-4eb4-8b0d-6844f6df113e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c950b1e-7948-4e56-b87d-e5a1f9b432ef",
   "metadata": {},
   "source": [
    "### Best Performing Model Applied to Test Data (Unseen Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18abc68d-c48e-4be4-b99c-f4cf971148f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "results3.sort_values('validate_accuracy', ascending=False).head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35b6129-5981-4594-81f5-2e4393feab0a",
   "metadata": {},
   "source": [
    "This model is expected to perform around 56-60% accuracy in the future on data it has not seen, given no major changes in the data source, which is better than the baseline prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38284848-c794-43c9-8fa2-7c38be8437e5",
   "metadata": {},
   "source": [
    "=========================================================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f62750-5420-47d4-92e5-67b1759ff80d",
   "metadata": {},
   "source": [
    "## V. CONCLUSION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6175a6-6dd7-484f-a8c6-f2ffb70be756",
   "metadata": {},
   "source": [
    "This project aimed to investigate the patterns of song lyrics across decades using Time Series Analysis and Natural Language Processing techniques including Topic Modeling, Sentiment Analysis, and Term Frequency using a Kaggle data set of the Billboard Top 100 Songs from 1958 - 2021 and lyrics pulled from the Genius.com API. We believed the lyrics of popular songs could be used for historical analysis using exploratory methods and hypothesis testing to identify changing societal trends in relationships, technology, sexuality, and vulgarity. Furthermore, we beleive we can predict the decade the song appeared on the Top 100 using features and machine learning methods.\n",
    "\n",
    "Through exploration and modeling, we determined _____________________.\n",
    "\n",
    "This information could be usiful in various contexts:\n",
    "- Anthropologic and Sociologic Academic Analysis\n",
    "- Marketing Analysis for companies associated with the music industry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3035711f-679d-4c25-aaf2-b7b5808e4042",
   "metadata": {},
   "source": [
    "### RECOMMENDATIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c5502a-20cc-4bc4-809f-cb9fcedfa509",
   "metadata": {},
   "source": [
    "TBD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012c906d-cb4a-4783-995a-9cdc7a4f1c7d",
   "metadata": {},
   "source": [
    "### NEXT STEPS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d3dbe7-5503-453a-9ac7-1a644ae753ad",
   "metadata": {},
   "source": [
    "TBD with more exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ca57c2-01a1-450b-99b8-4604562e3865",
   "metadata": {},
   "source": [
    "=========================================================================================================================================================="
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3067ead486e059ec00ffe7555bdb889e6e264a24dc711bf108106cc7baee8d5d"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
