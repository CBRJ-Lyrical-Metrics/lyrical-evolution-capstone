{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a54c8f7-aa1f-4305-bbf3-62fb05b75fbc",
   "metadata": {},
   "source": [
    "# A LYRICAL EVOLUTION: \n",
    "\n",
    "### An Investigation of the Cultural Lexicon & Historical Relevance of U.S. Popular Music from 1958 - Present"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49eb0c4-37d7-45cc-979b-57baaf3d0d2e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c102d20-adbd-44b0-a0fc-0062cacb04eb",
   "metadata": {},
   "source": [
    "**An NLP based Capstone Project & Final Report Created By:**\n",
    "\n",
    "Ben Smith, Chris Teceno, Jerry Nolf & Rachel Robbins-Mayhill\n",
    "Codeup   |   Innis Cohort   |   June 2022  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0330cf87-3d96-49e2-93f9-f638738c3096",
   "metadata": {},
   "source": [
    "<img src=\"dataset-cover.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819a8499-ba91-4a83-89c6-67dd43412f15",
   "metadata": {},
   "source": [
    "## Project Goal\n",
    "This project aimed to investigate the patterns of song lyrics across decades using Natural Language Processing techniques including Topic Modeling, Sentiment Analysis, and Term Frequency using a Kaggle data set of the Billboard Top 100 Songs from 1958 - 2021 and lyrics pulled from the Genius.com API. We believe the lyrics of popular songs could be used for historical analysis using exploratory methods and hypothesis testing to identify changing societal trends in relationships, technology, sexuality, and vulgarity. Furthermore, we beleive we can predict the decade the song appeared on the Top 100 using features and machine learning methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae16ef11-eaa7-47c1-a858-84593c0dfa32",
   "metadata": {},
   "source": [
    "## Project Description\n",
    "\n",
    "Songs are powerful tokens: they can soothe, validate, ignite, confront, and educate us – among other things. Like time capsules, they are captured for eternity. The slang and language used are often indicative of the times, and you can probably recall exactly when a song was made based on what is mentioned. Arguably, music is a catalyst for societal and cultural evolution like no other art form. It has been causing controversy and societal upheaval for decades, and it seems with every generation there’s a new musical trend that has the older generations shaking their heads. \n",
    "\n",
    "For centuries, songs have been passed down through generations, being sung as oral histories. However, with advancements of the 20th century, technology has made the world of music a much smaller place and, thanks to cheap, widely-available audio equipment, songs are now distributed on a much larger scale, having a farther-reaching impact, and a more permanent place in history. \n",
    "\n",
    "This project aimed to combine the record of lyrical history and technological advancements to evaluate the changes in the cultural lexicon and societal evolution over the last 50+ years. Using machine learning and natural language processing methodologies we investigated the topics prevalent in songs of the past, predicted the decade in which they were written, and conducted historical analysis through exploration to identify changing societal trends in relationships, technology, sexuality, and vulgarity.\n",
    "\n",
    "<img src='Billboard.png' width=\"350\" height=\"350\" align=\"left\"/> To do this, we acquired a [Kaggle](https://www.kaggle.com/datasets/dhruvildave/billboard-the-hot-100-songs) data set of the Billboard Top 100 Songs from its inception in 1958 to present. We then utilized the [Genius.com](https://genius.com/) API and LyricGenius Library to conduct web scraping to pull the lyrics for the specified songs which became the corpus for this project. After acquiring and preparing the corpus, our team conducted natural language processing exploration utilizing methods such as topic modeling, word clouds, and bigrams. We employed multiclass classification methods to create multiple machine learning models. The end goal was to create an NLP model that accurately predicted the decade a song first appeared on the Billboard Top 100 chart, based on the words and word combinations found in the lyrics of the song.\n",
    "\n",
    "We choose the Billboard Hot 100 song list as a focus because it is the music industry standard record chart in the United States for song popularity, published weekly by Billboard magazine. It provides a window into popular culture at a given time, by providing chart rankings of songs that were trending on sales, airplay, and now streaming for that week in the United States. It is arguably the best historical record of the impact of specific popular songs over time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd55c6df-51d2-412e-adce-1daa9ca98119",
   "metadata": {},
   "source": [
    "## Initial Thoughts & Hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe434ce1-b1f0-4415-8403-23081cd5e590",
   "metadata": {},
   "source": [
    "The initial hypothesis of this project was that we could use the top songs of each decade in conjunction with topic modeling to identify unique words or topics which could be used as features to accurately predict the decade a song was on the Billboard Top 100 using machine learning. The thought behind this was that popular songs have been the historians of a unique lexicon, specific to their place in time. We believe the lyrics of popular songs could be analyzed through machine learning to identify societal trends in relationships, technology, sexuality, and vulgarity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c142d940-b385-4db4-b002-1d76ab647b73",
   "metadata": {},
   "source": [
    "## Initial Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ddad9a-cfe1-4f58-8b8b-2b1cc20cd755",
   "metadata": {},
   "source": [
    "The focus of this project is on identifying the decade a song first appeared on the Billboard Top 100. Below are some of the initial questions this project looked to answer throughout the Data Science Pipeline.\n",
    " \n",
    "##### Data-Focused Questions\n",
    "- What are the most frequently occuring words?\n",
    "- What are the most frequently occuring bigrams (pairs of words) by each decade?\n",
    "- What decade did the song first appear in the top 100?\n",
    "- What topics are most unique to each decade?\n",
    "- Is there a correlation between sentiment and decade?\n",
    "- How do topics, such as violence, sexual explicitness, technology references, or relationship references, change over time?\n",
    "- How does foreign language usage change over time?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68b0078-9645-445d-9dec-996ac6aa654e",
   "metadata": {},
   "source": [
    "## Key Findings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0373f8-c854-40d9-a4ba-805f656e987e",
   "metadata": {},
   "source": [
    "The key findings for this presentation are available in slide format by clicking on the [Final Slide Presentation](https://www.canva.com/design/DAFCXoeG7z0/jNCtQkQFqyOTWS5Ckg8Xuw/view?utm_content=DAFCXoeG7z0&utm_campaign=designshare&utm_medium=link2&utm_source=sharebutton).\n",
    "\n",
    "\n",
    "TBD......."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff086dd-8e1c-41b6-89a0-7f4795d490e3",
   "metadata": {},
   "source": [
    "=========================================================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f447ae0-d3ba-4e65-9e65-923a31f642cb",
   "metadata": {},
   "source": [
    "## I. ACQUIRE\n",
    "To acquire the data for this project, we utilized a [Kaggle](https://www.kaggle.com/datasets/dhruvildave/billboard-the-hot-100-songs) data set of the Billboard Top 100 Songs from its inception in 1958 to present. \n",
    "\n",
    "The dataset provided:\n",
    "- date song was on the Billboard Top 100 \n",
    "- rank of song  \n",
    "- title\n",
    "- artist name\n",
    "- rank of song the previous week\n",
    "- rank of song at it's peak week\n",
    "- number of weeks song was on the Top 100  \n",
    "\n",
    "We selected only unique artists and songs, to ensure there were no duplicates, keeping only the earliest appearance on the chart to standardize the selections in the event of multiple appearances. Following song selection with the Kaggle dataset, we then obtained an API token to utilize the [Genius.com](https://genius.com/) API and [LyricGenius Library](https://pypi.org/project/lyricsgenius/) to conduct web scraping to pull the lyrics for the specified songs which became the corpus for this project.\n",
    "\n",
    "The acquired data can be easily accessed via a [Google Drive .csv file](https://drive.google.com/file/d/1S0dJ7-5x8NIgt1LranE3UETgl_JvukGT/view). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac72be3-1478-4f30-8653-a08976cab5a6",
   "metadata": {},
   "source": [
    "### Note about imports: \n",
    "Imports for this project are added in the sections in which they are required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a7a8a76-8a52-468b-84d5-13ad0a952e25",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'draft_prepare'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/jx/x5_xzwy107g6d0zd021r2ph40000gn/T/ipykernel_64111/2061801037.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mdraft_prepare\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mprep\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdraft_explore\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexplore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdraft_model\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'draft_prepare'"
     ]
    }
   ],
   "source": [
    "# import for acquisition\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "import draft_prepare as prep\n",
    "import draft_explore as explore\n",
    "import draft_model as model\n",
    "\n",
    "\n",
    "# import for data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Dict, List, Optional, Union, cast\n",
    "\n",
    "# import to ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292b3645-39b5-422e-8cf4-e884a2004574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# acquire data from .csv saved and processed using functions found in acquire.py\n",
    "df = pd.read_csv('../songs_0526.csv').drop(columns=['Unnamed: 0'])\n",
    "df = draft_prepare.clean_df(df)\n",
    "df = df.set_index('date')\n",
    "df = df[(df.decade != 1950) & (df.decade != 2020)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3880d05-efc6-46ab-972a-8c76a99d3a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain number of columns and rows for original dataframe\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd63d50-ff49-481a-8a6e-ebefdec84ba3",
   "metadata": {},
   "source": [
    "### The Original DataFrame Size: ____ rows, or documents, and ____ columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836ce5ea-d032-4f7d-8c2d-dc6b6375d8b8",
   "metadata": {},
   "source": [
    "=========================================================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0896e47-ee98-4192-9cb0-f79b187d5658",
   "metadata": {},
   "source": [
    "## II. PREPARE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9df564-01cc-463b-96a6-8536629a9e95",
   "metadata": {},
   "source": [
    "After data acquisition, the dataframe was analyzed and cleaned to facilitate functional exploration and clarify variable confusion. The preparation of this data can be replicated using the ______ function saved within the prepare.py file inside the [Lyrical Evolution](https://github.com/CBRJ-Lyrical-Metrics/song-lyrics-capstone) repository on GitHub. The function takes in the original acquire dataframe and returns it with the changes noted below.\n",
    "\n",
    "**Steps Taken to Clean & Prepare Data:**\n",
    "\n",
    "- Cleaning: \n",
    "    - Make all text lowercase\n",
    "    - Normalize, encode, and decode to remove accented text and special characters\n",
    "    - Tokenize strings to break words and punctuation into discrete units\n",
    "    - Expand abbreviated contractions\n",
    "    - Lemmatize words to acquire base words\n",
    "    - Remove stopwords\n",
    "    - Convert date to DateTime format\n",
    "    - Remove song part identifiers ('lyrics' 'verse', 'chorus', 'hook', 'embed')\n",
    "    \n",
    "---   \n",
    "- Address missing values, data errors, unnecessary data, and unclear values:\n",
    "    - \n",
    "    - Drop missing values to prevent impediments in exploration and modeling: ______ documents/observations that had null values in the ______ column \n",
    "    - Drop all rows where ________\n",
    "    - Total dropped documents = _______\n",
    "---    \n",
    "- Create feature engineered columns:\n",
    "    - Decade \n",
    "    - Chorus Count\n",
    "    - Verse Count\n",
    "    - Verse/Chorus Ratio\n",
    "    - Word Count\n",
    "    - Unique Words per Song\n",
    "    - Unique Words per Decade\n",
    "    - Bigrams\n",
    "    - Trigrams\n",
    "    \n",
    "- Apply Natural Language Processing (NLP Methods:\n",
    "    - Topic Modeling\n",
    "    - Sentiment Analysis\n",
    "    \n",
    "---\n",
    "- Split corpus into train, validate, and test samples \n",
    "\n",
    "\n",
    "**Note on Splitting Data:**\n",
    "\n",
    "\n",
    "\n",
    "**Note on Missing Value Handling:**\n",
    "The missing value removal equated to removing ______ observations/documents, which was about ___   \\% of the data set. It still left _______ observations, a substantial number. If given more time with the data, it is recommended to investigate other ways to impute the missing data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937b3c41-a336-4a8c-9777-c13ff07c28ef",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee80dea-08b3-4064-adcf-513ab910b2d5",
   "metadata": {},
   "source": [
    "## Results of Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32c4894-1f7c-4684-8b22-81b119ad2ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import for prepare\n",
    "import draft_prepare\n",
    "import unicodedata\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from time import strftime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c382de0-355d-40e6-9afe-137f3c86888b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the data preparation observations and tasks to clean the data using the prep_data function found in the prepare.py\n",
    "df = prepare.prep_data(df)\n",
    "# view first few rows of dataframe\n",
    "# obtain the number of rows and columns for the updated/cleaned dataframe \n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35f06a2-40f7-49ea-b3b2-502d3a90b253",
   "metadata": {},
   "source": [
    "## Prepared DataFrame Size: 134 rows, or documents, and 13 columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad099ea-da58-4d5c-81d7-b391b4449579",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83fdb06a-f042-4b96-8fda-bf90f16f0227",
   "metadata": {},
   "source": [
    "### PREPARE - SPLIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa98112-1a01-4d46-bfe5-70db5d26156e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import for split\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c20745-cd3b-4686-a502-99981315e1d0",
   "metadata": {},
   "source": [
    "After preparing the corpus, it was split into 3 samples; train, validate, and test using:\n",
    "\n",
    "- Random State: 42\n",
    "- Test = 20% of the original dataset\n",
    "- The remaining 80% of the dataset is divided between valiidate and train\n",
    "    - Validate (.30*.80) = 24% of the original dataset\n",
    "    - Train (.70*.80) = 56% of the original dataset\n",
    "    \n",
    "The split of this data can be replicated using the split_data function saved within the prepare.py file inside the [_____](_________) repository on GitHub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687cb248-9647-41c5-9d50-30a4afa44e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into train, validate, and test using the split_data function found in the prepare.py\n",
    "train, validate, test = prepare.split_data(df)\n",
    "# obtain the number of rows and columns for the splits\n",
    "train.shape, validate.shape, test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c0e03e-2952-4eb6-9d44-54a5a04b8bf4",
   "metadata": {},
   "source": [
    "=========================================================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919c78fe-d835-4aa1-ab27-80c23b51faf5",
   "metadata": {},
   "source": [
    "## III. EXPLORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd7468a-7db3-4a6c-ba59-ca948831622c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import for data visualization\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.ticker import StrMethodFormatter\n",
    "from matplotlib import style\n",
    "from wordcloud import WordCloud\n",
    "import explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ece60b-5838-41ee-ac9b-577d3db57fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Universal Visualization Formatting\n",
    "\n",
    "# determine figure size\n",
    "plt.rc('figure', figsize=(20, 8))\n",
    "# determine font size\n",
    "plt.rc('font', size=15)\n",
    "# determine style\n",
    "plt.style.use('seaborn-deep')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e368cfc-c8db-4375-9436-087118ad07ca",
   "metadata": {},
   "source": [
    "After acquiring and preparing the corpus, exploration was conducted. All univariate exploration was completed on the entire cleaned corpus in the workbook for this project. For the purpose of the final report, only the target variable will be displayed in order to reduce noise and provide focused context for the project. Following univariate exploration, the split sets (train, validate, and test samples) were utilized thorugh modeling, where only the train set was used for bivariate and multivariate exploration to prevent data leakage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b897a71-03f3-440f-a53c-7c5ca636dc48",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1854228a-7ff2-41dd-9e88-b55feba10cb3",
   "metadata": {},
   "source": [
    "### UNIVARIATE EXPLORATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617865e3-742d-4be4-8f35-25cc1a5d5d8e",
   "metadata": {},
   "source": [
    "#### UNIVARIATE EXPLORATION of TARGET VARIABLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f5ff97-3cea-44a4-935a-7d2cf7ac42db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create visualization\n",
    "df.language.value_counts().plot(kind='pie', y='Language', autopct=\"%1.1f%%\")\n",
    "# remove y axis label\n",
    "plt.ylabel(None)\n",
    "#add title\n",
    "plt.title('Top 4 Programming Langauges Across Corpus by Percentage')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3e7208-151f-4d23-89dd-c06b8b02a4b6",
   "metadata": {},
   "source": [
    "#### OBSERVATIONS: \n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4497c72-55d0-49eb-be38-154f2edf0a13",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c18e44d-ab8b-4cd3-af33-6388dd07862f",
   "metadata": {},
   "source": [
    "### EXPLORATION QUESTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa631c3e-d91c-4717-b4cc-01f09def4632",
   "metadata": {},
   "source": [
    "All bivariate exploration was conducted on the train corpus to prevent data leakage. The initial questions and univariate exploration guided the bivariate exploration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9502d911-af97-483e-a1d5-349aea49a78b",
   "metadata": {},
   "source": [
    "#### EXPLORE QUESTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888ede95-8e60-4d69-a6b0-a2b317559668",
   "metadata": {},
   "source": [
    "### QUESTION 1: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231badf5-44ed-479e-9061-269acdedde1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### ANSWER 1: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e572c3-dbdd-41b1-8774-d95ca678cf77",
   "metadata": {},
   "outputs": [],
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9faa39be-a1af-4ba8-89fb-21175c3d9c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "### QUESTION 2: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9babc7-ec4c-4c65-b66a-2e9abd311f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### ANSWER 2: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c890fe1-1aba-41e6-8aab-f8cd13f7ff17",
   "metadata": {},
   "outputs": [],
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb2dc44-bf0e-483a-be7f-48cdf293df70",
   "metadata": {},
   "outputs": [],
   "source": [
    "### QUESTION 3: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d345cc0e-e12c-4348-b4eb-1966e0a0709b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### ANSWER 3: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdbbc39-c2f5-4d07-a7a7-c1a19f2f21ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d93106b-1655-45ca-95f8-912ab18e5910",
   "metadata": {},
   "outputs": [],
   "source": [
    "### QUESTION 4: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8370dc1c-f435-49d2-acfc-20102cabf765",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### ANSWER 4: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8aac11-d688-4179-9e47-03ca04716aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f22a5b-4bf8-48fe-b613-1031a3ffcb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "### QUESTION 5: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c86926-b78b-4d12-b2c8-59ea5b9263e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### ANSWER 5: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9739c8f1-be77-41a4-abc4-8a7c8eb939ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506a35ce-6efe-49a5-9e1b-2fa9f52d04e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### EXPLORATION SUMMARY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c456be1-eb72-47a7-b705-67edb07d26e4",
   "metadata": {},
   "source": [
    "=========================================================================================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15980c0-808c-448b-9a74-6fb25fff882e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### SPLIT???"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b8ebc6-10c5-4182-9d63-0e4d761931d7",
   "metadata": {},
   "source": [
    "## IV. MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4d6649-e624-4553-a888-6d541bc4abed",
   "metadata": {},
   "source": [
    "### Focus of Model Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38284848-c794-43c9-8fa2-7c38be8437e5",
   "metadata": {},
   "source": [
    "=========================================================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f62750-5420-47d4-92e5-67b1759ff80d",
   "metadata": {},
   "source": [
    "## V. CONCLUSION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ca57c2-01a1-450b-99b8-4604562e3865",
   "metadata": {},
   "source": [
    "=========================================================================================================================================================="
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
