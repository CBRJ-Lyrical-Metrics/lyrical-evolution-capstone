{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Column not found: unique_words'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/f5/w0p7b4_j2kdg3kvlsz7nhtn40000gn/T/ipykernel_42773/835841938.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdraft_prepare\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mprep\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mdraft_model\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcontractions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/codeup-data-science/song-lyrics-capstone/Drafts/draft_model.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    356\u001b[0m \u001b[0mdecades\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"lyrics\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"all_words\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m decades[\"unique_words\"] = (\n\u001b[0;32m--> 358\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"decade\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"unique_words\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m )\n\u001b[1;32m    360\u001b[0m \u001b[0mdecades\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"common_unique_words\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecades\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_common_unique_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.9/site-packages/pandas/core/groupby/generic.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1536\u001b[0m                 \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1537\u001b[0m             )\n\u001b[0;32m-> 1538\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_gotitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndim\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.9/site-packages/pandas/core/base.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    230\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Column not found: {key}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m             \u001b[0msubset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0mndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Column not found: unique_words'"
     ]
    }
   ],
   "source": [
    "# import env\n",
    "# # Make HTTP requests\n",
    "# import requests\n",
    "# # Scrape data from an HTML document\n",
    "# from bs4 import BeautifulSoup\n",
    "# I/O\n",
    "#import os\n",
    "# Search and manipulate strings\n",
    "import re\n",
    "#import lyricsgenius\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import draft_prepare as prep\n",
    "import draft_model as model\n",
    "import contractions\n",
    "from collections import Counter\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used to download lexicon data\n",
    "# nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# playing with genius API\n",
    "\n",
    "# genius = lyricsgenius.Genius(env.token)\n",
    "\n",
    "# artist = genius.search_artist(\"Andy Shauf\", max_songs=3, sort=\"title\")\n",
    "# print(artist.songs)\n",
    "# song = artist.songs[0]\n",
    "# print(song.lyrics)\n",
    "# df = pd.read_csv(\"charts.csv\")\n",
    "# df.head()\n",
    "# df.shape\n",
    "# #convert date to datetime\n",
    "# df['date'] = pd.to_datetime(df['date'])\n",
    "# df.head()\n",
    "# first_date = pd.DataFrame(df.groupby(['artist', 'song']).date.min())\n",
    "# # add a column for year and decade\n",
    "# first_date['year'] = first_date.date.dt.year\n",
    "# first_date['decade'] = first_date.date.dt.year - (first_date.date.dt.year % 10)\n",
    "# first_date.reset_index(inplace=True)\n",
    "# first_date.head()\n",
    "# first_date.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def prep_data(df):\n",
    "#     '''\n",
    "#     get unique artist song combos\n",
    "#     make column for year and decade\n",
    "#     change song to title\n",
    "#     '''\n",
    "#     # convert date to datetime\n",
    "#     df['date'] = pd.to_datetime(df['date'])\n",
    "#     # rename song column to title\n",
    "#     df.rename(columns={'song': 'title'}, inplace=True)\n",
    "#     # subset data to make only unique artist title combos while keeping earliest date\n",
    "#     df = pd.DataFrame(df.groupby(['artist', 'title']).date.min())\n",
    "#     # make new column for year and decade\n",
    "#     df['year'] = df.date.dt.year\n",
    "#     df['decade'] = df.date.dt.year - (df.date.dt.year % 10)\n",
    "#     # reset index\n",
    "#     df.reset_index(inplace=True)\n",
    "#     return df\n",
    "# df = prep_data(pd.read_csv(\"charts.csv\"))\n",
    "# df.head()\n",
    "# df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This was used to get initial data\n",
    "\n",
    "\n",
    "# api = lyricsgenius.Genius(env.token, verbose=False)\n",
    "\n",
    "# def prep_songs():\n",
    "#     songs = pd.read_csv('charts.csv')\n",
    "#     songs['date'] = pd.to_datetime(songs.date)\n",
    "#     songs = songs.rename(columns={'song': 'title'})\n",
    "#     songs = pd.DataFrame(songs.groupby(['title', 'artist']).min().date).reset_index()\n",
    "#     return songs\n",
    "\n",
    "# def get_lyrics(title, artist):\n",
    "#     song = api.search_song(title, artist)\n",
    "#     lyrics = song.lyrics.replace('\\n', ' ')\n",
    "#     # TODO handle verse and chorus tags\n",
    "#     return lyrics\n",
    "\n",
    "# songs = prep_songs()\n",
    "\n",
    "# for i in range(len(songs)-1, 0 , -1):\n",
    "    \n",
    "#     title = songs.iloc[[i]].title.values[0]\n",
    "#     artist = songs.iloc[[i]].artist.values[0]\n",
    "    \n",
    "#     try:\n",
    "#         songs.loc[i, 'lyrics'] = get_lyrics(title, artist)\n",
    "#         songs.loc[i, 'status'] = 'lyrics acquired'\n",
    "#     except: \n",
    "#         songs.loc[i, 'status'] = 'an error ocurred'\n",
    "#     # save every 100 songs\n",
    "#     if i % 100 == 0:\n",
    "#         songs.to_csv('songs.csv')\n",
    "    \n",
    "#     print(f'{round((i / len(songs) * 100), 2)}%\\t complete', end='\\r')\n",
    "#songs.to_csv('songs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>artist</th>\n",
       "      <th>date</th>\n",
       "      <th>lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#1</td>\n",
       "      <td>Nelly</td>\n",
       "      <td>2001-10-20</td>\n",
       "      <td>#1 LyricsUh uh uh I just gotta bring it to the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#9 Dream</td>\n",
       "      <td>John Lennon</td>\n",
       "      <td>1974-12-21</td>\n",
       "      <td>#9 Dream Lyrics[Verse 1] So long ago Was it in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>#Beautiful</td>\n",
       "      <td>Mariah Carey Featuring Miguel</td>\n",
       "      <td>2013-05-25</td>\n",
       "      <td>#Beautiful Lyrics[Intro: Mariah Carey] Ah, ah,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>#SELFIE</td>\n",
       "      <td>The Chainsmokers</td>\n",
       "      <td>2014-03-15</td>\n",
       "      <td>#SELFIE Lyrics[Verse 1] When Jason was at the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>#thatPOWER</td>\n",
       "      <td>will.i.am Featuring Justin Bieber</td>\n",
       "      <td>2013-04-06</td>\n",
       "      <td>#thatPOWER Lyrics[Instrumental break]  [Pre-Ch...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        title                             artist        date  \\\n",
       "2          #1                              Nelly  2001-10-20   \n",
       "4    #9 Dream                        John Lennon  1974-12-21   \n",
       "5  #Beautiful      Mariah Carey Featuring Miguel  2013-05-25   \n",
       "6     #SELFIE                   The Chainsmokers  2014-03-15   \n",
       "7  #thatPOWER  will.i.am Featuring Justin Bieber  2013-04-06   \n",
       "\n",
       "                                              lyrics  \n",
       "2  #1 LyricsUh uh uh I just gotta bring it to the...  \n",
       "4  #9 Dream Lyrics[Verse 1] So long ago Was it in...  \n",
       "5  #Beautiful Lyrics[Intro: Mariah Carey] Ah, ah,...  \n",
       "6  #SELFIE Lyrics[Verse 1] When Jason was at the ...  \n",
       "7  #thatPOWER Lyrics[Instrumental break]  [Pre-Ch...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "songs = pd.read_csv('songs_0526.csv', index_col=0)\n",
    "#songs.reset_index(drop = True, inplace=True)\n",
    "#songs.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "songs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title     0\n",
       "artist    0\n",
       "date      0\n",
       "lyrics    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "songs.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23762, 23762, 23762)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "songs.shape[0],\n",
    "#how many lyrics contain 'Lyrics'\n",
    "songs.lyrics.str.contains('Lyrics').sum(),\n",
    "# how many lyrics contain 'Embed'\n",
    "songs.lyrics.str.contains('Embed').sum()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I am'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test contractions package\n",
    "contractions.fix('I\\'m')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#1 LyricsUh uh uh I just gotta bring it to they attention dirty, that\\'s all..  You better watch who you talkin bout; runnin your mouth, like you know me You gon\\' fuck around and show why the \"Show Me\" get called the \"Show Me\" Why one-on-one you can\\'t hold me if your last name was Hanes Only way you wear me out is stitch my name on your pants No resident of France; but you swear I\\'m from Paris Hundred-six karats - total? Naw that\\'s per wrist Trying to compurr this - my chain to yo\\' chain I\\'m like Sprint or Motorola - no service, out of your range You out of your brains, thinkin I\\'mma shout out your name You gotta come up with better ways than that to catch your fame All that pressure you applyin it\\'s time to ease off Before I hit you from the blindside takin your sleeves off As much as we\\'s floss, still hard to please boss Don\\'t be lyin bitchin and cryin - suck it up as a loss Cause your, acts is wack, your whole label is wack And matter fact, eh eh-eh eh a-hold that [Hook] I.. am..  #1 - no matter if you like it Here take it sit down & write it I.. am.. number one Hey hey hey hey hey hey - now let me ask you man What does it take to be #1? 2 is not a winner and three nobody remembers (hey) What does it take to be #1? Hey hey hey hey..  Do you like it when I shake it for ya, daddy? Move it all around? Let you get a peep before it touches the ground?  Hell yeah ma I love a girl that\\'s willin to learn Willin to get in the driver\\'s seat and willin to turn And not concerned about that he say, she say, did he say What I think he said? Squash that, he probably got that off eBay Or some, Internet access some, website chat line Mad cause I got mine, don\\'t wind up on the flat line Ohh if my uncle could see me now If he could see how many rappers wanna be me now Straight emulatin my style right to the \"down down\" Can\\'t leave out the store now better wait \\'til they calm down I got hella shorties, comin askin, \"Yo where the party?\" Ohh lordy - will I continue to act naughty? Mixing Cris\\' and Bacardi, got me thinkin fo\\' sho\\' I\\'m not a man of many words but there\\'s one thing I know - Pimp [Hook] I.. am.. number one - no matter if you like it Here take it sit down and write it Hey I.. am.. number one Hey hey hey hey hey hey hey hey Tell me now Dirty What does it take to be number one? Two is not a winner and three nobody remembers (tell me) What does it take to be number one? Hey hey hey hey hey hey hey hey  Check it, uhh, check, yo Aiyyo I\\'m tired of people judgin what\\'s real Hip-Hop Half the time you be them niggas who fuckin album flop YOU KNOW! Boat done sank and it ain\\'t left the dock C\\'MON! Mad cause I\\'m hot; HE JUST - mad cause he not You ain\\'t gotta gimme my props, just gimme the yachts Gimme my rocks, and keep my fans comin in flocks \\'Til you top the Superbowl, keep your mouth on lock Shhhhh.. {*crickets*} I\\'m awake, ha ha ha! I\\'m cocky on the mic but I\\'m humble in real life Taking nothin for granted blessin e\\'rything on my life Trying to see a new light at the top of the roof Baby name not Sigel but I speak The Truth I heat the booth - Nelly actin so uncouth Top down shirt off in the coupe, spreadin the loot With my family and friends, and my closest of kin And I\\'ll do it again if it means I\\'mma win [Hook] Hey Dirty I.. am.. number one - no matter if you like it Here take it sit down and write it I..I..I am.. number one Two is not a winner and three nobody remembers Number one Cause two is not a winner and three nobody remembers5Embed'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "songs = pd.read_csv('songs_0526.csv', index_col=0)\n",
    "songs.lyrics[2] # look at raw lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing for def model_clean in prepare.py\n",
    "\n",
    "# # remove everything before and including 'lyrics'\n",
    "# songs.lyrics = songs.lyrics.apply(lambda x: x.split('Lyrics')[1])\n",
    "# # remove everything after and including 'Embed'\n",
    "# songs.lyrics = songs.lyrics.apply(lambda x: x.rsplit('Embed')[0])\n",
    "# # remove everything contained in []\n",
    "# songs.lyrics = songs.lyrics.apply(lambda x: re.sub(r'\\[.*?\\]', '', x))\n",
    "# # remove everything contained in ()\n",
    "# songs.lyrics = songs.lyrics.apply(lambda x: re.sub(r'\\(.*?\\)', '', x))\n",
    "# # expand contractions\n",
    "# songs.lyrics = songs.lyrics.apply(lambda x: contractions.fix(x))\n",
    "\n",
    "# # convert date to datetime\n",
    "# songs['date'] = pd.to_datetime(songs['date'])\n",
    "# # make new column for decade\n",
    "# songs['decade'] = songs.date.dt.year - (songs.date.dt.year % 10)\n",
    "\n",
    "# # songs = songs[(songs.decade != 1950) & (songs.decade != 2020)]\n",
    "# songs['clean_lyrics'] = songs.lyrics.apply(prep.basic_clean)\n",
    "# songs['lemmatized'] = songs.clean_lyrics.apply(prep.lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>artist</th>\n",
       "      <th>date</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>decade</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#1</td>\n",
       "      <td>Nelly</td>\n",
       "      <td>2001-10-20</td>\n",
       "      <td>uh uh uh got bring attention dirty better watc...</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#9 Dream</td>\n",
       "      <td>John Lennon</td>\n",
       "      <td>1974-12-21</td>\n",
       "      <td>long ago dream dream know yes know seemed real...</td>\n",
       "      <td>1970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>#Beautiful</td>\n",
       "      <td>Mariah Carey Featuring Miguel</td>\n",
       "      <td>2013-05-25</td>\n",
       "      <td>ah ah beautiful ah ah beautiful hop back bike ...</td>\n",
       "      <td>2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>#SELFIE</td>\n",
       "      <td>The Chainsmokers</td>\n",
       "      <td>2014-03-15</td>\n",
       "      <td>jason table kept seeing look girl think make j...</td>\n",
       "      <td>2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>#thatPOWER</td>\n",
       "      <td>will.i.am Featuring Justin Bieber</td>\n",
       "      <td>2013-04-06</td>\n",
       "      <td>oh alive alive alive oh fly fly fly oh alive a...</td>\n",
       "      <td>2010</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        title                             artist       date  \\\n",
       "2          #1                              Nelly 2001-10-20   \n",
       "4    #9 Dream                        John Lennon 1974-12-21   \n",
       "5  #Beautiful      Mariah Carey Featuring Miguel 2013-05-25   \n",
       "6     #SELFIE                   The Chainsmokers 2014-03-15   \n",
       "7  #thatPOWER  will.i.am Featuring Justin Bieber 2013-04-06   \n",
       "\n",
       "                                              lyrics  decade  \n",
       "2  uh uh uh got bring attention dirty better watc...    2000  \n",
       "4  long ago dream dream know yes know seemed real...    1970  \n",
       "5  ah ah beautiful ah ah beautiful hop back bike ...    2010  \n",
       "6  jason table kept seeing look girl think make j...    2010  \n",
       "7  oh alive alive alive oh fly fly fly oh alive a...    2010  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "songs = prep.model_clean(songs)\n",
    "songs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'uh uh uh got bring attention dirty better watch talkin bout runnin mouth like know gon fuck around show show get called show oneonone cannot hold last name hanes way wear stitch name pant resident france swear paris hundredsix karat total naw per wrist trying compurr chain yo chain like sprint motorola service range brain thinkin imma shout name got come better way catch fame pressure applyin time ease hit blindside takin sleeve much wes floss still hard please bos lyin bitchin cryin suck loss act wack whole label wack matter fact eh eheh eh ahold matter like take sit write number one hey hey hey hey hey hey let ask man take winner three nobody remembers take hey hey hey hey like shake ya daddy move around let get peep touch ground hell yeah love girl willin learn willin get driver seat willin turn concerned say say say think said squash probably got ebay internet access website chat line mad got mine wind flat line ohh uncle could see could see many rapper want straight emulatin style right cannot leave store better wait til calm got hella shorties comin askin yo party ohh lordy continue act naughty mixing cris bacardi got thinkin fo sho man many word one thing know pimp number one matter like take sit write hey number one hey hey hey hey hey hey hey hey tell dirty take number one two winner three nobody remembers take number one hey hey hey hey hey hey hey hey check uhh check yo aiyyo tired people judgin real hiphop half time nigga fuckin album flop know boat done sank left dock cmon mad hot mad got give prop give yacht give rock keep fan comin flock til top superbowl keep mouth lock shhhhh cricket awake ha ha ha cocky mic humble real life taking nothin granted blessin erything life trying see new light top roof baby name sigel speak truth heat booth nelly actin uncouth top shirt coupe spreadin loot family friend closest kin mean imma win hey dirty number one matter like take sit write iii number one two winner three nobody remembers number one two winner three nobody remembers'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "songs.lyrics[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# modeling tryouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = model.run_logistic_reg_models(songs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results.sort_values('validate_accuracy', ascending=False).head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results2 = model.run_decision_tree_models(songs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results2.sort_values('validate_accuracy', ascending=False).head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting vectorizer to data...\r"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'decade'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3360\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3361\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3362\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.9/site-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.9/site-packages/pandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'decade'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/f5/w0p7b4_j2kdg3kvlsz7nhtn40000gn/T/ipykernel_37222/3202306278.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresults3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_random_forest_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msongs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/codeup-data-science/song-lyrics-capstone/Drafts/draft_model.py\u001b[0m in \u001b[0;36mrun_random_forest_models\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Fitting vectorizer to data...\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\\r\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"lyrics\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"decade\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m     \u001b[0;31m# split data into train and test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.9/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3456\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3457\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3458\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3459\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3460\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3361\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3362\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3363\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3365\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhasnans\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'decade'"
     ]
    }
   ],
   "source": [
    "results3 = model.run_random_forest_models(songs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results3.sort_values('validate_accuracy', ascending=False).head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# add features\n",
    "~~Replace Contractions~~\n",
    "\n",
    "~~Word Count~~\n",
    "\n",
    "~~Unique words per song~~\n",
    "\n",
    "~~Unique words per decade~~\n",
    "\n",
    "~~Ratio verse to chorus~~\n",
    "\n",
    "~~Number of verses~~\n",
    "\n",
    "    Length of verses\n",
    "    Length of chorus\n",
    "    Sexual Word Count\n",
    "    Vulgar Word Count\n",
    "\n",
    "~~Sentiment~~\n",
    "\n",
    "~~Topic Modeling~~\n",
    "\n",
    "~~Number of verses~~ \n",
    "\n",
    "~~Number of choruses~~ \n",
    "\n",
    "~~Ratio of verses to choruses~~\n",
    "\n",
    "~~Data/Time~~\n",
    "\n",
    "~~year~~\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>artist</th>\n",
       "      <th>date</th>\n",
       "      <th>lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#1</td>\n",
       "      <td>Nelly</td>\n",
       "      <td>2001-10-20</td>\n",
       "      <td>#1 LyricsUh uh uh I just gotta bring it to the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#9 Dream</td>\n",
       "      <td>John Lennon</td>\n",
       "      <td>1974-12-21</td>\n",
       "      <td>#9 Dream Lyrics[Verse 1] So long ago Was it in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>#Beautiful</td>\n",
       "      <td>Mariah Carey Featuring Miguel</td>\n",
       "      <td>2013-05-25</td>\n",
       "      <td>#Beautiful Lyrics[Intro: Mariah Carey] Ah, ah,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        title                         artist        date  \\\n",
       "2          #1                          Nelly  2001-10-20   \n",
       "4    #9 Dream                    John Lennon  1974-12-21   \n",
       "5  #Beautiful  Mariah Carey Featuring Miguel  2013-05-25   \n",
       "\n",
       "                                              lyrics  \n",
       "2  #1 LyricsUh uh uh I just gotta bring it to the...  \n",
       "4  #9 Dream Lyrics[Verse 1] So long ago Was it in...  \n",
       "5  #Beautiful Lyrics[Intro: Mariah Carey] Ah, ah,...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "songs = pd.read_csv('songs_0526.csv', index_col=0)\n",
    "songs.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all words in [] from lyrics\n",
    "songs['place_words'] = songs.lyrics.apply(lambda x: re.findall(r'\\[.*?\\]', x))\n",
    "# make place_words all lowercase\n",
    "songs['place_words'] = songs.place_words.apply(lambda x: [word.lower() for word in x])\n",
    "# make place_words a string\n",
    "songs['place_words'] = songs.place_words.apply(lambda x: ' '.join(x))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2                                     [hook] [hook] [hook]\n",
       "4        [verse 1] [pre-chorus 1] [chorus] [verse 2] [p...\n",
       "5        [intro: mariah carey] [verse 1: miguel] [choru...\n",
       "6        [verse 1] [drop 1] [verse 2] [drop 2] [verse 3...\n",
       "7        [instrumental break] [pre-chorus: justin biebe...\n",
       "                               ...                        \n",
       "28304                                                     \n",
       "28305                                                     \n",
       "28306    [verse 1: chad kroeger] [pre-chorus: chad kroe...\n",
       "28308    [intro: will smith & (sisqo)] [verse 1: will s...\n",
       "28310                                                     \n",
       "Name: place_words, Length: 23762, dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "songs.place_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert from list to string\n",
    "#songs.place_words = songs.place_words.apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "songs.reset_index(inplace=True)\n",
    "songs.rename(columns={'index': 'old_index'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('[verse', 27447),\n",
       " ('[chorus]', 21833),\n",
       " ('1]', 11316),\n",
       " ('2]', 11093),\n",
       " ('[chorus:', 8000),\n",
       " ('&', 7598),\n",
       " ('[bridge]', 5635),\n",
       " ('[pre-chorus]', 5020),\n",
       " ('3]', 3836),\n",
       " ('2:', 3543),\n",
       " ('1:', 3541),\n",
       " ('[outro][verse', 2688),\n",
       " ('[chorus', 2019),\n",
       " ('[chorus][verse', 1815),\n",
       " ('3:', 1737),\n",
       " ('[outro:', 1648),\n",
       " ('[pre-chorus:', 1589),\n",
       " ('[bridge:', 1481),\n",
       " ('lil', 1220),\n",
       " ('[post-chorus]', 1192),\n",
       " ('[hook]', 1130),\n",
       " ('[outro][intro]', 958),\n",
       " ('the', 942),\n",
       " ('[hook:', 935),\n",
       " ('[refrain]', 879)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#make one list from column place_words\n",
    "all_place_words= ''\n",
    "for i in range(len(songs)):\n",
    "    all_place_words += songs.place_words[i]\n",
    "# what are the counts for the common words\n",
    "#all_place_words\n",
    "Counter(all_place_words.split()).most_common(25)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12671"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many times is chorus in place_words?\n",
    "songs.place_words.str.contains('chorus').sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[verse 1] [pre-chorus 1] [chorus] [verse 2] [pre-chorus 2] [chorus] [outro]'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "songs.place_words[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_string = songs.place_words[1] \n",
    "# precleaned\n",
    "#'[verse 1] [pre-chorus 1] [chorus] [verse 2] [pre-chorus 2] [chorus] [outro]'\n",
    "\n",
    "# how many times is chorus in test_string?\n",
    "test_string.count('[chorus') # this gets multiple types of chorus without pre-chorus\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_string.count('verse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add column for chorus count\n",
    "songs['chorus_count'] = songs.place_words.apply(lambda x: x.count('[chorus'))\n",
    "# add column for verse count\n",
    "songs['verse_count'] = songs.place_words.apply(lambda x: x.count('verse'))\n",
    "# add column for ratio of verses to chorus\n",
    "songs['verse_chorus_ratio'] = songs.verse_count / songs.chorus_count\n",
    "# add column for pre-chorus count\n",
    "songs['pre_chorus_count'] = songs.place_words.apply(lambda x: x.count('[pre-chorus'))\n",
    "# add column for outro count\n",
    "songs['outro_count'] = songs.place_words.apply(lambda x: x.count('outro'))\n",
    "# add column for bridge count\n",
    "songs['bridge_count'] = songs.place_words.apply(lambda x: x.count('bridge'))\n",
    "# add column for hook count\n",
    "songs['hook_count'] = songs.place_words.apply(lambda x: x.count('hook'))\n",
    "# fill NaN in with 0\n",
    "songs.verse_chorus_ratio.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>old_index</th>\n",
       "      <th>title</th>\n",
       "      <th>artist</th>\n",
       "      <th>date</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>place_words</th>\n",
       "      <th>chorus_count</th>\n",
       "      <th>verse_count</th>\n",
       "      <th>verse_chorus_ratio</th>\n",
       "      <th>pre_chorus_count</th>\n",
       "      <th>outro_count</th>\n",
       "      <th>bridge_count</th>\n",
       "      <th>hook_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2656</th>\n",
       "      <td>3162</td>\n",
       "      <td>Buffalo Stance</td>\n",
       "      <td>Neneh Cherry</td>\n",
       "      <td>1989-04-01</td>\n",
       "      <td>Buffalo Stance Lyrics[Intro] Will you stop thi...</td>\n",
       "      <td>[intro] [verse 1] [interlude] [pre-chorus] [ch...</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4299</th>\n",
       "      <td>5086</td>\n",
       "      <td>Dip It Low</td>\n",
       "      <td>Christina Milian</td>\n",
       "      <td>2004-04-24</td>\n",
       "      <td>Dip It Low Lyrics[Intro: Christina Milian &amp; Fa...</td>\n",
       "      <td>[intro: christina milian &amp; fabolous] [verse 1:...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4377</th>\n",
       "      <td>5175</td>\n",
       "      <td>Do It Like You</td>\n",
       "      <td>Diggy Featuring Jeremih</td>\n",
       "      <td>2012-02-11</td>\n",
       "      <td>Do It Like You Lyrics[Chorus: Jeremih] Hey, th...</td>\n",
       "      <td>[chorus: jeremih] [verse 1: diggy] [chorus: je...</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      old_index           title                   artist        date  \\\n",
       "2656       3162  Buffalo Stance             Neneh Cherry  1989-04-01   \n",
       "4299       5086      Dip It Low         Christina Milian  2004-04-24   \n",
       "4377       5175  Do It Like You  Diggy Featuring Jeremih  2012-02-11   \n",
       "\n",
       "                                                 lyrics  \\\n",
       "2656  Buffalo Stance Lyrics[Intro] Will you stop thi...   \n",
       "4299  Dip It Low Lyrics[Intro: Christina Milian & Fa...   \n",
       "4377  Do It Like You Lyrics[Chorus: Jeremih] Hey, th...   \n",
       "\n",
       "                                            place_words  chorus_count  \\\n",
       "2656  [intro] [verse 1] [interlude] [pre-chorus] [ch...             3   \n",
       "4299  [intro: christina milian & fabolous] [verse 1:...             3   \n",
       "4377  [chorus: jeremih] [verse 1: diggy] [chorus: je...             3   \n",
       "\n",
       "      verse_count  verse_chorus_ratio  pre_chorus_count  outro_count  \\\n",
       "2656            2            0.666667                 2            1   \n",
       "4299            3            1.000000                 0            0   \n",
       "4377            2            0.666667                 0            1   \n",
       "\n",
       "      bridge_count  hook_count  \n",
       "2656             2           0  \n",
       "4299             0           0  \n",
       "4377             0           0  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "songs.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatizing... ********\r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>artist</th>\n",
       "      <th>date</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>raw_lyrics</th>\n",
       "      <th>decade</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#1</td>\n",
       "      <td>Nelly</td>\n",
       "      <td>2001-10-20</td>\n",
       "      <td>uh uh uh got bring attention dirty better watc...</td>\n",
       "      <td>#1 LyricsUh uh uh I just gotta bring it to the...</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#9 Dream</td>\n",
       "      <td>John Lennon</td>\n",
       "      <td>1974-12-21</td>\n",
       "      <td>long ago dream dream know yes know seemed real...</td>\n",
       "      <td>#9 Dream Lyrics[Verse 1] So long ago Was it in...</td>\n",
       "      <td>1970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>#Beautiful</td>\n",
       "      <td>Mariah Carey Featuring Miguel</td>\n",
       "      <td>2013-05-25</td>\n",
       "      <td>ah ah beautiful ah ah beautiful hop back bike ...</td>\n",
       "      <td>#Beautiful Lyrics[Intro: Mariah Carey] Ah, ah,...</td>\n",
       "      <td>2010</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        title                         artist       date  \\\n",
       "2          #1                          Nelly 2001-10-20   \n",
       "4    #9 Dream                    John Lennon 1974-12-21   \n",
       "5  #Beautiful  Mariah Carey Featuring Miguel 2013-05-25   \n",
       "\n",
       "                                              lyrics  \\\n",
       "2  uh uh uh got bring attention dirty better watc...   \n",
       "4  long ago dream dream know yes know seemed real...   \n",
       "5  ah ah beautiful ah ah beautiful hop back bike ...   \n",
       "\n",
       "                                          raw_lyrics  decade  \n",
       "2  #1 LyricsUh uh uh I just gotta bring it to the...    2000  \n",
       "4  #9 Dream Lyrics[Verse 1] So long ago Was it in...    1970  \n",
       "5  #Beautiful Lyrics[Intro: Mariah Carey] Ah, ah,...    2010  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = prep.clean_df(pd.read_csv('songs_0526.csv', index_col=0))\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = prep.get_place_words(df)\n",
    "# print(\"adding chorus count ********\", end=\"\\r\")\n",
    "# df[\"chorus_count\"] = df.place_words.apply(lambda x: x.count(\"[chorus\"))\n",
    "# # add column for verse count\n",
    "# print(\"adding verse count *********\", end=\"\\r\")\n",
    "# df[\"verse_count\"] = df.place_words.apply(lambda x: x.count(\"verse\"))\n",
    "# # if chorus count or verse count is 0, then set ratio to 0\n",
    "# df['verse_chorus_ratio'] = df.verse_count / df.chorus_count\n",
    "# df.loc[(df.chorus_count == 0) | (df.verse_count == 0), 'verse_chorus_ratio'] = 0\n",
    "# df['verse_chorus_ratio'] = df.verse_count / df.chorus_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_verse_chorus_ratio(x):\n",
    "#     if x.chorus_count == 0 or x.verse_count == 0:\n",
    "#         return 0\n",
    "#     else:\n",
    "#         return x.verse_count / x.chorus_count\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['verse_chorus_ratio'] = df.apply(get_verse_chorus_ratio, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>decade</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>23762.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1984.475633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>19.490497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1950.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1970.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1980.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2020.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             decade\n",
       "count  23762.000000\n",
       "mean    1984.475633\n",
       "std       19.490497\n",
       "min     1950.000000\n",
       "25%     1970.000000\n",
       "50%     1980.000000\n",
       "75%     2000.000000\n",
       "max     2020.000000"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features added ******************\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>artist</th>\n",
       "      <th>date</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>raw_lyrics</th>\n",
       "      <th>decade</th>\n",
       "      <th>character_count</th>\n",
       "      <th>word_count</th>\n",
       "      <th>unique_words</th>\n",
       "      <th>unique_words_count</th>\n",
       "      <th>...</th>\n",
       "      <th>place_words</th>\n",
       "      <th>chorus_count</th>\n",
       "      <th>verse_count</th>\n",
       "      <th>verse_chorus_ratio</th>\n",
       "      <th>pre_chorus_count</th>\n",
       "      <th>outro_count</th>\n",
       "      <th>bridge_count</th>\n",
       "      <th>hook_count</th>\n",
       "      <th>bigrams</th>\n",
       "      <th>trigrams</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#1</td>\n",
       "      <td>Nelly</td>\n",
       "      <td>2001-10-20</td>\n",
       "      <td>uh uh uh got bring attention dirty better watc...</td>\n",
       "      <td>#1 LyricsUh uh uh I just gotta bring it to the...</td>\n",
       "      <td>2000</td>\n",
       "      <td>2014</td>\n",
       "      <td>363</td>\n",
       "      <td>learn hiphop uhh album aiyyo mean catch right ...</td>\n",
       "      <td>228</td>\n",
       "      <td>...</td>\n",
       "      <td>[hook] [hook] [hook]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>[(uh, uh), (uh, uh), (uh, got), (got, bring), ...</td>\n",
       "      <td>[(uh, uh, uh), (uh, uh, got), (uh, got, bring)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#9 Dream</td>\n",
       "      <td>John Lennon</td>\n",
       "      <td>1974-12-21</td>\n",
       "      <td>long ago dream dream know yes know seemed real...</td>\n",
       "      <td>#9 Dream Lyrics[Verse 1] So long ago Was it in...</td>\n",
       "      <td>1970</td>\n",
       "      <td>842</td>\n",
       "      <td>135</td>\n",
       "      <td>dancing long call air feel bowakawa go dance s...</td>\n",
       "      <td>49</td>\n",
       "      <td>...</td>\n",
       "      <td>[verse 1] [pre-chorus 1] [chorus] [verse 2] [p...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[(long, ago), (ago, dream), (dream, dream), (d...</td>\n",
       "      <td>[(long, ago, dream), (ago, dream, dream), (dre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>#Beautiful</td>\n",
       "      <td>Mariah Carey Featuring Miguel</td>\n",
       "      <td>2013-05-25</td>\n",
       "      <td>ah ah beautiful ah ah beautiful hop back bike ...</td>\n",
       "      <td>#Beautiful Lyrics[Intro: Mariah Carey] Ah, ah,...</td>\n",
       "      <td>2010</td>\n",
       "      <td>768</td>\n",
       "      <td>129</td>\n",
       "      <td>feel beautiful oh mean stop dance yes yeah nig...</td>\n",
       "      <td>54</td>\n",
       "      <td>...</td>\n",
       "      <td>[intro: mariah carey] [verse 1: miguel] [choru...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[(ah, ah), (ah, beautiful), (beautiful, ah), (...</td>\n",
       "      <td>[(ah, ah, beautiful), (ah, beautiful, ah), (be...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        title                         artist       date  \\\n",
       "2          #1                          Nelly 2001-10-20   \n",
       "4    #9 Dream                    John Lennon 1974-12-21   \n",
       "5  #Beautiful  Mariah Carey Featuring Miguel 2013-05-25   \n",
       "\n",
       "                                              lyrics  \\\n",
       "2  uh uh uh got bring attention dirty better watc...   \n",
       "4  long ago dream dream know yes know seemed real...   \n",
       "5  ah ah beautiful ah ah beautiful hop back bike ...   \n",
       "\n",
       "                                          raw_lyrics  decade  character_count  \\\n",
       "2  #1 LyricsUh uh uh I just gotta bring it to the...    2000             2014   \n",
       "4  #9 Dream Lyrics[Verse 1] So long ago Was it in...    1970              842   \n",
       "5  #Beautiful Lyrics[Intro: Mariah Carey] Ah, ah,...    2010              768   \n",
       "\n",
       "   word_count                                       unique_words  \\\n",
       "2         363  learn hiphop uhh album aiyyo mean catch right ...   \n",
       "4         135  dancing long call air feel bowakawa go dance s...   \n",
       "5         129  feel beautiful oh mean stop dance yes yeah nig...   \n",
       "\n",
       "   unique_words_count  ...                                        place_words  \\\n",
       "2                 228  ...                               [hook] [hook] [hook]   \n",
       "4                  49  ...  [verse 1] [pre-chorus 1] [chorus] [verse 2] [p...   \n",
       "5                  54  ...  [intro: mariah carey] [verse 1: miguel] [choru...   \n",
       "\n",
       "  chorus_count verse_count  verse_chorus_ratio  pre_chorus_count  outro_count  \\\n",
       "2            0           0                 0.0                 0            0   \n",
       "4            2           2                 1.0                 2            1   \n",
       "5            2           2                 1.0                 0            1   \n",
       "\n",
       "   bridge_count  hook_count  \\\n",
       "2             0           3   \n",
       "4             0           0   \n",
       "5             0           0   \n",
       "\n",
       "                                             bigrams  \\\n",
       "2  [(uh, uh), (uh, uh), (uh, got), (got, bring), ...   \n",
       "4  [(long, ago), (ago, dream), (dream, dream), (d...   \n",
       "5  [(ah, ah), (ah, beautiful), (beautiful, ah), (...   \n",
       "\n",
       "                                            trigrams  \n",
       "2  [(uh, uh, uh), (uh, uh, got), (uh, got, bring)...  \n",
       "4  [(long, ago, dream), (ago, dream, dream), (dre...  \n",
       "5  [(ah, ah, beautiful), (ah, beautiful, ah), (be...  \n",
       "\n",
       "[3 rows x 22 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = prep.add_features(df)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>decade</th>\n",
       "      <th>character_count</th>\n",
       "      <th>word_count</th>\n",
       "      <th>unique_words_count</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>chorus_count</th>\n",
       "      <th>verse_count</th>\n",
       "      <th>verse_chorus_ratio</th>\n",
       "      <th>pre_chorus_count</th>\n",
       "      <th>outro_count</th>\n",
       "      <th>bridge_count</th>\n",
       "      <th>hook_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>23762.000000</td>\n",
       "      <td>23762.000000</td>\n",
       "      <td>23762.000000</td>\n",
       "      <td>23762.000000</td>\n",
       "      <td>23762.000000</td>\n",
       "      <td>23762.000000</td>\n",
       "      <td>23762.000000</td>\n",
       "      <td>23762.000000</td>\n",
       "      <td>23762.000000</td>\n",
       "      <td>23762.000000</td>\n",
       "      <td>23762.000000</td>\n",
       "      <td>23762.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1984.475633</td>\n",
       "      <td>850.922355</td>\n",
       "      <td>148.609965</td>\n",
       "      <td>70.219047</td>\n",
       "      <td>0.522191</td>\n",
       "      <td>1.567124</td>\n",
       "      <td>1.495413</td>\n",
       "      <td>0.466402</td>\n",
       "      <td>0.315883</td>\n",
       "      <td>0.315335</td>\n",
       "      <td>0.331664</td>\n",
       "      <td>0.126589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>19.490497</td>\n",
       "      <td>506.248160</td>\n",
       "      <td>89.010090</td>\n",
       "      <td>47.033070</td>\n",
       "      <td>0.746110</td>\n",
       "      <td>1.603118</td>\n",
       "      <td>1.438409</td>\n",
       "      <td>0.548411</td>\n",
       "      <td>0.799630</td>\n",
       "      <td>0.468627</td>\n",
       "      <td>0.564704</td>\n",
       "      <td>0.671409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1950.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.999900</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1970.000000</td>\n",
       "      <td>512.000000</td>\n",
       "      <td>89.000000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>0.318200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1980.000000</td>\n",
       "      <td>719.000000</td>\n",
       "      <td>124.000000</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>0.962800</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2000.000000</td>\n",
       "      <td>1026.000000</td>\n",
       "      <td>179.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>0.992000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2020.000000</td>\n",
       "      <td>10282.000000</td>\n",
       "      <td>1757.000000</td>\n",
       "      <td>906.000000</td>\n",
       "      <td>0.999900</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             decade  character_count    word_count  unique_words_count  \\\n",
       "count  23762.000000     23762.000000  23762.000000        23762.000000   \n",
       "mean    1984.475633       850.922355    148.609965           70.219047   \n",
       "std       19.490497       506.248160     89.010090           47.033070   \n",
       "min     1950.000000         0.000000      0.000000            0.000000   \n",
       "25%     1970.000000       512.000000     89.000000           42.000000   \n",
       "50%     1980.000000       719.000000    124.000000           56.000000   \n",
       "75%     2000.000000      1026.000000    179.000000           80.000000   \n",
       "max     2020.000000     10282.000000   1757.000000          906.000000   \n",
       "\n",
       "          sentiment  chorus_count   verse_count  verse_chorus_ratio  \\\n",
       "count  23762.000000  23762.000000  23762.000000        23762.000000   \n",
       "mean       0.522191      1.567124      1.495413            0.466402   \n",
       "std        0.746110      1.603118      1.438409            0.548411   \n",
       "min       -0.999900      0.000000      0.000000            0.000000   \n",
       "25%        0.318200      0.000000      0.000000            0.000000   \n",
       "50%        0.962800      2.000000      2.000000            0.500000   \n",
       "75%        0.992000      3.000000      2.000000            0.750000   \n",
       "max        0.999900     11.000000     16.000000           13.000000   \n",
       "\n",
       "       pre_chorus_count   outro_count  bridge_count    hook_count  \n",
       "count      23762.000000  23762.000000  23762.000000  23762.000000  \n",
       "mean           0.315883      0.315335      0.331664      0.126589  \n",
       "std            0.799630      0.468627      0.564704      0.671409  \n",
       "min            0.000000      0.000000      0.000000      0.000000  \n",
       "25%            0.000000      0.000000      0.000000      0.000000  \n",
       "50%            0.000000      0.000000      0.000000      0.000000  \n",
       "75%            0.000000      1.000000      1.000000      0.000000  \n",
       "max            6.000000      3.000000      6.000000      9.000000  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngrams(df):\n",
    "    \"\"\"\n",
    "    get bigrams/trigrams for each song\n",
    "    \"\"\"\n",
    "    # create bigram column\n",
    "    print(\"creating bigram column\", end=\"\\r\")\n",
    "    df[\"bigrams\"] = df.lyrics.apply(lambda x: list(nltk.ngrams(x.split(), 2)))\n",
    "    # create bigram column\n",
    "    print(\"creating trigram column\", end=\"\\r\")\n",
    "    df[\"trigrams\"] = df.lyrics.apply(lambda x: list(nltk.ngrams(x.split(), 3)))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating trigram column\r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>artist</th>\n",
       "      <th>date</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>raw_lyrics</th>\n",
       "      <th>decade</th>\n",
       "      <th>character_count</th>\n",
       "      <th>word_count</th>\n",
       "      <th>unique_words</th>\n",
       "      <th>unique_words_count</th>\n",
       "      <th>...</th>\n",
       "      <th>place_words</th>\n",
       "      <th>chorus_count</th>\n",
       "      <th>verse_count</th>\n",
       "      <th>verse_chorus_ratio</th>\n",
       "      <th>pre_chorus_count</th>\n",
       "      <th>outro_count</th>\n",
       "      <th>bridge_count</th>\n",
       "      <th>hook_count</th>\n",
       "      <th>bigrams</th>\n",
       "      <th>trigrams</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#1</td>\n",
       "      <td>Nelly</td>\n",
       "      <td>2001-10-20</td>\n",
       "      <td>uh uh uh got bring attention dirty better watc...</td>\n",
       "      <td>#1 LyricsUh uh uh I just gotta bring it to the...</td>\n",
       "      <td>2000</td>\n",
       "      <td>2014</td>\n",
       "      <td>363</td>\n",
       "      <td>learn hiphop uhh album aiyyo mean catch right ...</td>\n",
       "      <td>228</td>\n",
       "      <td>...</td>\n",
       "      <td>[hook] [hook] [hook]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>[(uh, uh), (uh, uh), (uh, got), (got, bring), ...</td>\n",
       "      <td>[(uh, uh, uh), (uh, uh, got), (uh, got, bring)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#9 Dream</td>\n",
       "      <td>John Lennon</td>\n",
       "      <td>1974-12-21</td>\n",
       "      <td>long ago dream dream know yes know seemed real...</td>\n",
       "      <td>#9 Dream Lyrics[Verse 1] So long ago Was it in...</td>\n",
       "      <td>1970</td>\n",
       "      <td>842</td>\n",
       "      <td>135</td>\n",
       "      <td>dancing long call air feel bowakawa go dance s...</td>\n",
       "      <td>49</td>\n",
       "      <td>...</td>\n",
       "      <td>[verse 1] [pre-chorus 1] [chorus] [verse 2] [p...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[(long, ago), (ago, dream), (dream, dream), (d...</td>\n",
       "      <td>[(long, ago, dream), (ago, dream, dream), (dre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>#Beautiful</td>\n",
       "      <td>Mariah Carey Featuring Miguel</td>\n",
       "      <td>2013-05-25</td>\n",
       "      <td>ah ah beautiful ah ah beautiful hop back bike ...</td>\n",
       "      <td>#Beautiful Lyrics[Intro: Mariah Carey] Ah, ah,...</td>\n",
       "      <td>2010</td>\n",
       "      <td>768</td>\n",
       "      <td>129</td>\n",
       "      <td>feel beautiful oh mean stop dance yes yeah nig...</td>\n",
       "      <td>54</td>\n",
       "      <td>...</td>\n",
       "      <td>[intro: mariah carey] [verse 1: miguel] [choru...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[(ah, ah), (ah, beautiful), (beautiful, ah), (...</td>\n",
       "      <td>[(ah, ah, beautiful), (ah, beautiful, ah), (be...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        title                         artist       date  \\\n",
       "2          #1                          Nelly 2001-10-20   \n",
       "4    #9 Dream                    John Lennon 1974-12-21   \n",
       "5  #Beautiful  Mariah Carey Featuring Miguel 2013-05-25   \n",
       "\n",
       "                                              lyrics  \\\n",
       "2  uh uh uh got bring attention dirty better watc...   \n",
       "4  long ago dream dream know yes know seemed real...   \n",
       "5  ah ah beautiful ah ah beautiful hop back bike ...   \n",
       "\n",
       "                                          raw_lyrics  decade  character_count  \\\n",
       "2  #1 LyricsUh uh uh I just gotta bring it to the...    2000             2014   \n",
       "4  #9 Dream Lyrics[Verse 1] So long ago Was it in...    1970              842   \n",
       "5  #Beautiful Lyrics[Intro: Mariah Carey] Ah, ah,...    2010              768   \n",
       "\n",
       "   word_count                                       unique_words  \\\n",
       "2         363  learn hiphop uhh album aiyyo mean catch right ...   \n",
       "4         135  dancing long call air feel bowakawa go dance s...   \n",
       "5         129  feel beautiful oh mean stop dance yes yeah nig...   \n",
       "\n",
       "   unique_words_count  ...                                        place_words  \\\n",
       "2                 228  ...                               [hook] [hook] [hook]   \n",
       "4                  49  ...  [verse 1] [pre-chorus 1] [chorus] [verse 2] [p...   \n",
       "5                  54  ...  [intro: mariah carey] [verse 1: miguel] [choru...   \n",
       "\n",
       "  chorus_count verse_count  verse_chorus_ratio  pre_chorus_count  outro_count  \\\n",
       "2            0           0                 0.0                 0            0   \n",
       "4            2           2                 1.0                 2            1   \n",
       "5            2           2                 1.0                 0            1   \n",
       "\n",
       "   bridge_count  hook_count  \\\n",
       "2             0           3   \n",
       "4             0           0   \n",
       "5             0           0   \n",
       "\n",
       "                                             bigrams  \\\n",
       "2  [(uh, uh), (uh, uh), (uh, got), (got, bring), ...   \n",
       "4  [(long, ago), (ago, dream), (dream, dream), (d...   \n",
       "5  [(ah, ah), (ah, beautiful), (beautiful, ah), (...   \n",
       "\n",
       "                                            trigrams  \n",
       "2  [(uh, uh, uh), (uh, uh, got), (uh, got, bring)...  \n",
       "4  [(long, ago, dream), (ago, dream, dream), (dre...  \n",
       "5  [(ah, ah, beautiful), (ah, beautiful, ah), (be...  \n",
       "\n",
       "[3 rows x 22 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = get_ngrams(df)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_words(text):\n",
    "    \"\"\"\n",
    "    Get unique words in text\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a list of words separated by a space\n",
    "    words = text.split()\n",
    "    # Create a variable to hold a list of unique words\n",
    "    unique_words = set(words)\n",
    "    # Join unique words list with a new string\n",
    "    new_string = \" \".join(unique_words)\n",
    "    return new_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>artist</th>\n",
       "      <th>date</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>raw_lyrics</th>\n",
       "      <th>decade</th>\n",
       "      <th>character_count</th>\n",
       "      <th>word_count</th>\n",
       "      <th>unique_words</th>\n",
       "      <th>unique_words_count</th>\n",
       "      <th>...</th>\n",
       "      <th>chorus_count</th>\n",
       "      <th>verse_count</th>\n",
       "      <th>verse_chorus_ratio</th>\n",
       "      <th>pre_chorus_count</th>\n",
       "      <th>outro_count</th>\n",
       "      <th>bridge_count</th>\n",
       "      <th>hook_count</th>\n",
       "      <th>bigrams</th>\n",
       "      <th>trigrams</th>\n",
       "      <th>n_unique_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#1</td>\n",
       "      <td>Nelly</td>\n",
       "      <td>2001-10-20</td>\n",
       "      <td>uh uh uh got bring attention dirty better watc...</td>\n",
       "      <td>#1 LyricsUh uh uh I just gotta bring it to the...</td>\n",
       "      <td>2000</td>\n",
       "      <td>2014</td>\n",
       "      <td>363</td>\n",
       "      <td>learn hiphop uhh album aiyyo mean catch right ...</td>\n",
       "      <td>228</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>[(uh, uh), (uh, uh), (uh, got), (got, bring), ...</td>\n",
       "      <td>[(uh, uh, uh), (uh, uh, got), (uh, got, bring)...</td>\n",
       "      <td>228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#9 Dream</td>\n",
       "      <td>John Lennon</td>\n",
       "      <td>1974-12-21</td>\n",
       "      <td>long ago dream dream know yes know seemed real...</td>\n",
       "      <td>#9 Dream Lyrics[Verse 1] So long ago Was it in...</td>\n",
       "      <td>1970</td>\n",
       "      <td>842</td>\n",
       "      <td>135</td>\n",
       "      <td>dancing long call air feel bowakawa go dance s...</td>\n",
       "      <td>49</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[(long, ago), (ago, dream), (dream, dream), (d...</td>\n",
       "      <td>[(long, ago, dream), (ago, dream, dream), (dre...</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>#Beautiful</td>\n",
       "      <td>Mariah Carey Featuring Miguel</td>\n",
       "      <td>2013-05-25</td>\n",
       "      <td>ah ah beautiful ah ah beautiful hop back bike ...</td>\n",
       "      <td>#Beautiful Lyrics[Intro: Mariah Carey] Ah, ah,...</td>\n",
       "      <td>2010</td>\n",
       "      <td>768</td>\n",
       "      <td>129</td>\n",
       "      <td>feel beautiful oh mean stop dance yes yeah nig...</td>\n",
       "      <td>54</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[(ah, ah), (ah, beautiful), (beautiful, ah), (...</td>\n",
       "      <td>[(ah, ah, beautiful), (ah, beautiful, ah), (be...</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        title                         artist       date  \\\n",
       "2          #1                          Nelly 2001-10-20   \n",
       "4    #9 Dream                    John Lennon 1974-12-21   \n",
       "5  #Beautiful  Mariah Carey Featuring Miguel 2013-05-25   \n",
       "\n",
       "                                              lyrics  \\\n",
       "2  uh uh uh got bring attention dirty better watc...   \n",
       "4  long ago dream dream know yes know seemed real...   \n",
       "5  ah ah beautiful ah ah beautiful hop back bike ...   \n",
       "\n",
       "                                          raw_lyrics  decade  character_count  \\\n",
       "2  #1 LyricsUh uh uh I just gotta bring it to the...    2000             2014   \n",
       "4  #9 Dream Lyrics[Verse 1] So long ago Was it in...    1970              842   \n",
       "5  #Beautiful Lyrics[Intro: Mariah Carey] Ah, ah,...    2010              768   \n",
       "\n",
       "   word_count                                       unique_words  \\\n",
       "2         363  learn hiphop uhh album aiyyo mean catch right ...   \n",
       "4         135  dancing long call air feel bowakawa go dance s...   \n",
       "5         129  feel beautiful oh mean stop dance yes yeah nig...   \n",
       "\n",
       "   unique_words_count  ...  chorus_count verse_count verse_chorus_ratio  \\\n",
       "2                 228  ...             0           0                0.0   \n",
       "4                  49  ...             2           2                1.0   \n",
       "5                  54  ...             2           2                1.0   \n",
       "\n",
       "   pre_chorus_count  outro_count  bridge_count  hook_count  \\\n",
       "2                 0            0             0           3   \n",
       "4                 2            1             0           0   \n",
       "5                 0            1             0           0   \n",
       "\n",
       "                                             bigrams  \\\n",
       "2  [(uh, uh), (uh, uh), (uh, got), (got, bring), ...   \n",
       "4  [(long, ago), (ago, dream), (dream, dream), (d...   \n",
       "5  [(ah, ah), (ah, beautiful), (beautiful, ah), (...   \n",
       "\n",
       "                                            trigrams  n_unique_words  \n",
       "2  [(uh, uh, uh), (uh, uh, got), (uh, got, bring)...             228  \n",
       "4  [(long, ago, dream), (ago, dream, dream), (dre...              49  \n",
       "5  [(ah, ah, beautiful), (ah, beautiful, ah), (be...              54  \n",
       "\n",
       "[3 rows x 23 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['unique_words'] = df.lyrics.apply(get_unique_words)\n",
    "df['n_unique_words'] = df.unique_words.apply(lambda x: len(x.split()))\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features added ******************\n"
     ]
    }
   ],
   "source": [
    "df= prep.get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>artist</th>\n",
       "      <th>date</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>raw_lyrics</th>\n",
       "      <th>decade</th>\n",
       "      <th>character_count</th>\n",
       "      <th>word_count</th>\n",
       "      <th>unique_words</th>\n",
       "      <th>unique_words_count</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>sentiment_category</th>\n",
       "      <th>place_words</th>\n",
       "      <th>chorus_count</th>\n",
       "      <th>verse_count</th>\n",
       "      <th>verse_chorus_ratio</th>\n",
       "      <th>pre_chorus_count</th>\n",
       "      <th>outro_count</th>\n",
       "      <th>bridge_count</th>\n",
       "      <th>hook_count</th>\n",
       "      <th>bigrams</th>\n",
       "      <th>trigrams</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#1</td>\n",
       "      <td>Nelly</td>\n",
       "      <td>2001-10-20</td>\n",
       "      <td>uh uh uh got bring attention dirty better watc...</td>\n",
       "      <td>#1 LyricsUh uh uh I just gotta bring it to the...</td>\n",
       "      <td>2000</td>\n",
       "      <td>2014</td>\n",
       "      <td>363</td>\n",
       "      <td>learn hiphop uhh album aiyyo mean catch right ...</td>\n",
       "      <td>228</td>\n",
       "      <td>0.9915</td>\n",
       "      <td>very positive</td>\n",
       "      <td>[hook] [hook] [hook]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>[(uh, uh), (uh, uh), (uh, got), (got, bring), ...</td>\n",
       "      <td>[(uh, uh, uh), (uh, uh, got), (uh, got, bring)...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  title artist       date                                             lyrics  \\\n",
       "0    #1  Nelly 2001-10-20  uh uh uh got bring attention dirty better watc...   \n",
       "\n",
       "                                          raw_lyrics  decade  character_count  \\\n",
       "0  #1 LyricsUh uh uh I just gotta bring it to the...    2000             2014   \n",
       "\n",
       "   word_count                                       unique_words  \\\n",
       "0         363  learn hiphop uhh album aiyyo mean catch right ...   \n",
       "\n",
       "   unique_words_count  sentiment sentiment_category           place_words  \\\n",
       "0                 228     0.9915      very positive  [hook] [hook] [hook]   \n",
       "\n",
       "   chorus_count  verse_count  verse_chorus_ratio  pre_chorus_count  \\\n",
       "0             0            0                 0.0                 0   \n",
       "\n",
       "   outro_count  bridge_count  hook_count  \\\n",
       "0            0             0           3   \n",
       "\n",
       "                                             bigrams  \\\n",
       "0  [(uh, uh), (uh, uh), (uh, got), (got, bring), ...   \n",
       "\n",
       "                                            trigrams  \n",
       "0  [(uh, uh, uh), (uh, uh, got), (uh, got, bring)...  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('max_columns', 50)\n",
    "df.head(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_common_unique_words(text, threshold=5):\n",
    "    \"\"\"\n",
    "    Get common unique words in dataframe, aka words that occur in multiple readme's\n",
    "    a word must appear in at least threshold readmes to be considered a common word\n",
    "    \"\"\"\n",
    "    #splitting text into individual words\n",
    "    words = text.split()\n",
    "    #using collections to get an ngrams count \n",
    "    counter = Counter(words)\n",
    "    #Create a list with all of the unique words using our default threshold of at least 5\n",
    "    common_unique_words = [\n",
    "        word for word, count in counter.items() if count >= threshold\n",
    "    ]\n",
    "    #Join the list with a new string\n",
    "    new_string = \" \".join(common_unique_words)\n",
    "    return new_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_most_common_word(string, n=1):\n",
    "    \"\"\"\n",
    "    Return the nth most common word in a string\n",
    "    \"\"\"\n",
    "    # Create a list of words separated by a space\n",
    "    words = string.split()\n",
    "    # Make an if statement that will only show the nth most common word based on the value set for n\n",
    "    if len(words) < n:\n",
    "        return \"\"\n",
    "    # Use collections to get an ngram count\n",
    "    word_counts = Counter(words)\n",
    "    # Return only the most common\n",
    "    return word_counts.most_common(n)[n - 1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topics added                        \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>artist</th>\n",
       "      <th>date</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>raw_lyrics</th>\n",
       "      <th>decade</th>\n",
       "      <th>character_count</th>\n",
       "      <th>word_count</th>\n",
       "      <th>unique_words</th>\n",
       "      <th>unique_words_count</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>sentiment_category</th>\n",
       "      <th>place_words</th>\n",
       "      <th>chorus_count</th>\n",
       "      <th>verse_count</th>\n",
       "      <th>verse_chorus_ratio</th>\n",
       "      <th>pre_chorus_count</th>\n",
       "      <th>outro_count</th>\n",
       "      <th>bridge_count</th>\n",
       "      <th>hook_count</th>\n",
       "      <th>bigrams</th>\n",
       "      <th>trigrams</th>\n",
       "      <th>topic_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#1</td>\n",
       "      <td>Nelly</td>\n",
       "      <td>2001-10-20</td>\n",
       "      <td>uh uh uh got bring attention dirty better watc...</td>\n",
       "      <td>#1 LyricsUh uh uh I just gotta bring it to the...</td>\n",
       "      <td>2000</td>\n",
       "      <td>2014</td>\n",
       "      <td>363</td>\n",
       "      <td>learn hiphop uhh album aiyyo mean catch right ...</td>\n",
       "      <td>228</td>\n",
       "      <td>0.9915</td>\n",
       "      <td>very positive</td>\n",
       "      <td>[hook] [hook] [hook]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>[(uh, uh), (uh, uh), (uh, got), (got, bring), ...</td>\n",
       "      <td>[(uh, uh, uh), (uh, uh, got), (uh, got, bring)...</td>\n",
       "      <td>sex</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#9 Dream</td>\n",
       "      <td>John Lennon</td>\n",
       "      <td>1974-12-21</td>\n",
       "      <td>long ago dream dream know yes know seemed real...</td>\n",
       "      <td>#9 Dream Lyrics[Verse 1] So long ago Was it in...</td>\n",
       "      <td>1970</td>\n",
       "      <td>842</td>\n",
       "      <td>135</td>\n",
       "      <td>dancing long call air feel bowakawa go dance s...</td>\n",
       "      <td>49</td>\n",
       "      <td>0.9169</td>\n",
       "      <td>very positive</td>\n",
       "      <td>[verse 1] [pre-chorus 1] [chorus] [verse 2] [p...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[(long, ago), (ago, dream), (dream, dream), (d...</td>\n",
       "      <td>[(long, ago, dream), (ago, dream, dream), (dre...</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#Beautiful</td>\n",
       "      <td>Mariah Carey Featuring Miguel</td>\n",
       "      <td>2013-05-25</td>\n",
       "      <td>ah ah beautiful ah ah beautiful hop back bike ...</td>\n",
       "      <td>#Beautiful Lyrics[Intro: Mariah Carey] Ah, ah,...</td>\n",
       "      <td>2010</td>\n",
       "      <td>768</td>\n",
       "      <td>129</td>\n",
       "      <td>feel beautiful oh mean stop dance yes yeah nig...</td>\n",
       "      <td>54</td>\n",
       "      <td>0.9989</td>\n",
       "      <td>very positive</td>\n",
       "      <td>[intro: mariah carey] [verse 1: miguel] [choru...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[(ah, ah), (ah, beautiful), (beautiful, ah), (...</td>\n",
       "      <td>[(ah, ah, beautiful), (ah, beautiful, ah), (be...</td>\n",
       "      <td>lost</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        title                         artist       date  \\\n",
       "0          #1                          Nelly 2001-10-20   \n",
       "1    #9 Dream                    John Lennon 1974-12-21   \n",
       "2  #Beautiful  Mariah Carey Featuring Miguel 2013-05-25   \n",
       "\n",
       "                                              lyrics  \\\n",
       "0  uh uh uh got bring attention dirty better watc...   \n",
       "1  long ago dream dream know yes know seemed real...   \n",
       "2  ah ah beautiful ah ah beautiful hop back bike ...   \n",
       "\n",
       "                                          raw_lyrics  decade  character_count  \\\n",
       "0  #1 LyricsUh uh uh I just gotta bring it to the...    2000             2014   \n",
       "1  #9 Dream Lyrics[Verse 1] So long ago Was it in...    1970              842   \n",
       "2  #Beautiful Lyrics[Intro: Mariah Carey] Ah, ah,...    2010              768   \n",
       "\n",
       "   word_count                                       unique_words  \\\n",
       "0         363  learn hiphop uhh album aiyyo mean catch right ...   \n",
       "1         135  dancing long call air feel bowakawa go dance s...   \n",
       "2         129  feel beautiful oh mean stop dance yes yeah nig...   \n",
       "\n",
       "   unique_words_count  sentiment sentiment_category  \\\n",
       "0                 228     0.9915      very positive   \n",
       "1                  49     0.9169      very positive   \n",
       "2                  54     0.9989      very positive   \n",
       "\n",
       "                                         place_words  chorus_count  \\\n",
       "0                               [hook] [hook] [hook]             0   \n",
       "1  [verse 1] [pre-chorus 1] [chorus] [verse 2] [p...             2   \n",
       "2  [intro: mariah carey] [verse 1: miguel] [choru...             2   \n",
       "\n",
       "   verse_count  verse_chorus_ratio  pre_chorus_count  outro_count  \\\n",
       "0            0                 0.0                 0            0   \n",
       "1            2                 1.0                 2            1   \n",
       "2            2                 1.0                 0            1   \n",
       "\n",
       "   bridge_count  hook_count  \\\n",
       "0             0           3   \n",
       "1             0           0   \n",
       "2             0           0   \n",
       "\n",
       "                                             bigrams  \\\n",
       "0  [(uh, uh), (uh, uh), (uh, got), (got, bring), ...   \n",
       "1  [(long, ago), (ago, dream), (dream, dream), (d...   \n",
       "2  [(ah, ah), (ah, beautiful), (beautiful, ah), (...   \n",
       "\n",
       "                                            trigrams topic_name  \n",
       "0  [(uh, uh, uh), (uh, uh, got), (uh, got, bring)...        sex  \n",
       "1  [(long, ago, dream), (ago, dream, dream), (dre...       love  \n",
       "2  [(ah, ah, beautiful), (ah, beautiful, ah), (be...       lost  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = prep.get_topics(df)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "breakups          4831\n",
       "lost              3237\n",
       "affection         2815\n",
       "sex               1790\n",
       "nature            1756\n",
       "love              1735\n",
       "good vibes        1284\n",
       "americana         1211\n",
       "youth             1116\n",
       "violence          1066\n",
       "money              672\n",
       "dance              653\n",
       "transcendental     550\n",
       "heartache          339\n",
       "jealousy           259\n",
       "holiday            227\n",
       "spanish            221\n",
       "Name: topic_name, dtype: int64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.topic_name.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine all strings in lyrics where decade is the same\n",
    "decades = (\n",
    "    df.groupby(\"decade\")[\"lyrics\"].apply(lambda x: \" \".join(x)).reset_index()\n",
    ")\n",
    "decades.rename(columns={\"lyrics\": \"all_words\"}, inplace=True)\n",
    "decades[\"unique_words\"] = (\n",
    "    df.groupby(\"decade\")[\"unique_words\"].apply(lambda x: \" \".join(x)).values\n",
    ")\n",
    "decades[\"common_unique_words\"] = decades.unique_words.apply(get_common_unique_words)\n",
    "decades[\"n_words\"] = decades[\"all_words\"].apply(lambda x: len(x.split()))\n",
    "decades[\"unique_word_count\"] = decades[\"all_words\"].apply(\n",
    "    lambda x: len(set(x.split()))\n",
    ")\n",
    "decades[\"mean_word_count\"] = (\n",
    "    df.groupby(\"decade\")[\"word_count\"].mean().values.round(1)\n",
    ")\n",
    "decades[\"median_word_count\"] = (\n",
    "    df.groupby(\"decade\")[\"word_count\"].median().values.round(1)\n",
    ")\n",
    "decades[\"most_common_word\"] = decades[\"unique_words\"].apply(\n",
    "    lambda x: n_most_common_word(x)\n",
    ")\n",
    "decades[\"2nd_most_common_word\"] = decades[\"unique_words\"].apply(\n",
    "    lambda x: n_most_common_word(x, 2)\n",
    ")\n",
    "decades[\"3rd_most_common_word\"] = decades[\"unique_words\"].apply(\n",
    "    lambda x: n_most_common_word(x, 3)\n",
    ")\n",
    "decades[\"4th_most_common_word\"] = decades[\"unique_words\"].apply(\n",
    "    lambda x: n_most_common_word(x, 4)\n",
    ")\n",
    "decades[\"5th_most_common_word\"] = decades[\"unique_words\"].apply(\n",
    "    lambda x: n_most_common_word(x, 5)\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>decade</th>\n",
       "      <th>all_words</th>\n",
       "      <th>unique_words</th>\n",
       "      <th>common_unique_words</th>\n",
       "      <th>n_words</th>\n",
       "      <th>unique_word_count</th>\n",
       "      <th>mean_word_count</th>\n",
       "      <th>median_word_count</th>\n",
       "      <th>most_common_word</th>\n",
       "      <th>2nd_most_common_word</th>\n",
       "      <th>3rd_most_common_word</th>\n",
       "      <th>4th_most_common_word</th>\n",
       "      <th>5th_most_common_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1950</td>\n",
       "      <td>never felt like kissed ya exist kissed ya neve...</td>\n",
       "      <td>oh live yeah til way life mind realize since m...</td>\n",
       "      <td>oh live yeah til way life mind realize since m...</td>\n",
       "      <td>43809</td>\n",
       "      <td>4697</td>\n",
       "      <td>84.7</td>\n",
       "      <td>72.0</td>\n",
       "      <td>love</td>\n",
       "      <td>know</td>\n",
       "      <td>heart</td>\n",
       "      <td>come</td>\n",
       "      <td>oh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1960</td>\n",
       "      <td>till moon desert sky till sea run dry till wor...</td>\n",
       "      <td>grows upstream oh give live flow dream desert ...</td>\n",
       "      <td>grows oh give live flow dream desert river run...</td>\n",
       "      <td>414717</td>\n",
       "      <td>14120</td>\n",
       "      <td>90.8</td>\n",
       "      <td>85.0</td>\n",
       "      <td>love</td>\n",
       "      <td>know</td>\n",
       "      <td>oh</td>\n",
       "      <td>got</td>\n",
       "      <td>go</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1970</td>\n",
       "      <td>long ago dream dream know yes know seemed real...</td>\n",
       "      <td>dancing long call air feel bowakawa go dance s...</td>\n",
       "      <td>dancing long call air feel go dance strange th...</td>\n",
       "      <td>457047</td>\n",
       "      <td>14059</td>\n",
       "      <td>113.9</td>\n",
       "      <td>105.0</td>\n",
       "      <td>love</td>\n",
       "      <td>know</td>\n",
       "      <td>got</td>\n",
       "      <td>oh</td>\n",
       "      <td>like</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1980</td>\n",
       "      <td>car hop bebop sang wop diddy wop diddy wop doo...</td>\n",
       "      <td>oh doo nowhere go give woo end yeah bad night ...</td>\n",
       "      <td>oh doo nowhere go give woo end yeah bad night ...</td>\n",
       "      <td>451583</td>\n",
       "      <td>13486</td>\n",
       "      <td>125.2</td>\n",
       "      <td>117.0</td>\n",
       "      <td>know</td>\n",
       "      <td>love</td>\n",
       "      <td>time</td>\n",
       "      <td>got</td>\n",
       "      <td>like</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1990</td>\n",
       "      <td>love gave heart best keep satisfied took love ...</td>\n",
       "      <td>feel someone find inside give cruel deny shine...</td>\n",
       "      <td>feel someone find inside give cruel deny shine...</td>\n",
       "      <td>492327</td>\n",
       "      <td>19391</td>\n",
       "      <td>172.4</td>\n",
       "      <td>144.0</td>\n",
       "      <td>know</td>\n",
       "      <td>love</td>\n",
       "      <td>like</td>\n",
       "      <td>want</td>\n",
       "      <td>time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2000</td>\n",
       "      <td>uh uh uh got bring attention dirty better watc...</td>\n",
       "      <td>learn hiphop uhh album aiyyo mean catch right ...</td>\n",
       "      <td>learn hiphop uhh album aiyyo mean catch right ...</td>\n",
       "      <td>616267</td>\n",
       "      <td>21487</td>\n",
       "      <td>194.4</td>\n",
       "      <td>162.5</td>\n",
       "      <td>know</td>\n",
       "      <td>like</td>\n",
       "      <td>got</td>\n",
       "      <td>want</td>\n",
       "      <td>get</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2010</td>\n",
       "      <td>ah ah beautiful ah ah beautiful hop back bike ...</td>\n",
       "      <td>feel beautiful oh mean stop dance yes yeah nig...</td>\n",
       "      <td>feel beautiful oh mean stop dance yes yeah nig...</td>\n",
       "      <td>819659</td>\n",
       "      <td>26068</td>\n",
       "      <td>205.0</td>\n",
       "      <td>184.0</td>\n",
       "      <td>like</td>\n",
       "      <td>know</td>\n",
       "      <td>got</td>\n",
       "      <td>get</td>\n",
       "      <td>want</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2020</td>\n",
       "      <td>tell old man largemouth fishin another time go...</td>\n",
       "      <td>may dream rain show chanc somebody ask wait br...</td>\n",
       "      <td>may dream rain show somebody ask wait bring ch...</td>\n",
       "      <td>235861</td>\n",
       "      <td>14865</td>\n",
       "      <td>227.9</td>\n",
       "      <td>211.0</td>\n",
       "      <td>like</td>\n",
       "      <td>got</td>\n",
       "      <td>know</td>\n",
       "      <td>get</td>\n",
       "      <td>yeah</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   decade                                          all_words  \\\n",
       "0    1950  never felt like kissed ya exist kissed ya neve...   \n",
       "1    1960  till moon desert sky till sea run dry till wor...   \n",
       "2    1970  long ago dream dream know yes know seemed real...   \n",
       "3    1980  car hop bebop sang wop diddy wop diddy wop doo...   \n",
       "4    1990  love gave heart best keep satisfied took love ...   \n",
       "5    2000  uh uh uh got bring attention dirty better watc...   \n",
       "6    2010  ah ah beautiful ah ah beautiful hop back bike ...   \n",
       "7    2020  tell old man largemouth fishin another time go...   \n",
       "\n",
       "                                        unique_words  \\\n",
       "0  oh live yeah til way life mind realize since m...   \n",
       "1  grows upstream oh give live flow dream desert ...   \n",
       "2  dancing long call air feel bowakawa go dance s...   \n",
       "3  oh doo nowhere go give woo end yeah bad night ...   \n",
       "4  feel someone find inside give cruel deny shine...   \n",
       "5  learn hiphop uhh album aiyyo mean catch right ...   \n",
       "6  feel beautiful oh mean stop dance yes yeah nig...   \n",
       "7  may dream rain show chanc somebody ask wait br...   \n",
       "\n",
       "                                 common_unique_words  n_words  \\\n",
       "0  oh live yeah til way life mind realize since m...    43809   \n",
       "1  grows oh give live flow dream desert river run...   414717   \n",
       "2  dancing long call air feel go dance strange th...   457047   \n",
       "3  oh doo nowhere go give woo end yeah bad night ...   451583   \n",
       "4  feel someone find inside give cruel deny shine...   492327   \n",
       "5  learn hiphop uhh album aiyyo mean catch right ...   616267   \n",
       "6  feel beautiful oh mean stop dance yes yeah nig...   819659   \n",
       "7  may dream rain show somebody ask wait bring ch...   235861   \n",
       "\n",
       "   unique_word_count  mean_word_count  median_word_count most_common_word  \\\n",
       "0               4697             84.7               72.0             love   \n",
       "1              14120             90.8               85.0             love   \n",
       "2              14059            113.9              105.0             love   \n",
       "3              13486            125.2              117.0             know   \n",
       "4              19391            172.4              144.0             know   \n",
       "5              21487            194.4              162.5             know   \n",
       "6              26068            205.0              184.0             like   \n",
       "7              14865            227.9              211.0             like   \n",
       "\n",
       "  2nd_most_common_word 3rd_most_common_word 4th_most_common_word  \\\n",
       "0                 know                heart                 come   \n",
       "1                 know                   oh                  got   \n",
       "2                 know                  got                   oh   \n",
       "3                 love                 time                  got   \n",
       "4                 love                 like                 want   \n",
       "5                 like                  got                 want   \n",
       "6                 know                  got                  get   \n",
       "7                  got                 know                  get   \n",
       "\n",
       "  5th_most_common_word  \n",
       "0                   oh  \n",
       "1                   go  \n",
       "2                 like  \n",
       "3                 like  \n",
       "4                 time  \n",
       "5                  get  \n",
       "6                 want  \n",
       "7                 yeah  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decades.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1950, 1960, 1970, 1980, 1990, 2000, 2010, 2020])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decades.decade.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #add columns to decades dataframe for topic name counts\n",
    "# for topic in df.topic_name.unique():\n",
    "#     decades[f'{topic}_count'] = decades.decade.apply(lambda x: df[(df.decade == x) & (df.topic_name == topic)].shape[0])\n",
    "#     decades[f'{topic}_percent'] = decades.decade.apply(lambda x: df[(df.decade == x) & (df.topic_name == topic)].shape[0]/df[df.decade == x].shape[0] *100)# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # make a list of all the columns that have percent in the name\n",
    "# percent_cols = [col for col in decades.columns if \"percent\" in col]\n",
    "# decades_percent = decades[percent_cols]\n",
    "# decades_percent.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# feature engineering modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features added ******************\n",
      "topics added                        \n"
     ]
    }
   ],
   "source": [
    "# get fresh df\n",
    "df = prep.get_data()\n",
    "# add topics\n",
    "df = prep.get_topics(df)\n",
    "\n",
    "#drop incomplete decades for modeling\n",
    "# remove incomplete decades (1950, 2020)\n",
    "df = df[(df.decade != 1950) & (df.decade != 2020)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>artist</th>\n",
       "      <th>date</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>raw_lyrics</th>\n",
       "      <th>decade</th>\n",
       "      <th>character_count</th>\n",
       "      <th>word_count</th>\n",
       "      <th>unique_words</th>\n",
       "      <th>unique_words_count</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>sentiment_category</th>\n",
       "      <th>place_words</th>\n",
       "      <th>chorus_count</th>\n",
       "      <th>verse_count</th>\n",
       "      <th>verse_chorus_ratio</th>\n",
       "      <th>pre_chorus_count</th>\n",
       "      <th>outro_count</th>\n",
       "      <th>bridge_count</th>\n",
       "      <th>hook_count</th>\n",
       "      <th>bigrams</th>\n",
       "      <th>trigrams</th>\n",
       "      <th>topic_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#1</td>\n",
       "      <td>Nelly</td>\n",
       "      <td>2001-10-20</td>\n",
       "      <td>uh uh uh got bring attention dirty better watc...</td>\n",
       "      <td>#1 LyricsUh uh uh I just gotta bring it to the...</td>\n",
       "      <td>2000</td>\n",
       "      <td>2014</td>\n",
       "      <td>363</td>\n",
       "      <td>learn hiphop uhh album aiyyo mean catch right ...</td>\n",
       "      <td>228</td>\n",
       "      <td>0.9915</td>\n",
       "      <td>very positive</td>\n",
       "      <td>[hook] [hook] [hook]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>[(uh, uh), (uh, uh), (uh, got), (got, bring), ...</td>\n",
       "      <td>[(uh, uh, uh), (uh, uh, got), (uh, got, bring)...</td>\n",
       "      <td>sex</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#9 Dream</td>\n",
       "      <td>John Lennon</td>\n",
       "      <td>1974-12-21</td>\n",
       "      <td>long ago dream dream know yes know seemed real...</td>\n",
       "      <td>#9 Dream Lyrics[Verse 1] So long ago Was it in...</td>\n",
       "      <td>1970</td>\n",
       "      <td>842</td>\n",
       "      <td>135</td>\n",
       "      <td>dancing long call air feel bowakawa go dance s...</td>\n",
       "      <td>49</td>\n",
       "      <td>0.9169</td>\n",
       "      <td>very positive</td>\n",
       "      <td>[verse 1] [pre-chorus 1] [chorus] [verse 2] [p...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[(long, ago), (ago, dream), (dream, dream), (d...</td>\n",
       "      <td>[(long, ago, dream), (ago, dream, dream), (dre...</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#Beautiful</td>\n",
       "      <td>Mariah Carey Featuring Miguel</td>\n",
       "      <td>2013-05-25</td>\n",
       "      <td>ah ah beautiful ah ah beautiful hop back bike ...</td>\n",
       "      <td>#Beautiful Lyrics[Intro: Mariah Carey] Ah, ah,...</td>\n",
       "      <td>2010</td>\n",
       "      <td>768</td>\n",
       "      <td>129</td>\n",
       "      <td>feel beautiful oh mean stop dance yes yeah nig...</td>\n",
       "      <td>54</td>\n",
       "      <td>0.9989</td>\n",
       "      <td>very positive</td>\n",
       "      <td>[intro: mariah carey] [verse 1: miguel] [choru...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[(ah, ah), (ah, beautiful), (beautiful, ah), (...</td>\n",
       "      <td>[(ah, ah, beautiful), (ah, beautiful, ah), (be...</td>\n",
       "      <td>lost</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#SELFIE</td>\n",
       "      <td>The Chainsmokers</td>\n",
       "      <td>2014-03-15</td>\n",
       "      <td>jason table kept seeing look girl think make j...</td>\n",
       "      <td>#SELFIE Lyrics[Verse 1] When Jason was at the ...</td>\n",
       "      <td>2010</td>\n",
       "      <td>954</td>\n",
       "      <td>172</td>\n",
       "      <td>feel dance first shot valencia throw tacky wai...</td>\n",
       "      <td>100</td>\n",
       "      <td>0.9218</td>\n",
       "      <td>very positive</td>\n",
       "      <td>[verse 1] [drop 1] [verse 2] [drop 2] [verse 3...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[(jason, table), (table, kept), (kept, seeing)...</td>\n",
       "      <td>[(jason, table, kept), (table, kept, seeing), ...</td>\n",
       "      <td>violence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#thatPOWER</td>\n",
       "      <td>will.i.am Featuring Justin Bieber</td>\n",
       "      <td>2013-04-06</td>\n",
       "      <td>oh alive alive alive oh fly fly fly oh alive a...</td>\n",
       "      <td>#thatPOWER Lyrics[Instrumental break]  [Pre-Ch...</td>\n",
       "      <td>2010</td>\n",
       "      <td>1238</td>\n",
       "      <td>208</td>\n",
       "      <td>high fresh fire chest bank yyyyes hatin burnin...</td>\n",
       "      <td>77</td>\n",
       "      <td>0.9984</td>\n",
       "      <td>very positive</td>\n",
       "      <td>[instrumental break] [pre-chorus: justin biebe...</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[(oh, alive), (alive, alive), (alive, alive), ...</td>\n",
       "      <td>[(oh, alive, alive), (alive, alive, alive), (a...</td>\n",
       "      <td>sex</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        title                             artist       date  \\\n",
       "0          #1                              Nelly 2001-10-20   \n",
       "1    #9 Dream                        John Lennon 1974-12-21   \n",
       "2  #Beautiful      Mariah Carey Featuring Miguel 2013-05-25   \n",
       "3     #SELFIE                   The Chainsmokers 2014-03-15   \n",
       "4  #thatPOWER  will.i.am Featuring Justin Bieber 2013-04-06   \n",
       "\n",
       "                                              lyrics  \\\n",
       "0  uh uh uh got bring attention dirty better watc...   \n",
       "1  long ago dream dream know yes know seemed real...   \n",
       "2  ah ah beautiful ah ah beautiful hop back bike ...   \n",
       "3  jason table kept seeing look girl think make j...   \n",
       "4  oh alive alive alive oh fly fly fly oh alive a...   \n",
       "\n",
       "                                          raw_lyrics  decade  character_count  \\\n",
       "0  #1 LyricsUh uh uh I just gotta bring it to the...    2000             2014   \n",
       "1  #9 Dream Lyrics[Verse 1] So long ago Was it in...    1970              842   \n",
       "2  #Beautiful Lyrics[Intro: Mariah Carey] Ah, ah,...    2010              768   \n",
       "3  #SELFIE Lyrics[Verse 1] When Jason was at the ...    2010              954   \n",
       "4  #thatPOWER Lyrics[Instrumental break]  [Pre-Ch...    2010             1238   \n",
       "\n",
       "   word_count                                       unique_words  \\\n",
       "0         363  learn hiphop uhh album aiyyo mean catch right ...   \n",
       "1         135  dancing long call air feel bowakawa go dance s...   \n",
       "2         129  feel beautiful oh mean stop dance yes yeah nig...   \n",
       "3         172  feel dance first shot valencia throw tacky wai...   \n",
       "4         208  high fresh fire chest bank yyyyes hatin burnin...   \n",
       "\n",
       "   unique_words_count  sentiment sentiment_category  \\\n",
       "0                 228     0.9915      very positive   \n",
       "1                  49     0.9169      very positive   \n",
       "2                  54     0.9989      very positive   \n",
       "3                 100     0.9218      very positive   \n",
       "4                  77     0.9984      very positive   \n",
       "\n",
       "                                         place_words  chorus_count  \\\n",
       "0                               [hook] [hook] [hook]             0   \n",
       "1  [verse 1] [pre-chorus 1] [chorus] [verse 2] [p...             2   \n",
       "2  [intro: mariah carey] [verse 1: miguel] [choru...             2   \n",
       "3  [verse 1] [drop 1] [verse 2] [drop 2] [verse 3...             0   \n",
       "4  [instrumental break] [pre-chorus: justin biebe...             3   \n",
       "\n",
       "   verse_count  verse_chorus_ratio  pre_chorus_count  outro_count  \\\n",
       "0            0            0.000000                 0            0   \n",
       "1            2            1.000000                 2            1   \n",
       "2            2            1.000000                 0            1   \n",
       "3            3            0.000000                 0            0   \n",
       "4            2            0.666667                 3            1   \n",
       "\n",
       "   bridge_count  hook_count  \\\n",
       "0             0           3   \n",
       "1             0           0   \n",
       "2             0           0   \n",
       "3             0           0   \n",
       "4             1           0   \n",
       "\n",
       "                                             bigrams  \\\n",
       "0  [(uh, uh), (uh, uh), (uh, got), (got, bring), ...   \n",
       "1  [(long, ago), (ago, dream), (dream, dream), (d...   \n",
       "2  [(ah, ah), (ah, beautiful), (beautiful, ah), (...   \n",
       "3  [(jason, table), (table, kept), (kept, seeing)...   \n",
       "4  [(oh, alive), (alive, alive), (alive, alive), ...   \n",
       "\n",
       "                                            trigrams topic_name  \n",
       "0  [(uh, uh, uh), (uh, uh, got), (uh, got, bring)...        sex  \n",
       "1  [(long, ago, dream), (ago, dream, dream), (dre...       love  \n",
       "2  [(ah, ah, beautiful), (ah, beautiful, ah), (be...       lost  \n",
       "3  [(jason, table, kept), (table, kept, seeing), ...   violence  \n",
       "4  [(oh, alive, alive), (alive, alive, alive), (a...        sex  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train validate and test\n",
    "train, validate, test = prep.split_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_common_unique_words(text, threshold=5):\n",
    "    \"\"\"\n",
    "    Get common unique words in dataframe, aka words that occur in multiple readme's\n",
    "    a word must appear in at least threshold readmes to be considered a common word\n",
    "    \"\"\"\n",
    "\n",
    "    words = text.split()\n",
    "    counter = Counter(words)\n",
    "    common_unique_words = [\n",
    "        word for word, count in counter.items() if count >= threshold\n",
    "    ]\n",
    "    new_string = \" \".join(common_unique_words)\n",
    "    return new_string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def n_most_common_word(string, n=1):\n",
    "    \"\"\"\n",
    "    Return the nth most common word in a string\n",
    "    \"\"\"\n",
    "    # Create a list of words separated by a space\n",
    "    words = string.split()\n",
    "    # Make an if statement that will only show the nth most common word based on the value set for n\n",
    "    if len(words) < n:\n",
    "        return \"\"\n",
    "    # Use collections to get an ngram count\n",
    "    word_counts = Counter(words)\n",
    "    # Return only the most common\n",
    "    return word_counts.most_common(n)[n - 1][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine all strings in lyrics where decade is the same\n",
    "decades = train.groupby(\"decade\")[\"lyrics\"].apply(lambda x: \" \".join(x)).reset_index()\n",
    "decades.rename(columns={\"lyrics\": \"all_words\"}, inplace=True)\n",
    "decades[\"unique_words\"] = (\n",
    "    train.groupby(\"decade\")[\"unique_words\"].apply(lambda x: \" \".join(x)).values\n",
    ")\n",
    "decades[\"common_unique_words\"] = decades.unique_words.apply(get_common_unique_words)\n",
    "decades[\"n_words\"] = decades[\"all_words\"].apply(lambda x: len(x.split()))\n",
    "decades[\"unique_word_count\"] = decades[\"all_words\"].apply(lambda x: len(set(x.split())))\n",
    "decades[\"mean_word_count\"] = (\n",
    "    train.groupby(\"decade\")[\"word_count\"].mean().values.round(1)\n",
    ")\n",
    "decades[\"median_word_count\"] = (\n",
    "    train.groupby(\"decade\")[\"word_count\"].median().values.round(1)\n",
    ")\n",
    "decades[\"most_common_word\"] = decades[\"unique_words\"].apply(\n",
    "    lambda x: n_most_common_word(x)\n",
    ")\n",
    "decades[\"2nd_most_common_word\"] = decades[\"unique_words\"].apply(\n",
    "    lambda x: n_most_common_word(x, 2)\n",
    ")\n",
    "decades[\"3rd_most_common_word\"] = decades[\"unique_words\"].apply(\n",
    "    lambda x: n_most_common_word(x, 3)\n",
    ")\n",
    "decades[\"4th_most_common_word\"] = decades[\"unique_words\"].apply(\n",
    "    lambda x: n_most_common_word(x, 4)\n",
    ")\n",
    "decades[\"5th_most_common_word\"] = decades[\"unique_words\"].apply(\n",
    "    lambda x: n_most_common_word(x, 5)\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>decade</th>\n",
       "      <th>all_words</th>\n",
       "      <th>unique_words</th>\n",
       "      <th>common_unique_words</th>\n",
       "      <th>n_words</th>\n",
       "      <th>unique_word_count</th>\n",
       "      <th>mean_word_count</th>\n",
       "      <th>median_word_count</th>\n",
       "      <th>most_common_word</th>\n",
       "      <th>2nd_most_common_word</th>\n",
       "      <th>3rd_most_common_word</th>\n",
       "      <th>4th_most_common_word</th>\n",
       "      <th>5th_most_common_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1960</td>\n",
       "      <td>hey girl want know going miss much go hey girl...</td>\n",
       "      <td>long inside go miss live die deep want know su...</td>\n",
       "      <td>long inside go miss live die deep want know su...</td>\n",
       "      <td>232224</td>\n",
       "      <td>10340</td>\n",
       "      <td>90.8</td>\n",
       "      <td>85.0</td>\n",
       "      <td>love</td>\n",
       "      <td>know</td>\n",
       "      <td>oh</td>\n",
       "      <td>go</td>\n",
       "      <td>got</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1970</td>\n",
       "      <td>walking sidewalk purple afternoon accosted bar...</td>\n",
       "      <td>wearing sidewalk imaginary someone oh crawling...</td>\n",
       "      <td>wearing sidewalk someone oh go upon holding co...</td>\n",
       "      <td>256399</td>\n",
       "      <td>10518</td>\n",
       "      <td>114.2</td>\n",
       "      <td>105.0</td>\n",
       "      <td>love</td>\n",
       "      <td>know</td>\n",
       "      <td>got</td>\n",
       "      <td>oh</td>\n",
       "      <td>like</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1980</td>\n",
       "      <td>get high horse let go one one make feel real y...</td>\n",
       "      <td>stuff feel high oh go yeah cared plan fan else...</td>\n",
       "      <td>stuff feel high oh go yeah cared plan fan else...</td>\n",
       "      <td>250920</td>\n",
       "      <td>9522</td>\n",
       "      <td>124.3</td>\n",
       "      <td>117.0</td>\n",
       "      <td>know</td>\n",
       "      <td>love</td>\n",
       "      <td>time</td>\n",
       "      <td>got</td>\n",
       "      <td>oh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1990</td>\n",
       "      <td>want yeah let hear say warren g warren g want ...</td>\n",
       "      <td>skirt dipped high head fat hennessey thought l...</td>\n",
       "      <td>skirt dipped high head fat hennessey thought l...</td>\n",
       "      <td>279460</td>\n",
       "      <td>14357</td>\n",
       "      <td>174.7</td>\n",
       "      <td>146.0</td>\n",
       "      <td>know</td>\n",
       "      <td>love</td>\n",
       "      <td>like</td>\n",
       "      <td>time</td>\n",
       "      <td>want</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2000</td>\n",
       "      <td>shawty shawty shawty yeah yeah get lonely busi...</td>\n",
       "      <td>find someone oh give sitting go cash little me...</td>\n",
       "      <td>find someone oh give sitting go cash little me...</td>\n",
       "      <td>338454</td>\n",
       "      <td>15686</td>\n",
       "      <td>190.7</td>\n",
       "      <td>161.0</td>\n",
       "      <td>know</td>\n",
       "      <td>like</td>\n",
       "      <td>got</td>\n",
       "      <td>want</td>\n",
       "      <td>get</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2010</td>\n",
       "      <td>mmm mmm oh yeah oh yeah yeah alright save need...</td>\n",
       "      <td>close stay saving guard feel oh call mean nowh...</td>\n",
       "      <td>close stay saving guard feel oh call mean nowh...</td>\n",
       "      <td>463311</td>\n",
       "      <td>19705</td>\n",
       "      <td>206.9</td>\n",
       "      <td>185.0</td>\n",
       "      <td>like</td>\n",
       "      <td>know</td>\n",
       "      <td>got</td>\n",
       "      <td>get</td>\n",
       "      <td>want</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   decade                                          all_words  \\\n",
       "0    1960  hey girl want know going miss much go hey girl...   \n",
       "1    1970  walking sidewalk purple afternoon accosted bar...   \n",
       "2    1980  get high horse let go one one make feel real y...   \n",
       "3    1990  want yeah let hear say warren g warren g want ...   \n",
       "4    2000  shawty shawty shawty yeah yeah get lonely busi...   \n",
       "5    2010  mmm mmm oh yeah oh yeah yeah alright save need...   \n",
       "\n",
       "                                        unique_words  \\\n",
       "0  long inside go miss live die deep want know su...   \n",
       "1  wearing sidewalk imaginary someone oh crawling...   \n",
       "2  stuff feel high oh go yeah cared plan fan else...   \n",
       "3  skirt dipped high head fat hennessey thought l...   \n",
       "4  find someone oh give sitting go cash little me...   \n",
       "5  close stay saving guard feel oh call mean nowh...   \n",
       "\n",
       "                                 common_unique_words  n_words  \\\n",
       "0  long inside go miss live die deep want know su...   232224   \n",
       "1  wearing sidewalk someone oh go upon holding co...   256399   \n",
       "2  stuff feel high oh go yeah cared plan fan else...   250920   \n",
       "3  skirt dipped high head fat hennessey thought l...   279460   \n",
       "4  find someone oh give sitting go cash little me...   338454   \n",
       "5  close stay saving guard feel oh call mean nowh...   463311   \n",
       "\n",
       "   unique_word_count  mean_word_count  median_word_count most_common_word  \\\n",
       "0              10340             90.8               85.0             love   \n",
       "1              10518            114.2              105.0             love   \n",
       "2               9522            124.3              117.0             know   \n",
       "3              14357            174.7              146.0             know   \n",
       "4              15686            190.7              161.0             know   \n",
       "5              19705            206.9              185.0             like   \n",
       "\n",
       "  2nd_most_common_word 3rd_most_common_word 4th_most_common_word  \\\n",
       "0                 know                   oh                   go   \n",
       "1                 know                  got                   oh   \n",
       "2                 love                 time                  got   \n",
       "3                 love                 like                 time   \n",
       "4                 like                  got                 want   \n",
       "5                 know                  got                  get   \n",
       "\n",
       "  5th_most_common_word  \n",
       "0                  got  \n",
       "1                 like  \n",
       "2                   oh  \n",
       "3                 want  \n",
       "4                  get  \n",
       "5                 want  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_common_bigrams(text, threshold=5):\n",
    "    \"\"\"\n",
    "    This function takes in a text and returns a list of the top 5 bigrams that are common to the text.\n",
    "    \"\"\"\n",
    "    # get all bigrams in text\n",
    "    bigrams = pd.Series(nltk.ngrams(text.split(), 2)).value_counts()\n",
    "    # filter out bigrams that are less than threshold\n",
    "    bigrams = bigrams[bigrams > threshold]\n",
    "    # return all that occur more than 5 times\n",
    "    return bigrams.index.values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "decades[\"common_bigrams\"] = decades.all_words.apply(lambda x: get_common_bigrams(x))\n",
    "\n",
    "\n",
    "# add bigrams to decade dataframe\n",
    "decades[\"bigrams\"] = decades.all_words.apply(\n",
    "    lambda x: pd.Series(nltk.ngrams(x.split(), 2)).values\n",
    ")\n",
    "\n",
    "# makes sets of all common bigrams in all decades so no bigrams are repeated\n",
    "set_60s = set(decades[decades.decade == 1960].common_bigrams.values[0])\n",
    "set_70s = set(decades[decades.decade == 1970].common_bigrams.values[0])\n",
    "set_80s = set(decades[decades.decade == 1980].common_bigrams.values[0])\n",
    "set_90s = set(decades[decades.decade == 1990].common_bigrams.values[0])\n",
    "set_00s = set(decades[decades.decade == 2000].common_bigrams.values[0])\n",
    "set_10s = set(decades[decades.decade == 2010].common_bigrams.values[0])\n",
    "# remove words found in other decades\n",
    "unique_to_60 = set_60s-set_70s-set_80s-set_90s-set_00s-set_10s\n",
    "unique_to_70 = set_70s-set_60s-set_80s-set_90s-set_00s-set_10s\n",
    "unique_to_80 = set_80s-set_60s-set_70s-set_90s-set_00s-set_10s\n",
    "unique_to_90 = set_90s-set_60s-set_70s-set_80s-set_00s-set_10s\n",
    "unique_to_00 = set_00s-set_60s-set_70s-set_80s-set_90s-set_10s\n",
    "unique_to_10 = set_10s-set_60s-set_70s-set_80s-set_90s-set_00s\n",
    "# make a series to add to the dataframe\n",
    "unique_to_decade = [unique_to_60, unique_to_70, unique_to_80, unique_to_90, unique_to_00, unique_to_10]\n",
    "decades[\"bigrams_unique_to_decade\"] = list(unique_to_decade)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>decade</th>\n",
       "      <th>all_words</th>\n",
       "      <th>unique_words</th>\n",
       "      <th>common_unique_words</th>\n",
       "      <th>n_words</th>\n",
       "      <th>unique_word_count</th>\n",
       "      <th>mean_word_count</th>\n",
       "      <th>median_word_count</th>\n",
       "      <th>most_common_word</th>\n",
       "      <th>2nd_most_common_word</th>\n",
       "      <th>3rd_most_common_word</th>\n",
       "      <th>4th_most_common_word</th>\n",
       "      <th>5th_most_common_word</th>\n",
       "      <th>common_bigrams</th>\n",
       "      <th>bigrams</th>\n",
       "      <th>bigrams_unique_to_decade</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1960</td>\n",
       "      <td>hey girl want know going miss much go hey girl...</td>\n",
       "      <td>long inside go miss live die deep want know su...</td>\n",
       "      <td>long inside go miss live die deep want know su...</td>\n",
       "      <td>232224</td>\n",
       "      <td>10340</td>\n",
       "      <td>90.8</td>\n",
       "      <td>85.0</td>\n",
       "      <td>love</td>\n",
       "      <td>know</td>\n",
       "      <td>oh</td>\n",
       "      <td>go</td>\n",
       "      <td>got</td>\n",
       "      <td>[(yeah, yeah), (la, la), (oh, oh), (let, u), (...</td>\n",
       "      <td>[(hey, girl), (girl, want), (want, know), (kno...</td>\n",
       "      <td>{(around, leaf), (cryin, eye), (purple, haze),...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1970</td>\n",
       "      <td>walking sidewalk purple afternoon accosted bar...</td>\n",
       "      <td>wearing sidewalk imaginary someone oh crawling...</td>\n",
       "      <td>wearing sidewalk someone oh go upon holding co...</td>\n",
       "      <td>256399</td>\n",
       "      <td>10518</td>\n",
       "      <td>114.2</td>\n",
       "      <td>105.0</td>\n",
       "      <td>love</td>\n",
       "      <td>know</td>\n",
       "      <td>got</td>\n",
       "      <td>oh</td>\n",
       "      <td>like</td>\n",
       "      <td>[(la, la), (yeah, yeah), (love, love), (oh, oh...</td>\n",
       "      <td>[(walking, sidewalk), (sidewalk, purple), (pur...</td>\n",
       "      <td>{(lied, would), (flatter, flatter), (jones, mr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1980</td>\n",
       "      <td>get high horse let go one one make feel real y...</td>\n",
       "      <td>stuff feel high oh go yeah cared plan fan else...</td>\n",
       "      <td>stuff feel high oh go yeah cared plan fan else...</td>\n",
       "      <td>250920</td>\n",
       "      <td>9522</td>\n",
       "      <td>124.3</td>\n",
       "      <td>117.0</td>\n",
       "      <td>know</td>\n",
       "      <td>love</td>\n",
       "      <td>time</td>\n",
       "      <td>got</td>\n",
       "      <td>oh</td>\n",
       "      <td>[(oh, oh), (yeah, yeah), (let, u), (love, love...</td>\n",
       "      <td>[(get, high), (high, horse), (horse, let), (le...</td>\n",
       "      <td>{(matter, take), (beat, come), (glad, happy), ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   decade                                          all_words  \\\n",
       "0    1960  hey girl want know going miss much go hey girl...   \n",
       "1    1970  walking sidewalk purple afternoon accosted bar...   \n",
       "2    1980  get high horse let go one one make feel real y...   \n",
       "\n",
       "                                        unique_words  \\\n",
       "0  long inside go miss live die deep want know su...   \n",
       "1  wearing sidewalk imaginary someone oh crawling...   \n",
       "2  stuff feel high oh go yeah cared plan fan else...   \n",
       "\n",
       "                                 common_unique_words  n_words  \\\n",
       "0  long inside go miss live die deep want know su...   232224   \n",
       "1  wearing sidewalk someone oh go upon holding co...   256399   \n",
       "2  stuff feel high oh go yeah cared plan fan else...   250920   \n",
       "\n",
       "   unique_word_count  mean_word_count  median_word_count most_common_word  \\\n",
       "0              10340             90.8               85.0             love   \n",
       "1              10518            114.2              105.0             love   \n",
       "2               9522            124.3              117.0             know   \n",
       "\n",
       "  2nd_most_common_word 3rd_most_common_word 4th_most_common_word  \\\n",
       "0                 know                   oh                   go   \n",
       "1                 know                  got                   oh   \n",
       "2                 love                 time                  got   \n",
       "\n",
       "  5th_most_common_word                                     common_bigrams  \\\n",
       "0                  got  [(yeah, yeah), (la, la), (oh, oh), (let, u), (...   \n",
       "1                 like  [(la, la), (yeah, yeah), (love, love), (oh, oh...   \n",
       "2                   oh  [(oh, oh), (yeah, yeah), (let, u), (love, love...   \n",
       "\n",
       "                                             bigrams  \\\n",
       "0  [(hey, girl), (girl, want), (want, know), (kno...   \n",
       "1  [(walking, sidewalk), (sidewalk, purple), (pur...   \n",
       "2  [(get, high), (high, horse), (horse, let), (le...   \n",
       "\n",
       "                            bigrams_unique_to_decade  \n",
       "0  {(around, leaf), (cryin, eye), (purple, haze),...  \n",
       "1  {(lied, would), (flatter, flatter), (jones, mr...  \n",
       "2  {(matter, take), (beat, come), (glad, happy), ...  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decades.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2588"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(decades.bigrams_unique_to_decade.values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2588\n",
      "3043\n",
      "3267\n",
      "3264\n",
      "4620\n",
      "7299\n"
     ]
    }
   ],
   "source": [
    "# print len of bigrams_unique_to_decade for all decades\n",
    "for  i in range(len(decades)):\n",
    "    print(len(decades.bigrams_unique_to_decade[i])) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def bigram_count(bigrams, decade):\n",
    "    \"\"\"\n",
    "    This function takes in a list of bigrams and returns the count of bigrams that are in the decades unique set.\n",
    "    \"\"\"\n",
    "    if decade == 1960:\n",
    "        unique_set = unique_to_60\n",
    "    elif decade == 1970:\n",
    "        unique_set = unique_to_70\n",
    "    elif decade == 1980:\n",
    "        unique_set = unique_to_80\n",
    "    elif decade == 1990:\n",
    "        unique_set = unique_to_90\n",
    "    elif decade == 2000:\n",
    "        unique_set = unique_to_00\n",
    "    elif decade == 2010:\n",
    "        unique_set = unique_to_10\n",
    "    else:\n",
    "        print(\"Error: decade not found\")\n",
    "\n",
    "    # count the number of bigrams that are in the unique set\n",
    "    count = 0\n",
    "    for bigram in bigrams:\n",
    "        if bigram in unique_set:\n",
    "            count += 1\n",
    "    return count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def baseline_accuracy2(series, mode):\n",
    "    \"\"\"\n",
    "    Calculate baseline accuracy\n",
    "    \"\"\"\n",
    "    test = pd.DataFrame(series)\n",
    "    test[\"mode\"] = mode\n",
    "    baseline_accuracy = accuracy_score(test[\"decade\"], test[\"mode\"])\n",
    "    return baseline_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>artist</th>\n",
       "      <th>date</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>raw_lyrics</th>\n",
       "      <th>decade</th>\n",
       "      <th>character_count</th>\n",
       "      <th>word_count</th>\n",
       "      <th>unique_words</th>\n",
       "      <th>unique_words_count</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>sentiment_category</th>\n",
       "      <th>place_words</th>\n",
       "      <th>chorus_count</th>\n",
       "      <th>verse_count</th>\n",
       "      <th>verse_chorus_ratio</th>\n",
       "      <th>pre_chorus_count</th>\n",
       "      <th>outro_count</th>\n",
       "      <th>bridge_count</th>\n",
       "      <th>hook_count</th>\n",
       "      <th>bigrams</th>\n",
       "      <th>trigrams</th>\n",
       "      <th>topic_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#1</td>\n",
       "      <td>Nelly</td>\n",
       "      <td>2001-10-20</td>\n",
       "      <td>uh uh uh got bring attention dirty better watc...</td>\n",
       "      <td>#1 LyricsUh uh uh I just gotta bring it to the...</td>\n",
       "      <td>2000</td>\n",
       "      <td>2014</td>\n",
       "      <td>363</td>\n",
       "      <td>learn hiphop uhh album aiyyo mean catch right ...</td>\n",
       "      <td>228</td>\n",
       "      <td>0.9915</td>\n",
       "      <td>very positive</td>\n",
       "      <td>[hook] [hook] [hook]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>[(uh, uh), (uh, uh), (uh, got), (got, bring), ...</td>\n",
       "      <td>[(uh, uh, uh), (uh, uh, got), (uh, got, bring)...</td>\n",
       "      <td>sex</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#9 Dream</td>\n",
       "      <td>John Lennon</td>\n",
       "      <td>1974-12-21</td>\n",
       "      <td>long ago dream dream know yes know seemed real...</td>\n",
       "      <td>#9 Dream Lyrics[Verse 1] So long ago Was it in...</td>\n",
       "      <td>1970</td>\n",
       "      <td>842</td>\n",
       "      <td>135</td>\n",
       "      <td>dancing long call air feel bowakawa go dance s...</td>\n",
       "      <td>49</td>\n",
       "      <td>0.9169</td>\n",
       "      <td>very positive</td>\n",
       "      <td>[verse 1] [pre-chorus 1] [chorus] [verse 2] [p...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[(long, ago), (ago, dream), (dream, dream), (d...</td>\n",
       "      <td>[(long, ago, dream), (ago, dream, dream), (dre...</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#Beautiful</td>\n",
       "      <td>Mariah Carey Featuring Miguel</td>\n",
       "      <td>2013-05-25</td>\n",
       "      <td>ah ah beautiful ah ah beautiful hop back bike ...</td>\n",
       "      <td>#Beautiful Lyrics[Intro: Mariah Carey] Ah, ah,...</td>\n",
       "      <td>2010</td>\n",
       "      <td>768</td>\n",
       "      <td>129</td>\n",
       "      <td>feel beautiful oh mean stop dance yes yeah nig...</td>\n",
       "      <td>54</td>\n",
       "      <td>0.9989</td>\n",
       "      <td>very positive</td>\n",
       "      <td>[intro: mariah carey] [verse 1: miguel] [choru...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[(ah, ah), (ah, beautiful), (beautiful, ah), (...</td>\n",
       "      <td>[(ah, ah, beautiful), (ah, beautiful, ah), (be...</td>\n",
       "      <td>lost</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#SELFIE</td>\n",
       "      <td>The Chainsmokers</td>\n",
       "      <td>2014-03-15</td>\n",
       "      <td>jason table kept seeing look girl think make j...</td>\n",
       "      <td>#SELFIE Lyrics[Verse 1] When Jason was at the ...</td>\n",
       "      <td>2010</td>\n",
       "      <td>954</td>\n",
       "      <td>172</td>\n",
       "      <td>feel dance first shot valencia throw tacky wai...</td>\n",
       "      <td>100</td>\n",
       "      <td>0.9218</td>\n",
       "      <td>very positive</td>\n",
       "      <td>[verse 1] [drop 1] [verse 2] [drop 2] [verse 3...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[(jason, table), (table, kept), (kept, seeing)...</td>\n",
       "      <td>[(jason, table, kept), (table, kept, seeing), ...</td>\n",
       "      <td>violence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#thatPOWER</td>\n",
       "      <td>will.i.am Featuring Justin Bieber</td>\n",
       "      <td>2013-04-06</td>\n",
       "      <td>oh alive alive alive oh fly fly fly oh alive a...</td>\n",
       "      <td>#thatPOWER Lyrics[Instrumental break]  [Pre-Ch...</td>\n",
       "      <td>2010</td>\n",
       "      <td>1238</td>\n",
       "      <td>208</td>\n",
       "      <td>high fresh fire chest bank yyyyes hatin burnin...</td>\n",
       "      <td>77</td>\n",
       "      <td>0.9984</td>\n",
       "      <td>very positive</td>\n",
       "      <td>[instrumental break] [pre-chorus: justin biebe...</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[(oh, alive), (alive, alive), (alive, alive), ...</td>\n",
       "      <td>[(oh, alive, alive), (alive, alive, alive), (a...</td>\n",
       "      <td>sex</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        title                             artist       date  \\\n",
       "0          #1                              Nelly 2001-10-20   \n",
       "1    #9 Dream                        John Lennon 1974-12-21   \n",
       "2  #Beautiful      Mariah Carey Featuring Miguel 2013-05-25   \n",
       "3     #SELFIE                   The Chainsmokers 2014-03-15   \n",
       "4  #thatPOWER  will.i.am Featuring Justin Bieber 2013-04-06   \n",
       "\n",
       "                                              lyrics  \\\n",
       "0  uh uh uh got bring attention dirty better watc...   \n",
       "1  long ago dream dream know yes know seemed real...   \n",
       "2  ah ah beautiful ah ah beautiful hop back bike ...   \n",
       "3  jason table kept seeing look girl think make j...   \n",
       "4  oh alive alive alive oh fly fly fly oh alive a...   \n",
       "\n",
       "                                          raw_lyrics  decade  character_count  \\\n",
       "0  #1 LyricsUh uh uh I just gotta bring it to the...    2000             2014   \n",
       "1  #9 Dream Lyrics[Verse 1] So long ago Was it in...    1970              842   \n",
       "2  #Beautiful Lyrics[Intro: Mariah Carey] Ah, ah,...    2010              768   \n",
       "3  #SELFIE Lyrics[Verse 1] When Jason was at the ...    2010              954   \n",
       "4  #thatPOWER Lyrics[Instrumental break]  [Pre-Ch...    2010             1238   \n",
       "\n",
       "   word_count                                       unique_words  \\\n",
       "0         363  learn hiphop uhh album aiyyo mean catch right ...   \n",
       "1         135  dancing long call air feel bowakawa go dance s...   \n",
       "2         129  feel beautiful oh mean stop dance yes yeah nig...   \n",
       "3         172  feel dance first shot valencia throw tacky wai...   \n",
       "4         208  high fresh fire chest bank yyyyes hatin burnin...   \n",
       "\n",
       "   unique_words_count  sentiment sentiment_category  \\\n",
       "0                 228     0.9915      very positive   \n",
       "1                  49     0.9169      very positive   \n",
       "2                  54     0.9989      very positive   \n",
       "3                 100     0.9218      very positive   \n",
       "4                  77     0.9984      very positive   \n",
       "\n",
       "                                         place_words  chorus_count  \\\n",
       "0                               [hook] [hook] [hook]             0   \n",
       "1  [verse 1] [pre-chorus 1] [chorus] [verse 2] [p...             2   \n",
       "2  [intro: mariah carey] [verse 1: miguel] [choru...             2   \n",
       "3  [verse 1] [drop 1] [verse 2] [drop 2] [verse 3...             0   \n",
       "4  [instrumental break] [pre-chorus: justin biebe...             3   \n",
       "\n",
       "   verse_count  verse_chorus_ratio  pre_chorus_count  outro_count  \\\n",
       "0            0            0.000000                 0            0   \n",
       "1            2            1.000000                 2            1   \n",
       "2            2            1.000000                 0            1   \n",
       "3            3            0.000000                 0            0   \n",
       "4            2            0.666667                 3            1   \n",
       "\n",
       "   bridge_count  hook_count  \\\n",
       "0             0           3   \n",
       "1             0           0   \n",
       "2             0           0   \n",
       "3             0           0   \n",
       "4             1           0   \n",
       "\n",
       "                                             bigrams  \\\n",
       "0  [(uh, uh), (uh, uh), (uh, got), (got, bring), ...   \n",
       "1  [(long, ago), (ago, dream), (dream, dream), (d...   \n",
       "2  [(ah, ah), (ah, beautiful), (beautiful, ah), (...   \n",
       "3  [(jason, table), (table, kept), (kept, seeing)...   \n",
       "4  [(oh, alive), (alive, alive), (alive, alive), ...   \n",
       "\n",
       "                                            trigrams topic_name  \n",
       "0  [(uh, uh, uh), (uh, uh, got), (uh, got, bring)...        sex  \n",
       "1  [(long, ago, dream), (ago, dream, dream), (dre...       love  \n",
       "2  [(ah, ah, beautiful), (ah, beautiful, ah), (be...       lost  \n",
       "3  [(jason, table, kept), (table, kept, seeing), ...   violence  \n",
       "4  [(oh, alive, alive), (alive, alive, alive), (a...        sex  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[\"bigrams_from_60s\"] = df.bigrams.apply(lambda x: bigram_count(x, 1960))\n",
    "# df[\"bigrams_from_70s\"] = df.bigrams.apply(lambda x: bigram_count(x, 1970))\n",
    "# df[\"bigrams_from_80s\"] = df.bigrams.apply(lambda x: bigram_count(x, 1980))\n",
    "# df[\"bigrams_from_90s\"] = df.bigrams.apply(lambda x: bigram_count(x, 1990))\n",
    "# df[\"bigrams_from_00s\"] = df.bigrams.apply(lambda x: bigram_count(x, 2000))\n",
    "# df[\"bigrams_from_10s\"] = df.bigrams.apply(lambda x: bigram_count(x, 2010))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title                 0\n",
       "artist                0\n",
       "date                  0\n",
       "lyrics                0\n",
       "raw_lyrics            0\n",
       "decade                0\n",
       "character_count       0\n",
       "word_count            0\n",
       "unique_words          0\n",
       "unique_words_count    0\n",
       "sentiment             0\n",
       "sentiment_category    0\n",
       "place_words           0\n",
       "chorus_count          0\n",
       "verse_count           0\n",
       "verse_chorus_ratio    0\n",
       "pre_chorus_count      0\n",
       "outro_count           0\n",
       "bridge_count          0\n",
       "hook_count            0\n",
       "bigrams               0\n",
       "trigrams              0\n",
       "topic_name            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>decade</th>\n",
       "      <th>character_count</th>\n",
       "      <th>word_count</th>\n",
       "      <th>unique_words_count</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>chorus_count</th>\n",
       "      <th>verse_count</th>\n",
       "      <th>verse_chorus_ratio</th>\n",
       "      <th>pre_chorus_count</th>\n",
       "      <th>outro_count</th>\n",
       "      <th>bridge_count</th>\n",
       "      <th>hook_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>22210.000000</td>\n",
       "      <td>22210.000000</td>\n",
       "      <td>22210.000000</td>\n",
       "      <td>22210.000000</td>\n",
       "      <td>22210.000000</td>\n",
       "      <td>22210.000000</td>\n",
       "      <td>22210.000000</td>\n",
       "      <td>22210.000000</td>\n",
       "      <td>22210.000000</td>\n",
       "      <td>22210.000000</td>\n",
       "      <td>22210.000000</td>\n",
       "      <td>22210.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1983.622692</td>\n",
       "      <td>838.231968</td>\n",
       "      <td>146.402521</td>\n",
       "      <td>68.675236</td>\n",
       "      <td>0.536993</td>\n",
       "      <td>1.553534</td>\n",
       "      <td>1.490545</td>\n",
       "      <td>0.461749</td>\n",
       "      <td>0.314498</td>\n",
       "      <td>0.311436</td>\n",
       "      <td>0.337371</td>\n",
       "      <td>0.135119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>17.866680</td>\n",
       "      <td>493.384062</td>\n",
       "      <td>86.837353</td>\n",
       "      <td>45.273977</td>\n",
       "      <td>0.735576</td>\n",
       "      <td>1.614286</td>\n",
       "      <td>1.452577</td>\n",
       "      <td>0.553325</td>\n",
       "      <td>0.801475</td>\n",
       "      <td>0.467253</td>\n",
       "      <td>0.568076</td>\n",
       "      <td>0.692989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1960.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.999900</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1970.000000</td>\n",
       "      <td>512.000000</td>\n",
       "      <td>89.000000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>0.421500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1980.000000</td>\n",
       "      <td>710.000000</td>\n",
       "      <td>123.000000</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>0.964800</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2000.000000</td>\n",
       "      <td>1004.000000</td>\n",
       "      <td>175.000000</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>0.992300</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2010.000000</td>\n",
       "      <td>10282.000000</td>\n",
       "      <td>1757.000000</td>\n",
       "      <td>906.000000</td>\n",
       "      <td>0.999900</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             decade  character_count    word_count  unique_words_count  \\\n",
       "count  22210.000000     22210.000000  22210.000000        22210.000000   \n",
       "mean    1983.622692       838.231968    146.402521           68.675236   \n",
       "std       17.866680       493.384062     86.837353           45.273977   \n",
       "min     1960.000000         0.000000      0.000000            0.000000   \n",
       "25%     1970.000000       512.000000     89.000000           42.000000   \n",
       "50%     1980.000000       710.000000    123.000000           56.000000   \n",
       "75%     2000.000000      1004.000000    175.000000           77.000000   \n",
       "max     2010.000000     10282.000000   1757.000000          906.000000   \n",
       "\n",
       "          sentiment  chorus_count   verse_count  verse_chorus_ratio  \\\n",
       "count  22210.000000  22210.000000  22210.000000        22210.000000   \n",
       "mean       0.536993      1.553534      1.490545            0.461749   \n",
       "std        0.735576      1.614286      1.452577            0.553325   \n",
       "min       -0.999900      0.000000      0.000000            0.000000   \n",
       "25%        0.421500      0.000000      0.000000            0.000000   \n",
       "50%        0.964800      2.000000      2.000000            0.400000   \n",
       "75%        0.992300      3.000000      2.000000            0.750000   \n",
       "max        0.999900     11.000000     16.000000           13.000000   \n",
       "\n",
       "       pre_chorus_count   outro_count  bridge_count    hook_count  \n",
       "count      22210.000000  22210.000000  22210.000000  22210.000000  \n",
       "mean           0.314498      0.311436      0.337371      0.135119  \n",
       "std            0.801475      0.467253      0.568076      0.692989  \n",
       "min            0.000000      0.000000      0.000000      0.000000  \n",
       "25%            0.000000      0.000000      0.000000      0.000000  \n",
       "50%            0.000000      0.000000      0.000000      0.000000  \n",
       "75%            0.000000      1.000000      1.000000      0.000000  \n",
       "max            6.000000      3.000000      6.000000      9.000000  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['decade',\n",
       " 'character_count',\n",
       " 'word_count',\n",
       " 'unique_words_count',\n",
       " 'sentiment',\n",
       " 'chorus_count',\n",
       " 'verse_count',\n",
       " 'verse_chorus_ratio',\n",
       " 'pre_chorus_count',\n",
       " 'outro_count',\n",
       " 'bridge_count',\n",
       " 'hook_count']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numeric_cols = [\n",
    "    col for col in train.columns if train[col].dtype in [\"int64\", \"float64\"]\n",
    "]\n",
    "numeric_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "###############################################################################\n",
    "#                                                                             #\n",
    "#               run models on feature engineered columns                      #\n",
    "#                                                                             #\n",
    "###############################################################################\n",
    "\n",
    "\n",
    "def run_models_on_feature_engineered_columns(df, scale=True, bigrams_only=False):\n",
    "    \"\"\"\n",
    "    Run models on data varying solver and C value\n",
    "    \"\"\"\n",
    "    # # add bigram counts to main df\n",
    "    # df[\"bigrams\"] = df.lyrics.apply(\n",
    "    #     lambda x: pd.Series(nltk.ngrams(x.split(), 2)).values\n",
    "    #)\n",
    "    df[\"bigrams_from_60s\"] = df.bigrams.apply(lambda x: bigram_count(x, 1960))\n",
    "    df[\"bigrams_from_70s\"] = df.bigrams.apply(lambda x: bigram_count(x, 1970))\n",
    "    df[\"bigrams_from_80s\"] = df.bigrams.apply(lambda x: bigram_count(x, 1980))\n",
    "    df[\"bigrams_from_90s\"] = df.bigrams.apply(lambda x: bigram_count(x, 1990))\n",
    "    df[\"bigrams_from_00s\"] = df.bigrams.apply(lambda x: bigram_count(x, 2000))\n",
    "    df[\"bigrams_from_10s\"] = df.bigrams.apply(lambda x: bigram_count(x, 2010))\n",
    "\n",
    "    # split data into train, validate, and test sets\n",
    "    train, validate, test = prep.split_data(df)\n",
    "    y_train = train.decade\n",
    "    y_validate = validate.decade\n",
    "    y_test = test.decade\n",
    "    # list all columns with dtype int or float\n",
    "    numeric_cols = [\n",
    "        col for col in train.columns if train[col].dtype in [\"int64\", \"float64\"]\n",
    "    ]\n",
    "    # keep only numeric columns\n",
    "    X_train = train[numeric_cols].drop(['decade'], axis=1)\n",
    "    X_validate = validate[numeric_cols].drop(['decade'], axis=1)\n",
    "    X_test = test[numeric_cols].drop(['decade'], axis=1)\n",
    "    if bigrams_only:\n",
    "        bigram_columns = [col for col in train.columns if \"_bigrams\" in col]\n",
    "        X_train = train[bigram_columns]\n",
    "        X_validate = validate[bigram_columns]\n",
    "        X_test = test[bigram_columns]\n",
    "    if scale:\n",
    "        print(\"Scaling data\")\n",
    "        # scale data\n",
    "        scaler = MinMaxScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_validate = scaler.transform(X_validate)\n",
    "        X_test = scaler.transform(X_test)\n",
    "\n",
    "    # get mode to use as baseline\n",
    "    mode = df.decade.mode().values[0]\n",
    "    # get baseline_accuracy\n",
    "    train_baseline = baseline_accuracy2(y_train, mode)\n",
    "    validate_baseline = baseline_accuracy2(y_validate, mode)\n",
    "    test_baseline = baseline_accuracy2(y_test, mode)\n",
    "    # make a df for results\n",
    "    results = pd.DataFrame()\n",
    "    # make baseline model\n",
    "    baseline_model = pd.Series(\n",
    "        {\n",
    "            \"model_number\": \"baseline\",\n",
    "            \"model_type\": \"baseline\",\n",
    "            \"train_accuracy\": train_baseline,\n",
    "            \"validate_accuracy\": validate_baseline,\n",
    "            \"test_accuracy\": test_baseline,\n",
    "            \"better_than_baseline\": False,\n",
    "        }\n",
    "    )\n",
    "    # add baseline model to results df\n",
    "    results = pd.concat([results, baseline_model], axis=0)\n",
    "    # make more models varying solver\n",
    "    model_number = results.shape[1]\n",
    "    c_values = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "    for solver in [\"liblinear\", \"lbfgs\", \"newton-cg\", \"sag\", \"saga\"]:\n",
    "        for c in c_values:\n",
    "            # make the model\n",
    "            lm = LogisticRegression(C=c, solver=solver).fit(X_train, y_train)\n",
    "            # run model on data splits\n",
    "            train[\"predicted\"] = lm.predict(X_train)\n",
    "            validate[\"predicted\"] = lm.predict(X_validate)\n",
    "            test[\"predicted\"] = lm.predict(X_test)\n",
    "            # make results series to add to results df\n",
    "            stats = pd.Series(\n",
    "                {\n",
    "                    \"model_number\": model_number,\n",
    "                    \"model_type\": \"LogisticRegression\",\n",
    "                    \"solver\": solver,\n",
    "                    \"C\": c,\n",
    "                    \"train_accuracy\": accuracy_score(y_train, train[\"predicted\"]),\n",
    "                    \"validate_accuracy\": accuracy_score(\n",
    "                        y_validate, validate[\"predicted\"]\n",
    "                    ),\n",
    "                    \"test_accuracy\": accuracy_score(y_test, test[\"predicted\"]),\n",
    "                    \"baseline_accuracy\": validate_baseline,\n",
    "                    \"better_than_baseline\": accuracy_score(\n",
    "                        y_validate, validate[\"predicted\"]\n",
    "                    )\n",
    "                    > validate_baseline,\n",
    "                }\n",
    "            )\n",
    "            # add to results df\n",
    "            results = pd.concat([results, stats], axis=1)\n",
    "            model_number += 1\n",
    "\n",
    "    return results.T.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = run_models_on_feature_engineered_columns(df, scale=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_number</th>\n",
       "      <th>model_type</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>validate_accuracy</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>better_than_baseline</th>\n",
       "      <th>solver</th>\n",
       "      <th>C</th>\n",
       "      <th>baseline_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.370829</td>\n",
       "      <td>0.36466</td>\n",
       "      <td>0.366727</td>\n",
       "      <td>True</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.20559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.36713</td>\n",
       "      <td>0.36466</td>\n",
       "      <td>0.361099</td>\n",
       "      <td>True</td>\n",
       "      <td>liblinear</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.20559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.370749</td>\n",
       "      <td>0.364284</td>\n",
       "      <td>0.366276</td>\n",
       "      <td>True</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>1</td>\n",
       "      <td>0.20559</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   model_number          model_type train_accuracy validate_accuracy  \\\n",
       "17           17  LogisticRegression       0.370829           0.36466   \n",
       "3             3  LogisticRegression        0.36713           0.36466   \n",
       "18           18  LogisticRegression       0.370749          0.364284   \n",
       "\n",
       "   test_accuracy better_than_baseline     solver    C baseline_accuracy  \n",
       "17      0.366727                 True  newton-cg  0.1           0.20559  \n",
       "3       0.361099                 True  liblinear  0.1           0.20559  \n",
       "18      0.366276                 True  newton-cg    1           0.20559  "
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.sort_values(['validate_accuracy','test_accuracy'], ascending=False).head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaling data\n"
     ]
    }
   ],
   "source": [
    "results2 = run_models_on_feature_engineered_columns(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_number</th>\n",
       "      <th>model_type</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>validate_accuracy</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>better_than_baseline</th>\n",
       "      <th>solver</th>\n",
       "      <th>C</th>\n",
       "      <th>baseline_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.746723</td>\n",
       "      <td>0.375914</td>\n",
       "      <td>0.377533</td>\n",
       "      <td>True</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>1</td>\n",
       "      <td>0.20559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>32</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.746723</td>\n",
       "      <td>0.375914</td>\n",
       "      <td>0.377533</td>\n",
       "      <td>True</td>\n",
       "      <td>saga</td>\n",
       "      <td>1</td>\n",
       "      <td>0.20559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.746884</td>\n",
       "      <td>0.375539</td>\n",
       "      <td>0.377983</td>\n",
       "      <td>True</td>\n",
       "      <td>sag</td>\n",
       "      <td>1</td>\n",
       "      <td>0.20559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.74608</td>\n",
       "      <td>0.374977</td>\n",
       "      <td>0.379109</td>\n",
       "      <td>True</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>1</td>\n",
       "      <td>0.20559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.743186</td>\n",
       "      <td>0.371975</td>\n",
       "      <td>0.37258</td>\n",
       "      <td>True</td>\n",
       "      <td>liblinear</td>\n",
       "      <td>1</td>\n",
       "      <td>0.20559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>33</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.792394</td>\n",
       "      <td>0.363909</td>\n",
       "      <td>0.368753</td>\n",
       "      <td>True</td>\n",
       "      <td>saga</td>\n",
       "      <td>10</td>\n",
       "      <td>0.20559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.792233</td>\n",
       "      <td>0.363534</td>\n",
       "      <td>0.368978</td>\n",
       "      <td>True</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>10</td>\n",
       "      <td>0.20559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.792394</td>\n",
       "      <td>0.363534</td>\n",
       "      <td>0.368978</td>\n",
       "      <td>True</td>\n",
       "      <td>sag</td>\n",
       "      <td>10</td>\n",
       "      <td>0.20559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.788454</td>\n",
       "      <td>0.362784</td>\n",
       "      <td>0.367627</td>\n",
       "      <td>True</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>10</td>\n",
       "      <td>0.20559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.791027</td>\n",
       "      <td>0.362221</td>\n",
       "      <td>0.364475</td>\n",
       "      <td>True</td>\n",
       "      <td>liblinear</td>\n",
       "      <td>10</td>\n",
       "      <td>0.20559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.794163</td>\n",
       "      <td>0.355468</td>\n",
       "      <td>0.3638</td>\n",
       "      <td>True</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>100</td>\n",
       "      <td>0.20559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.617271</td>\n",
       "      <td>0.354343</td>\n",
       "      <td>0.358172</td>\n",
       "      <td>True</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.20559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.617271</td>\n",
       "      <td>0.354343</td>\n",
       "      <td>0.358172</td>\n",
       "      <td>True</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.20559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.617271</td>\n",
       "      <td>0.354343</td>\n",
       "      <td>0.358172</td>\n",
       "      <td>True</td>\n",
       "      <td>sag</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.20559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>31</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.617271</td>\n",
       "      <td>0.354343</td>\n",
       "      <td>0.358172</td>\n",
       "      <td>True</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.20559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>34</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.798344</td>\n",
       "      <td>0.35378</td>\n",
       "      <td>0.358847</td>\n",
       "      <td>True</td>\n",
       "      <td>saga</td>\n",
       "      <td>100</td>\n",
       "      <td>0.20559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.798585</td>\n",
       "      <td>0.351716</td>\n",
       "      <td>0.356821</td>\n",
       "      <td>True</td>\n",
       "      <td>sag</td>\n",
       "      <td>100</td>\n",
       "      <td>0.20559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.798826</td>\n",
       "      <td>0.350966</td>\n",
       "      <td>0.356596</td>\n",
       "      <td>True</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>100</td>\n",
       "      <td>0.20559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>35</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.798585</td>\n",
       "      <td>0.350403</td>\n",
       "      <td>0.356146</td>\n",
       "      <td>True</td>\n",
       "      <td>saga</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.20559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.795851</td>\n",
       "      <td>0.350028</td>\n",
       "      <td>0.359748</td>\n",
       "      <td>True</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.20559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.797379</td>\n",
       "      <td>0.349278</td>\n",
       "      <td>0.352319</td>\n",
       "      <td>True</td>\n",
       "      <td>liblinear</td>\n",
       "      <td>100</td>\n",
       "      <td>0.20559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.798504</td>\n",
       "      <td>0.348903</td>\n",
       "      <td>0.35367</td>\n",
       "      <td>True</td>\n",
       "      <td>sag</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.20559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.79762</td>\n",
       "      <td>0.348715</td>\n",
       "      <td>0.350293</td>\n",
       "      <td>True</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.20559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.59725</td>\n",
       "      <td>0.348152</td>\n",
       "      <td>0.353219</td>\n",
       "      <td>True</td>\n",
       "      <td>liblinear</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.20559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.797057</td>\n",
       "      <td>0.344401</td>\n",
       "      <td>0.348942</td>\n",
       "      <td>True</td>\n",
       "      <td>liblinear</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.20559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.389483</td>\n",
       "      <td>0.312324</td>\n",
       "      <td>0.316749</td>\n",
       "      <td>True</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.20559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.389483</td>\n",
       "      <td>0.312324</td>\n",
       "      <td>0.316749</td>\n",
       "      <td>True</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.20559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>30</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.389483</td>\n",
       "      <td>0.312324</td>\n",
       "      <td>0.316749</td>\n",
       "      <td>True</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.20559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.389483</td>\n",
       "      <td>0.312137</td>\n",
       "      <td>0.316749</td>\n",
       "      <td>True</td>\n",
       "      <td>sag</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.20559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.370829</td>\n",
       "      <td>0.309698</td>\n",
       "      <td>0.314948</td>\n",
       "      <td>True</td>\n",
       "      <td>liblinear</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.20559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.292916</td>\n",
       "      <td>0.279122</td>\n",
       "      <td>0.284106</td>\n",
       "      <td>True</td>\n",
       "      <td>liblinear</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.20559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.288012</td>\n",
       "      <td>0.275933</td>\n",
       "      <td>0.280279</td>\n",
       "      <td>True</td>\n",
       "      <td>lbfgs</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.20559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.288012</td>\n",
       "      <td>0.275933</td>\n",
       "      <td>0.280279</td>\n",
       "      <td>True</td>\n",
       "      <td>newton-cg</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.20559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.288012</td>\n",
       "      <td>0.275933</td>\n",
       "      <td>0.280279</td>\n",
       "      <td>True</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.20559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.288172</td>\n",
       "      <td>0.275746</td>\n",
       "      <td>0.279604</td>\n",
       "      <td>True</td>\n",
       "      <td>sag</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.20559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>baseline</td>\n",
       "      <td>baseline</td>\n",
       "      <td>0.205677</td>\n",
       "      <td>0.20559</td>\n",
       "      <td>0.205763</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   model_number          model_type train_accuracy validate_accuracy  \\\n",
       "18           18  LogisticRegression       0.746723          0.375914   \n",
       "32           32  LogisticRegression       0.746723          0.375914   \n",
       "25           25  LogisticRegression       0.746884          0.375539   \n",
       "11           11  LogisticRegression        0.74608          0.374977   \n",
       "4             4  LogisticRegression       0.743186          0.371975   \n",
       "33           33  LogisticRegression       0.792394          0.363909   \n",
       "19           19  LogisticRegression       0.792233          0.363534   \n",
       "26           26  LogisticRegression       0.792394          0.363534   \n",
       "12           12  LogisticRegression       0.788454          0.362784   \n",
       "5             5  LogisticRegression       0.791027          0.362221   \n",
       "13           13  LogisticRegression       0.794163          0.355468   \n",
       "10           10  LogisticRegression       0.617271          0.354343   \n",
       "17           17  LogisticRegression       0.617271          0.354343   \n",
       "24           24  LogisticRegression       0.617271          0.354343   \n",
       "31           31  LogisticRegression       0.617271          0.354343   \n",
       "34           34  LogisticRegression       0.798344           0.35378   \n",
       "27           27  LogisticRegression       0.798585          0.351716   \n",
       "20           20  LogisticRegression       0.798826          0.350966   \n",
       "35           35  LogisticRegression       0.798585          0.350403   \n",
       "14           14  LogisticRegression       0.795851          0.350028   \n",
       "6             6  LogisticRegression       0.797379          0.349278   \n",
       "28           28  LogisticRegression       0.798504          0.348903   \n",
       "21           21  LogisticRegression        0.79762          0.348715   \n",
       "3             3  LogisticRegression        0.59725          0.348152   \n",
       "7             7  LogisticRegression       0.797057          0.344401   \n",
       "9             9  LogisticRegression       0.389483          0.312324   \n",
       "16           16  LogisticRegression       0.389483          0.312324   \n",
       "30           30  LogisticRegression       0.389483          0.312324   \n",
       "23           23  LogisticRegression       0.389483          0.312137   \n",
       "2             2  LogisticRegression       0.370829          0.309698   \n",
       "1             1  LogisticRegression       0.292916          0.279122   \n",
       "8             8  LogisticRegression       0.288012          0.275933   \n",
       "15           15  LogisticRegression       0.288012          0.275933   \n",
       "29           29  LogisticRegression       0.288012          0.275933   \n",
       "22           22  LogisticRegression       0.288172          0.275746   \n",
       "0      baseline            baseline       0.205677           0.20559   \n",
       "\n",
       "   test_accuracy better_than_baseline     solver      C baseline_accuracy  \n",
       "18      0.377533                 True  newton-cg      1           0.20559  \n",
       "32      0.377533                 True       saga      1           0.20559  \n",
       "25      0.377983                 True        sag      1           0.20559  \n",
       "11      0.379109                 True      lbfgs      1           0.20559  \n",
       "4        0.37258                 True  liblinear      1           0.20559  \n",
       "33      0.368753                 True       saga     10           0.20559  \n",
       "19      0.368978                 True  newton-cg     10           0.20559  \n",
       "26      0.368978                 True        sag     10           0.20559  \n",
       "12      0.367627                 True      lbfgs     10           0.20559  \n",
       "5       0.364475                 True  liblinear     10           0.20559  \n",
       "13        0.3638                 True      lbfgs    100           0.20559  \n",
       "10      0.358172                 True      lbfgs    0.1           0.20559  \n",
       "17      0.358172                 True  newton-cg    0.1           0.20559  \n",
       "24      0.358172                 True        sag    0.1           0.20559  \n",
       "31      0.358172                 True       saga    0.1           0.20559  \n",
       "34      0.358847                 True       saga    100           0.20559  \n",
       "27      0.356821                 True        sag    100           0.20559  \n",
       "20      0.356596                 True  newton-cg    100           0.20559  \n",
       "35      0.356146                 True       saga   1000           0.20559  \n",
       "14      0.359748                 True      lbfgs   1000           0.20559  \n",
       "6       0.352319                 True  liblinear    100           0.20559  \n",
       "28       0.35367                 True        sag   1000           0.20559  \n",
       "21      0.350293                 True  newton-cg   1000           0.20559  \n",
       "3       0.353219                 True  liblinear    0.1           0.20559  \n",
       "7       0.348942                 True  liblinear   1000           0.20559  \n",
       "9       0.316749                 True      lbfgs   0.01           0.20559  \n",
       "16      0.316749                 True  newton-cg   0.01           0.20559  \n",
       "30      0.316749                 True       saga   0.01           0.20559  \n",
       "23      0.316749                 True        sag   0.01           0.20559  \n",
       "2       0.314948                 True  liblinear   0.01           0.20559  \n",
       "1       0.284106                 True  liblinear  0.001           0.20559  \n",
       "8       0.280279                 True      lbfgs  0.001           0.20559  \n",
       "15      0.280279                 True  newton-cg  0.001           0.20559  \n",
       "29      0.280279                 True       saga  0.001           0.20559  \n",
       "22      0.279604                 True        sag  0.001           0.20559  \n",
       "0       0.205763                False        NaN    NaN               NaN  "
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results2.sort_values(['validate_accuracy','test_accuracy'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"bigrams_from_60s\"] = df.bigrams.apply(lambda x: bigram_count(x, 1960))\n",
    "df[\"bigrams_from_70s\"] = df.bigrams.apply(lambda x: bigram_count(x, 1970))\n",
    "df[\"bigrams_from_80s\"] = df.bigrams.apply(lambda x: bigram_count(x, 1980))\n",
    "df[\"bigrams_from_90s\"] = df.bigrams.apply(lambda x: bigram_count(x, 1990))\n",
    "df[\"bigrams_from_00s\"] = df.bigrams.apply(lambda x: bigram_count(x, 2000))\n",
    "df[\"bigrams_from_10s\"] = df.bigrams.apply(lambda x: bigram_count(x, 2010))\n",
    "# split data into train, validate, and test sets\n",
    "train, validate, test = prep.split_data(df)\n",
    "y_train = train.decade\n",
    "y_validate = validate.decade\n",
    "y_test = test.decade\n",
    "# list all columns with dtype int or float\n",
    "numeric_cols = [\n",
    "    col for col in train.columns if train[col].dtype in [\"int64\", \"float64\"]\n",
    "]\n",
    "# keep only numeric columns\n",
    "X_train = train[numeric_cols].drop(['decade'], axis=1)\n",
    "X_validate = validate[numeric_cols]\n",
    "X_test = test[numeric_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>character_count</th>\n",
       "      <th>word_count</th>\n",
       "      <th>unique_words_count</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>chorus_count</th>\n",
       "      <th>verse_count</th>\n",
       "      <th>verse_chorus_ratio</th>\n",
       "      <th>pre_chorus_count</th>\n",
       "      <th>outro_count</th>\n",
       "      <th>bridge_count</th>\n",
       "      <th>hook_count</th>\n",
       "      <th>bigrams_from_60s</th>\n",
       "      <th>bigrams_from_70s</th>\n",
       "      <th>bigrams_from_80s</th>\n",
       "      <th>bigrams_from_90s</th>\n",
       "      <th>bigrams_from_00s</th>\n",
       "      <th>bigrams_from_10s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11947</th>\n",
       "      <td>648</td>\n",
       "      <td>97</td>\n",
       "      <td>57</td>\n",
       "      <td>-0.8591</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17460</th>\n",
       "      <td>1570</td>\n",
       "      <td>264</td>\n",
       "      <td>156</td>\n",
       "      <td>0.9971</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>76</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17464</th>\n",
       "      <td>885</td>\n",
       "      <td>164</td>\n",
       "      <td>74</td>\n",
       "      <td>0.9786</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8924</th>\n",
       "      <td>1182</td>\n",
       "      <td>223</td>\n",
       "      <td>75</td>\n",
       "      <td>0.9961</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7759</th>\n",
       "      <td>239</td>\n",
       "      <td>46</td>\n",
       "      <td>32</td>\n",
       "      <td>-0.8635</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11685</th>\n",
       "      <td>685</td>\n",
       "      <td>122</td>\n",
       "      <td>57</td>\n",
       "      <td>0.9937</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9629</th>\n",
       "      <td>1796</td>\n",
       "      <td>298</td>\n",
       "      <td>132</td>\n",
       "      <td>-0.9971</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8988</th>\n",
       "      <td>329</td>\n",
       "      <td>60</td>\n",
       "      <td>24</td>\n",
       "      <td>0.9911</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1526</th>\n",
       "      <td>762</td>\n",
       "      <td>129</td>\n",
       "      <td>64</td>\n",
       "      <td>0.9990</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1851</th>\n",
       "      <td>413</td>\n",
       "      <td>69</td>\n",
       "      <td>47</td>\n",
       "      <td>0.9565</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12437 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       character_count  word_count  unique_words_count  sentiment  \\\n",
       "11947              648          97                  57    -0.8591   \n",
       "17460             1570         264                 156     0.9971   \n",
       "17464              885         164                  74     0.9786   \n",
       "8924              1182         223                  75     0.9961   \n",
       "7759               239          46                  32    -0.8635   \n",
       "...                ...         ...                 ...        ...   \n",
       "11685              685         122                  57     0.9937   \n",
       "9629              1796         298                 132    -0.9971   \n",
       "8988               329          60                  24     0.9911   \n",
       "1526               762         129                  64     0.9990   \n",
       "1851               413          69                  47     0.9565   \n",
       "\n",
       "       chorus_count  verse_count  verse_chorus_ratio  pre_chorus_count  \\\n",
       "11947             2            2            1.000000                 0   \n",
       "17460             3            3            1.000000                 0   \n",
       "17464             3            2            0.666667                 2   \n",
       "8924              3            2            0.666667                 2   \n",
       "7759              0            0            0.000000                 0   \n",
       "...             ...          ...                 ...               ...   \n",
       "11685             3            3            1.000000                 0   \n",
       "9629              3            3            1.000000                 0   \n",
       "8988              0            3            0.000000                 0   \n",
       "1526              3            2            0.666667                 2   \n",
       "1851              2            2            1.000000                 0   \n",
       "\n",
       "       outro_count  bridge_count  hook_count  bigrams_from_60s  \\\n",
       "11947            1             0           0                 0   \n",
       "17460            0             0           0                 0   \n",
       "17464            1             0           0                 0   \n",
       "8924             1             1           0                 5   \n",
       "7759             0             0           0                 1   \n",
       "...            ...           ...         ...               ...   \n",
       "11685            1             0           0                 2   \n",
       "9629             1             0           0                 0   \n",
       "8988             0             2           0                 2   \n",
       "1526             1             2           0                 0   \n",
       "1851             1             0           0                 1   \n",
       "\n",
       "       bigrams_from_70s  bigrams_from_80s  bigrams_from_90s  bigrams_from_00s  \\\n",
       "11947                 1                 0                 0                 1   \n",
       "17460                 1                 0                76                 2   \n",
       "17464                 0                 2                 5                 0   \n",
       "8924                  2                 0                 2                10   \n",
       "7759                  0                 0                 0                 1   \n",
       "...                 ...               ...               ...               ...   \n",
       "11685                 0                17                 0                 0   \n",
       "9629                  2                 4                 0                 0   \n",
       "8988                  0                 0                 0                 0   \n",
       "1526                  3                 1                 3                 2   \n",
       "1851                  3                 1                 0                 0   \n",
       "\n",
       "       bigrams_from_10s  \n",
       "11947                 0  \n",
       "17460                 0  \n",
       "17464                 2  \n",
       "8924                  1  \n",
       "7759                  2  \n",
       "...                 ...  \n",
       "11685                 1  \n",
       "9629                 58  \n",
       "8988                  2  \n",
       "1526                  1  \n",
       "1851                  0  \n",
       "\n",
       "[12437 rows x 17 columns]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(file=\"songs_0526.csv\", use_cache = True):\n",
    "    \"\"\"\n",
    "    Get data from csv file,\n",
    "    clean the data,\n",
    "    add features\n",
    "    return df\n",
    "    \"\"\"\n",
    "    filename = \"preppred_songs.csv\"\n",
    "    if os.path.isfile(filename) and use_cache:\n",
    "        print(\"Reading from csv...\")\n",
    "        return pd.read_csv(filename)\n",
    "    # read csv file\n",
    "    print(\"reading csv file...\", end=\"\\r\")\n",
    "    df = pd.read_csv(file)\n",
    "    # clean data\n",
    "    df = prep.clean_df(df)\n",
    "    # add features\n",
    "    df = prep.add_features(df)\n",
    "    # add topics\n",
    "    df = prep.get_topics(df)\n",
    "    print(\"Saving to csv in local directory...\")\n",
    "    df.to_csv(filename, index=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features added ******************\n",
      "topics added                        \n",
      "Saving to csv in local directory...\n"
     ]
    }
   ],
   "source": [
    "df=prep.get_data(use_cache=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading from csv...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>artist</th>\n",
       "      <th>date</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>raw_lyrics</th>\n",
       "      <th>decade</th>\n",
       "      <th>character_count</th>\n",
       "      <th>word_count</th>\n",
       "      <th>unique_words</th>\n",
       "      <th>unique_words_count</th>\n",
       "      <th>...</th>\n",
       "      <th>chorus_count</th>\n",
       "      <th>verse_count</th>\n",
       "      <th>verse_chorus_ratio</th>\n",
       "      <th>pre_chorus_count</th>\n",
       "      <th>outro_count</th>\n",
       "      <th>bridge_count</th>\n",
       "      <th>hook_count</th>\n",
       "      <th>bigrams</th>\n",
       "      <th>trigrams</th>\n",
       "      <th>topic_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#1</td>\n",
       "      <td>Nelly</td>\n",
       "      <td>2001-10-20</td>\n",
       "      <td>uh uh uh got bring attention dirty better watc...</td>\n",
       "      <td>#1 LyricsUh uh uh I just gotta bring it to the...</td>\n",
       "      <td>2000</td>\n",
       "      <td>2014</td>\n",
       "      <td>363</td>\n",
       "      <td>word erything chat pant man eh rapper want hal...</td>\n",
       "      <td>228</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>[('uh', 'uh'), ('uh', 'uh'), ('uh', 'got'), ('...</td>\n",
       "      <td>[('uh', 'uh', 'uh'), ('uh', 'uh', 'got'), ('uh...</td>\n",
       "      <td>sex</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#9 Dream</td>\n",
       "      <td>John Lennon</td>\n",
       "      <td>1974-12-21</td>\n",
       "      <td>long ago dream dream know yes know seemed real...</td>\n",
       "      <td>#9 Dream Lyrics[Verse 1] So long ago Was it in...</td>\n",
       "      <td>1970</td>\n",
       "      <td>842</td>\n",
       "      <td>135</td>\n",
       "      <td>warm cold feel whispered strange tree hear dan...</td>\n",
       "      <td>49</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[('long', 'ago'), ('ago', 'dream'), ('dream', ...</td>\n",
       "      <td>[('long', 'ago', 'dream'), ('ago', 'dream', 'd...</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#Beautiful</td>\n",
       "      <td>Mariah Carey Featuring Miguel</td>\n",
       "      <td>2013-05-25</td>\n",
       "      <td>ah ah beautiful ah ah beautiful hop back bike ...</td>\n",
       "      <td>#Beautiful Lyrics[Intro: Mariah Carey] Ah, ah,...</td>\n",
       "      <td>2010</td>\n",
       "      <td>768</td>\n",
       "      <td>129</td>\n",
       "      <td>stop feel wind dance lord moonlight thing ligh...</td>\n",
       "      <td>54</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[('ah', 'ah'), ('ah', 'beautiful'), ('beautifu...</td>\n",
       "      <td>[('ah', 'ah', 'beautiful'), ('ah', 'beautiful'...</td>\n",
       "      <td>lost</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#SELFIE</td>\n",
       "      <td>The Chainsmokers</td>\n",
       "      <td>2014-03-15</td>\n",
       "      <td>jason table kept seeing look girl think make j...</td>\n",
       "      <td>#SELFIE Lyrics[Verse 1] When Jason was at the ...</td>\n",
       "      <td>2010</td>\n",
       "      <td>954</td>\n",
       "      <td>172</td>\n",
       "      <td>filter want smoke see valencia okay oh pause g...</td>\n",
       "      <td>100</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[('jason', 'table'), ('table', 'kept'), ('kept...</td>\n",
       "      <td>[('jason', 'table', 'kept'), ('table', 'kept',...</td>\n",
       "      <td>violence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#thatPOWER</td>\n",
       "      <td>will.i.am Featuring Justin Bieber</td>\n",
       "      <td>2013-04-06</td>\n",
       "      <td>oh alive alive alive oh fly fly fly oh alive a...</td>\n",
       "      <td>#thatPOWER Lyrics[Instrumental break]  [Pre-Ch...</td>\n",
       "      <td>2010</td>\n",
       "      <td>1238</td>\n",
       "      <td>208</td>\n",
       "      <td>bigger bank oh good hustle used alive door lov...</td>\n",
       "      <td>77</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[('oh', 'alive'), ('alive', 'alive'), ('alive'...</td>\n",
       "      <td>[('oh', 'alive', 'alive'), ('alive', 'alive', ...</td>\n",
       "      <td>sex</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23757</th>\n",
       "      <td>Which Way You Goin' Billy?</td>\n",
       "      <td>The Poppy Family (Featuring Susan Jacks)</td>\n",
       "      <td>1970-03-28</td>\n",
       "      <td>way going billy go way going billy go really l...</td>\n",
       "      <td>Which Way You Goin’ Billy? LyricsWhich way you...</td>\n",
       "      <td>1970</td>\n",
       "      <td>535</td>\n",
       "      <td>98</td>\n",
       "      <td>go soul want whole would thought billy show lo...</td>\n",
       "      <td>36</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[('way', 'going'), ('going', 'billy'), ('billy...</td>\n",
       "      <td>[('way', 'going', 'billy'), ('going', 'billy',...</td>\n",
       "      <td>affection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23758</th>\n",
       "      <td>White Christmas</td>\n",
       "      <td>The Drifters Featuring Clyde McPhatter And Bil...</td>\n",
       "      <td>1960-12-19</td>\n",
       "      <td>ooh doop doop doop doo doop ooh doop doop doop...</td>\n",
       "      <td>White Christmas LyricsOoh Doop doop, doop doo ...</td>\n",
       "      <td>1960</td>\n",
       "      <td>526</td>\n",
       "      <td>90</td>\n",
       "      <td>hear ooooh ooh sleigh treetop dreaming may jin...</td>\n",
       "      <td>29</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[('ooh', 'doop'), ('doop', 'doop'), ('doop', '...</td>\n",
       "      <td>[('ooh', 'doop', 'doop'), ('doop', 'doop', 'do...</td>\n",
       "      <td>holiday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23759</th>\n",
       "      <td>Why Don't You &amp; I</td>\n",
       "      <td>Santana Featuring Alex Band Or Chad Kroeger</td>\n",
       "      <td>2003-07-26</td>\n",
       "      <td>since moment spotted like walking round little...</td>\n",
       "      <td>Why Don’t You &amp; I Lyrics[Verse 1: Chad Kroeger...</td>\n",
       "      <td>2000</td>\n",
       "      <td>1146</td>\n",
       "      <td>195</td>\n",
       "      <td>moon go walking butterfly man ooh hold heaven ...</td>\n",
       "      <td>65</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[('since', 'moment'), ('moment', 'spotted'), (...</td>\n",
       "      <td>[('since', 'moment', 'spotted'), ('moment', 's...</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23760</th>\n",
       "      <td>Wild Wild West</td>\n",
       "      <td>Will Smith Featuring Dru Hill &amp; Kool Mo Dee</td>\n",
       "      <td>1999-05-22</td>\n",
       "      <td>mmmts uhh wickiwild wild wickiwickiwild wickiw...</td>\n",
       "      <td>Wild Wild West Lyrics[Intro: Will Smith &amp; (Sis...</td>\n",
       "      <td>1990</td>\n",
       "      <td>2269</td>\n",
       "      <td>408</td>\n",
       "      <td>defender tree want man quickest see buffalo go...</td>\n",
       "      <td>170</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[('mmmts', 'uhh'), ('uhh', 'wickiwild'), ('wic...</td>\n",
       "      <td>[('mmmts', 'uhh', 'wickiwild'), ('uhh', 'wicki...</td>\n",
       "      <td>lost</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23761</th>\n",
       "      <td>Witch Doctor (2007)</td>\n",
       "      <td>Alvin And The Chipmunks Featuring Chris Classic</td>\n",
       "      <td>2008-01-12</td>\n",
       "      <td>movie version dj yeah place chipmunk mic witch...</td>\n",
       "      <td>Witch Doctor Lyrics{Movie version - DJ: Yeah! ...</td>\n",
       "      <td>2000</td>\n",
       "      <td>1406</td>\n",
       "      <td>276</td>\n",
       "      <td>doctor miser dj go ooo advice keeping place oo...</td>\n",
       "      <td>63</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[('movie', 'version'), ('version', 'dj'), ('dj...</td>\n",
       "      <td>[('movie', 'version', 'dj'), ('version', 'dj',...</td>\n",
       "      <td>transcendental</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23762 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            title  \\\n",
       "0                              #1   \n",
       "1                        #9 Dream   \n",
       "2                      #Beautiful   \n",
       "3                         #SELFIE   \n",
       "4                      #thatPOWER   \n",
       "...                           ...   \n",
       "23757  Which Way You Goin' Billy?   \n",
       "23758             White Christmas   \n",
       "23759           Why Don't You & I   \n",
       "23760              Wild Wild West   \n",
       "23761         Witch Doctor (2007)   \n",
       "\n",
       "                                                  artist        date  \\\n",
       "0                                                  Nelly  2001-10-20   \n",
       "1                                            John Lennon  1974-12-21   \n",
       "2                          Mariah Carey Featuring Miguel  2013-05-25   \n",
       "3                                       The Chainsmokers  2014-03-15   \n",
       "4                      will.i.am Featuring Justin Bieber  2013-04-06   \n",
       "...                                                  ...         ...   \n",
       "23757           The Poppy Family (Featuring Susan Jacks)  1970-03-28   \n",
       "23758  The Drifters Featuring Clyde McPhatter And Bil...  1960-12-19   \n",
       "23759        Santana Featuring Alex Band Or Chad Kroeger  2003-07-26   \n",
       "23760        Will Smith Featuring Dru Hill & Kool Mo Dee  1999-05-22   \n",
       "23761    Alvin And The Chipmunks Featuring Chris Classic  2008-01-12   \n",
       "\n",
       "                                                  lyrics  \\\n",
       "0      uh uh uh got bring attention dirty better watc...   \n",
       "1      long ago dream dream know yes know seemed real...   \n",
       "2      ah ah beautiful ah ah beautiful hop back bike ...   \n",
       "3      jason table kept seeing look girl think make j...   \n",
       "4      oh alive alive alive oh fly fly fly oh alive a...   \n",
       "...                                                  ...   \n",
       "23757  way going billy go way going billy go really l...   \n",
       "23758  ooh doop doop doop doo doop ooh doop doop doop...   \n",
       "23759  since moment spotted like walking round little...   \n",
       "23760  mmmts uhh wickiwild wild wickiwickiwild wickiw...   \n",
       "23761  movie version dj yeah place chipmunk mic witch...   \n",
       "\n",
       "                                              raw_lyrics  decade  \\\n",
       "0      #1 LyricsUh uh uh I just gotta bring it to the...    2000   \n",
       "1      #9 Dream Lyrics[Verse 1] So long ago Was it in...    1970   \n",
       "2      #Beautiful Lyrics[Intro: Mariah Carey] Ah, ah,...    2010   \n",
       "3      #SELFIE Lyrics[Verse 1] When Jason was at the ...    2010   \n",
       "4      #thatPOWER Lyrics[Instrumental break]  [Pre-Ch...    2010   \n",
       "...                                                  ...     ...   \n",
       "23757  Which Way You Goin’ Billy? LyricsWhich way you...    1970   \n",
       "23758  White Christmas LyricsOoh Doop doop, doop doo ...    1960   \n",
       "23759  Why Don’t You & I Lyrics[Verse 1: Chad Kroeger...    2000   \n",
       "23760  Wild Wild West Lyrics[Intro: Will Smith & (Sis...    1990   \n",
       "23761  Witch Doctor Lyrics{Movie version - DJ: Yeah! ...    2000   \n",
       "\n",
       "       character_count  word_count  \\\n",
       "0                 2014         363   \n",
       "1                  842         135   \n",
       "2                  768         129   \n",
       "3                  954         172   \n",
       "4                 1238         208   \n",
       "...                ...         ...   \n",
       "23757              535          98   \n",
       "23758              526          90   \n",
       "23759             1146         195   \n",
       "23760             2269         408   \n",
       "23761             1406         276   \n",
       "\n",
       "                                            unique_words  unique_words_count  \\\n",
       "0      word erything chat pant man eh rapper want hal...                 228   \n",
       "1      warm cold feel whispered strange tree hear dan...                  49   \n",
       "2      stop feel wind dance lord moonlight thing ligh...                  54   \n",
       "3      filter want smoke see valencia okay oh pause g...                 100   \n",
       "4      bigger bank oh good hustle used alive door lov...                  77   \n",
       "...                                                  ...                 ...   \n",
       "23757  go soul want whole would thought billy show lo...                  36   \n",
       "23758  hear ooooh ooh sleigh treetop dreaming may jin...                  29   \n",
       "23759  moon go walking butterfly man ooh hold heaven ...                  65   \n",
       "23760  defender tree want man quickest see buffalo go...                 170   \n",
       "23761  doctor miser dj go ooo advice keeping place oo...                  63   \n",
       "\n",
       "       ...  chorus_count verse_count verse_chorus_ratio  pre_chorus_count  \\\n",
       "0      ...             0           0           0.000000                 0   \n",
       "1      ...             2           2           1.000000                 2   \n",
       "2      ...             2           2           1.000000                 0   \n",
       "3      ...             0           3           0.000000                 0   \n",
       "4      ...             3           2           0.666667                 3   \n",
       "...    ...           ...         ...                ...               ...   \n",
       "23757  ...             0           0           0.000000                 0   \n",
       "23758  ...             0           0           0.000000                 0   \n",
       "23759  ...             4           2           0.500000                 3   \n",
       "23760  ...             1           3           3.000000                 1   \n",
       "23761  ...             0           0           0.000000                 0   \n",
       "\n",
       "       outro_count  bridge_count  hook_count  \\\n",
       "0                0             0           3   \n",
       "1                1             0           0   \n",
       "2                1             0           0   \n",
       "3                0             0           0   \n",
       "4                1             1           0   \n",
       "...            ...           ...         ...   \n",
       "23757            0             0           0   \n",
       "23758            0             0           0   \n",
       "23759            0             1           0   \n",
       "23760            0             0           0   \n",
       "23761            0             0           0   \n",
       "\n",
       "                                                 bigrams  \\\n",
       "0      [('uh', 'uh'), ('uh', 'uh'), ('uh', 'got'), ('...   \n",
       "1      [('long', 'ago'), ('ago', 'dream'), ('dream', ...   \n",
       "2      [('ah', 'ah'), ('ah', 'beautiful'), ('beautifu...   \n",
       "3      [('jason', 'table'), ('table', 'kept'), ('kept...   \n",
       "4      [('oh', 'alive'), ('alive', 'alive'), ('alive'...   \n",
       "...                                                  ...   \n",
       "23757  [('way', 'going'), ('going', 'billy'), ('billy...   \n",
       "23758  [('ooh', 'doop'), ('doop', 'doop'), ('doop', '...   \n",
       "23759  [('since', 'moment'), ('moment', 'spotted'), (...   \n",
       "23760  [('mmmts', 'uhh'), ('uhh', 'wickiwild'), ('wic...   \n",
       "23761  [('movie', 'version'), ('version', 'dj'), ('dj...   \n",
       "\n",
       "                                                trigrams      topic_name  \n",
       "0      [('uh', 'uh', 'uh'), ('uh', 'uh', 'got'), ('uh...             sex  \n",
       "1      [('long', 'ago', 'dream'), ('ago', 'dream', 'd...            love  \n",
       "2      [('ah', 'ah', 'beautiful'), ('ah', 'beautiful'...            lost  \n",
       "3      [('jason', 'table', 'kept'), ('table', 'kept',...        violence  \n",
       "4      [('oh', 'alive', 'alive'), ('alive', 'alive', ...             sex  \n",
       "...                                                  ...             ...  \n",
       "23757  [('way', 'going', 'billy'), ('going', 'billy',...       affection  \n",
       "23758  [('ooh', 'doop', 'doop'), ('doop', 'doop', 'do...         holiday  \n",
       "23759  [('since', 'moment', 'spotted'), ('moment', 's...            love  \n",
       "23760  [('mmmts', 'uhh', 'wickiwild'), ('uhh', 'wicki...            lost  \n",
       "23761  [('movie', 'version', 'dj'), ('version', 'dj',...  transcendental  \n",
       "\n",
       "[23762 rows x 23 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = prep.get_data()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3067ead486e059ec00ffe7555bdb889e6e264a24dc711bf108106cc7baee8d5d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
