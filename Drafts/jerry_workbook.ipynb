{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6000f56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Unicode, Regex, json for text digestion\n",
    "import unicodedata\n",
    "import re\n",
    "import json\n",
    "\n",
    "import datetime\n",
    "# Time formatting\n",
    "from time import strftime\n",
    "\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# nltk: natural language toolkit -> tokenization, stopwords\n",
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer as stemmer\n",
    "# nltk.download('stopwords')\n",
    "import nltk.sentiment\n",
    "sia = nltk.sentiment.SentimentIntensityAnalyzer()\n",
    "\n",
    "# Quieeet!!! Y'all can't stop me now...\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9740e14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import draft_prepare as p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac5ce7b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23762, 5)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('songs_0526.csv')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e551bcdb",
   "metadata": {},
   "source": [
    "### Preparing the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47098db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = p.clean_df(df, extra_words = [], exclude_words = [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c23494d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>artist</th>\n",
       "      <th>date</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>decade</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>#1</td>\n",
       "      <td>Nelly</td>\n",
       "      <td>2001-10-20</td>\n",
       "      <td>uh uh uh got bring attention dirty better watc...</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>#9 Dream</td>\n",
       "      <td>John Lennon</td>\n",
       "      <td>1974-12-21</td>\n",
       "      <td>long ago dream dream know yes know seemed real...</td>\n",
       "      <td>1970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>#Beautiful</td>\n",
       "      <td>Mariah Carey Featuring Miguel</td>\n",
       "      <td>2013-05-25</td>\n",
       "      <td>ah ah beautiful ah ah beautiful hop back bike ...</td>\n",
       "      <td>2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>#SELFIE</td>\n",
       "      <td>The Chainsmokers</td>\n",
       "      <td>2014-03-15</td>\n",
       "      <td>jason table kept seeing look girl think make j...</td>\n",
       "      <td>2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>#thatPOWER</td>\n",
       "      <td>will.i.am Featuring Justin Bieber</td>\n",
       "      <td>2013-04-06</td>\n",
       "      <td>oh alive alive alive oh fly fly fly oh alive a...</td>\n",
       "      <td>2010</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0       title                             artist       date  \\\n",
       "0           2          #1                              Nelly 2001-10-20   \n",
       "1           4    #9 Dream                        John Lennon 1974-12-21   \n",
       "2           5  #Beautiful      Mariah Carey Featuring Miguel 2013-05-25   \n",
       "3           6     #SELFIE                   The Chainsmokers 2014-03-15   \n",
       "4           7  #thatPOWER  will.i.am Featuring Justin Bieber 2013-04-06   \n",
       "\n",
       "                                              lyrics  decade  \n",
       "0  uh uh uh got bring attention dirty better watc...    2000  \n",
       "1  long ago dream dream know yes know seemed real...    1970  \n",
       "2  ah ah beautiful ah ah beautiful hop back bike ...    2010  \n",
       "3  jason table kept seeing look girl think make j...    2010  \n",
       "4  oh alive alive alive oh fly fly fly oh alive a...    2010  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = p.model_clean(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "64cb7be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # What song has the lowest sentiment?\n",
    "# (df.sort_values(by = ['sentiment'], ascending = True).head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8970527c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # What song has the highest sentiment?\n",
    "# df.sort_values(by = ['sentiment'], ascending = False).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8411c6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Average sentiment per decade?\n",
    "# df.groupby(['decade'])['sentiment'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eecdfb2e",
   "metadata": {},
   "source": [
    "## Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff4e247",
   "metadata": {},
   "source": [
    "#### Latent Dirichlet Allocation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "09e50afa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Shape of the sparse matrix\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<22210x24547 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 1263109 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an instance\n",
    "cv = CountVectorizer(max_df = .95, min_df = 2, stop_words = 'english')\n",
    "\n",
    "# Fit and transform the lemmatized lyrics data\n",
    "cv_fit = cv.fit_transform(df.lyrics)\n",
    "\n",
    "print('\\nShape of the sparse matrix\\n')\n",
    "cv_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "179104c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of topics: 20\n",
      "Number of columns of the LDA fit 24547\n"
     ]
    }
   ],
   "source": [
    "# Create the instance for LDA\n",
    "lda = LatentDirichletAllocation(n_components = 20, random_state = 42)\n",
    "\n",
    "# Fit the vectorizer with the LDA\n",
    "lda.fit(cv_fit)\n",
    "\n",
    "print('Number of topics:', len(lda.components_))\n",
    "print('Number of columns of the LDA fit', len(lda.components_[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "39afde02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of feature names: 24547\n"
     ]
    }
   ],
   "source": [
    "feature = cv.get_feature_names()\n",
    "\n",
    "print('Length of feature names:', len(feature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3185f8e9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 50 words in topic 0\n",
      "-------------------------\n",
      "['understand', 'hurt', 'treat', 'man', 'good', 'yeah', 'mean', 'di', 'hand', 'bad', 'talk', 'look', 'inside', 'believe', 'try', 'boy', 'heart', 'hold', 'oh', 'friend', 'care', 'mind', 'time', 'feeling', 'lie', 'true', 'right', 'girl', 'come', 'better', 'fool', 'baby', 'touch', 'thing', 'real', 'let', 'somebody', 'make', 'think', 'way', 'really', 'got', 'like', 'say', 'love', 'feel', 'tell', 'need', 'want', 'know'] \n",
      "\n",
      "\n",
      "Top 50 words in topic 1\n",
      "-------------------------\n",
      "['easy', 'live', 'dream', 'word', 'start', 'feeling', 'goodbye', 'darling', 'wait', 'eye', 'need', 'kiss', 'lose', 'away', 'arm', 'come', 'think', 'hurt', 'believe', 'fall', 'hard', 'long', 'mind', 'thing', 'want', 'change', 'loving', 'break', 'true', 'feel', 'hold', 'like', 'day', 'oh', 'try', 'forever', 'baby', 'life', 'stay', 'right', 'got', 'say', 'going', 'let', 'way', 'know', 'make', 'time', 'heart', 'love'] \n",
      "\n",
      "\n",
      "Top 50 words in topic 2\n",
      "-------------------------\n",
      "['loving', 'mow', 'huh', 'watching', 'alright', 'mind', 'ow', 'mean', 'time', 'listen', 'mmm', 'bring', 'eh', 'life', 'feel', 'ohohoh', 'sweet', 'mercy', 'bom', 'love', 'woah', 'girl', 'say', 'ah', 'let', 'night', 'hoo', 'uh', 'right', 'good', 'yes', 'lord', 'going', 'thing', 'way', 'like', 'said', 'oooh', 'baby', 'make', 'want', 'come', 'ohoh', 'know', 'stop', 'got', 'whoa', 'hey', 'yeah', 'oh'] \n",
      "\n",
      "\n",
      "Top 50 words in topic 3\n",
      "-------------------------\n",
      "['tell', 'mr', 'lil', 'rock', 'boy', 'way', 'ay', 'fresh', 'come', 'thang', 'break', 'shorty', 'big', 'uh', 'yeah', 'say', 'drink', 'hoe', 'throw', 'bye', 'girl', 'imma', 'hand', 'bout', 'fuck', 'booty', 'shawty', 'bounce', 'pop', 'shit', 'step', 'man', 'right', 'hit', 'drop', 'bitch', 'hey', 'let', 'know', 'wit', 'low', 'club', 'want', 'got', 'make', 'yo', 'gon', 'like', 'nigga', 'ya'] \n",
      "\n",
      "\n",
      "Top 50 words in topic 4\n",
      "-------------------------\n",
      "['truck', 'ride', 'long', 'city', 'know', 'beer', 'jean', 'brother', 'river', 'good', 'drive', 'drink', 'lot', 'look', 'hey', 'runnin', 'night', 'red', 'say', 'blue', 'cold', 'black', 'car', 'hand', 'wild', 'wine', 'roll', 'man', 'right', 'come', 'soul', 'time', 'way', 'big', 'road', 'home', 'boy', 'country', 'said', 'got', 'daddy', 'town', 'like', 'going', 'bit', 'mama', 'rock', 'old', 'na', 'little'] \n",
      "\n",
      "\n",
      "Top 50 words in topic 5\n",
      "-------------------------\n",
      "['believe', 'year', 'home', 'saw', 'sorry', 'lie', 'oh', 'end', 'let', 'door', 'mean', 'long', 'coming', 'best', 'remember', 'mind', 'make', 'fall', 'guess', 'told', 'right', 'better', 'stand', 'come', 'god', 'knew', 'left', 'feel', 'head', 'day', 'used', 'face', 'tell', 'eye', 'thought', 'friend', 'wrong', 'like', 'leave', 'way', 'look', 'say', 'think', 'said', 'know', 'gone', 'good', 'thing', 'life', 'time'] \n",
      "\n",
      "\n",
      "Top 50 words in topic 6\n",
      "-------------------------\n",
      "['place', 'trouble', 'born', 'want', 'dream', 'midnight', 'gone', 'sleep', 'memory', 'walking', 'mile', 'let', 'mind', 'life', 'stay', 'follow', 'come', 'forget', 'say', 'set', 'heart', 'dark', 'way', 'road', 'alright', 'left', 'save', 'today', 'tear', 'far', 'lost', 'time', 'tomorrow', 'right', 'lonely', 'got', 'eye', 'light', 'like', 'day', 'talk', 'run', 'free', 'know', 'long', 'walk', 'tonight', 'going', 'night', 'away'] \n",
      "\n",
      "\n",
      "Top 50 words in topic 7\n",
      "-------------------------\n",
      "['watch', 'run', 'young', 'face', 'need', 'right', 'told', 'talk', 'whip', 'stop', 'rap', 'way', 'diamond', 'play', 'lil', 'yo', 'let', 'life', 'come', 'tell', 'bout', 'talkin', 'feel', 'god', 'time', 'king', 'hard', 'say', 'going', 'man', 'tryna', 'gettin', 'fuckin', 'real', 'hit', 'money', 'look', 'ayy', 'check', 'gon', 'make', 'fuck', 'know', 'want', 'got', 'yeah', 'shit', 'bitch', 'nigga', 'like'] \n",
      "\n",
      "\n",
      "Top 50 words in topic 8\n",
      "-------------------------\n",
      "['ground', 'living', 'smile', 'bird', 'bright', 'welcome', 'earth', 'sea', 'wind', 'dancing', 'feel', 'wing', 'york', 'peace', 'time', 'boogie', 'die', 'cloud', 'people', 'away', 'let', 'higher', 'mountain', 'eye', 'life', 'heaven', 'magic', 'make', 'hand', 'sunshine', 'moon', 'dream', 'place', 'believe', 'rain', 'child', 'wish', 'blue', 'shine', 'star', 'new', 'day', 'light', 'fly', 'live', 'sun', 'high', 'sky', 'come', 'world'] \n",
      "\n",
      "\n",
      "Top 50 words in topic 9\n",
      "-------------------------\n",
      "['miracle', 'daughter', 'sister', 'soldier', 'born', 'plan', 'lip', 'land', 'good', 'band', 'eye', 'say', 'little', 'brother', 'time', 'life', 'letter', 'lord', 'boy', 'like', 'johnny', 'want', 'better', 'arm', 'make', 'blue', 'understand', 'mirror', 'hero', 'son', 'dream', 'poor', 'look', 'know', 'lie', 'street', 'yes', 'loving', 'living', 'going', 'hand', 'said', 'men', 'got', 'happy', 'people', 'young', 'hold', 'woman', 'man'] \n",
      "\n",
      "\n",
      "Top 50 words in topic 10\n",
      "-------------------------\n",
      "['lit', 'need', 'tell', 'die', 'different', 'boat', 'tryna', 'bank', 'ayy', 'hoe', 'hate', 'woo', 'smoke', 'say', 'friend', 'car', 'blood', 'choo', 'stupid', 'way', 'water', 'gon', 'think', 'want', 'ride', 'hallelujah', 'gang', 'got', 'train', 'bout', 'pull', 'shit', 'bag', 'know', 'black', 'city', 'real', 'new', 'like', 'ho', 'huh', 'fuck', 'nigga', 'bitch', 'really', 'money', 'big', 'uh', 'bad', 'yeah'] \n",
      "\n",
      "\n",
      "Top 50 words in topic 11\n",
      "-------------------------\n",
      "['cmon', 'fine', 'home', 'drive', 'hold', 'tight', 'feeling', 'honey', 'hand', 'loving', 'dancing', 'mind', 'need', 'long', 'tell', 'babe', 'way', 'oh', 'hey', 'slow', 'look', 'love', 'thing', 'ready', 'little', 'feel', 'crazy', 'kiss', 'lady', 'alright', 'say', 'body', 'night', 'time', 'know', 'girl', 'shake', 'make', 'tonight', 'let', 'good', 'like', 'want', 'dance', 'right', 'got', 'going', 'yeah', 'come', 'baby'] \n",
      "\n",
      "\n",
      "Top 50 words in topic 12\n",
      "-------------------------\n",
      "['heard', 'let', 'life', 'word', 'right', 'lie', 'feeling', 'voice', 'sleep', 'close', 'day', 'sound', 'thing', 'running', 'game', 'make', 'singing', 'long', 'coming', 'thinking', 'playing', 'woah', 'morning', 'time', 'feel', 'come', 'radio', 'eye', 'say', 'know', 'boom', 'light', 'miss', 'beat', 'got', 'boy', 'night', 'sweet', 'music', 'hear', 'turn', 'love', 'want', 'way', 'play', 'sing', 'dream', 'like', 'song', 'home'] \n",
      "\n",
      "\n",
      "Top 50 words in topic 13\n",
      "-------------------------\n",
      "['shout', 'night', 'burn', 'control', 'movin', 'way', 'jam', 'ready', 'time', 'bump', 'feel', 'chain', 'hit', 'heat', 'fun', 'people', 'know', 'funk', 'loose', 'rhythm', 'right', 'going', 'beat', 'watch', 'foot', 'sexy', 'music', 'yeah', 'nah', 'turn', 'everybody', 'funky', 'freak', 'floor', 'dance', 'stop', 'ride', 'bring', 'rock', 'make', 'got', 'body', 'roll', 'doo', 'like', 'round', 'hot', 'party', 'want', 'let'] \n",
      "\n",
      "\n",
      "Top 50 words in topic 14\n",
      "-------------------------\n",
      "['downtown', 'getting', 'joe', 'rainy', 'sittin', 'clap', 'look', 'bird', 'bar', 'big', 'people', 'good', 'saturday', 'ahead', 'come', 'everybodys', 'think', 'monday', 'raise', 'tell', 'sit', 'said', 'right', 'bout', 'mornin', 'dat', 'weekend', 'hey', 'going', 'friday', 'feelin', 'school', 'talkin', 'walkin', 'say', 'mmm', 'yeah', 'sunday', 'know', 'like', 'wait', 'til', 'kid', 'night', 'way', 'little', 'got', 'help', 'everybody', 'day'] \n",
      "\n",
      "\n",
      "Top 50 words in topic 15\n",
      "-------------------------\n",
      "['make', 'ron', 'ey', 'ecstasy', 'impossible', 'away', 'ahh', 'doctor', 'faded', 'heart', 'edge', 'got', 'sweet', 'ring', 'oohoohooh', 'coo', 'heartbeat', 'heaven', 'halo', 'pump', 'way', 'life', 'feeling', 'loved', 'feelin', 'eye', 'desire', 'bell', 'hush', 'oh', 'motion', 'push', 'day', 'fine', 'burning', 'cool', 'woo', 'burnin', 'oohooh', 'wake', 'love', 'livin', 'falling', 'alive', 'feel', 'like', 'angel', 'yeah', 'ah', 'ooh'] \n",
      "\n",
      "\n",
      "Top 50 words in topic 16\n",
      "-------------------------\n",
      "['gon', 'head', 'hood', 'diamond', 'pussy', 'tryna', 'hoe', 'stop', 'pop', 'come', 'ice', 'way', 'rich', 'ride', 'right', 'street', 'hard', 'new', 'mad', 'thing', 'big', 'sex', 'said', 'game', 'car', 'need', 'damn', 'want', 'lot', 'girl', 'gun', 'say', 'man', 'jump', 'lookin', 'bitch', 'fuck', 'shit', 'make', 'hit', 'time', 'better', 'work', 'yeah', 'boy', 'like', 'money', 'know', 'nigga', 'got'] \n",
      "\n",
      "\n",
      "Top 50 words in topic 17\n",
      "-------------------------\n",
      "['right', 'hair', 'eye', 'stroke', 'dab', 'sweet', 'kind', 'come', 'kiss', 'good', 'candy', 'think', 'honey', 'said', 'fever', 'better', 'nana', 'kissed', 'ding', 'summertime', 'bubble', 'mr', 'dig', 'oh', 'let', 'swing', 'lean', 'ooo', 'fine', 'make', 'middle', 'know', 'dum', 'world', 'send', 'got', 'bang', 'look', 'going', 'lonely', 'like', 'guy', 'mind', 'summer', 'boy', 'little', 'ha', 'run', 'pretty', 'girl'] \n",
      "\n",
      "\n",
      "Top 50 words in topic 18\n",
      "-------------------------\n",
      "['cuerpo', 'para', 'cest', 'mami', 'eh', 'al', 'ay', 'va', 'ya', 'sin', 'loca', 'ella', 'chicken', 'dale', 'soy', 'pero', 'lady', 'los', 'ma', 'esta', 'sha', 'una', 'annie', 'lalalalala', 'lalalala', 'como', 'ta', 'ti', 'count', 'solo', 'por', 'amor', 'lalala', 'quiero', 'si', 'okay', 'lala', 'pa', 'le', 'en', 'se', 'mi', 'yo', 'lay', 'lo', 'te', 'el', 'tu', 'que', 'la'] \n",
      "\n",
      "\n",
      "Top 50 words in topic 19\n",
      "-------------------------\n",
      "['care', 'night', 'look', 'level', 'mind', 'tender', 'kiss', 'beep', 'baby', 'shot', 'thing', 'ready', 'girl', 'affair', 'face', 'life', 'work', 'make', 'bon', 'dee', 'hmm', 'new', 'fun', 'know', 'yeah', 'hate', 'giving', 'best', 'honey', 'going', 'time', 'power', 'like', 'good', 'need', 'got', 'day', 'kind', 'sugar', 'thank', 'way', 'loving', 'beautiful', 'ba', 'yes', 'sweet', 'lover', 'want', 'da', 'love'] \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for ind, topic in enumerate(lda.components_):\n",
    "    print('Top 50 words in topic {}'.format(ind))\n",
    "    print('-'*25)\n",
    "    top_50 = topic.argsort()[-50:]\n",
    "    print([feature[i] for i in top_50], '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "91a6db55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "# Final df transforming cv_fit\n",
    "df_final = lda.transform(cv_fit)\n",
    "\n",
    "# Make copy to save original df \n",
    "df_new = copy.deepcopy(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3895538a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking the probability distribution of one text data belonging to the topic.\n",
      "\n",
      "Few words from 1st row: uh uh uh got bring attention dirty better watch talkin bout runnin mouth like know gon f \n",
      "\n",
      "Probability distribution: [1.68918923e-04 1.68918923e-04 1.36420460e-01 2.90691474e-01\n",
      " 1.68918922e-04 3.55899462e-02 1.68918922e-04 2.81193071e-01\n",
      " 1.68918923e-04 1.68918923e-04 1.68918923e-04 1.68918923e-04\n",
      " 5.21967981e-02 2.22907729e-02 6.18119775e-02 1.68918920e-04\n",
      " 1.17778473e-01 1.68918924e-04 1.68918923e-04 1.68918924e-04]\n"
     ]
    }
   ],
   "source": [
    "print('\\nChecking the probability distribution of one text data belonging to the topic.\\n')\n",
    "\n",
    "print('Few words from 1st row:', df.lyrics[0][:88], '\\n')\n",
    "\n",
    "print('Probability distribution:', df_final[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "99674ee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document belong to the topic 3 with the probability of 0.39\n"
     ]
    }
   ],
   "source": [
    "prob = df_final[0][df_final[0].argmax()].round(2)\n",
    "\n",
    "print('Document belong to the topic', df_final[0].argmax(), 'with the probability of', prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3d62de3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>artist</th>\n",
       "      <th>date</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>decade</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>#1</td>\n",
       "      <td>Nelly</td>\n",
       "      <td>2001-10-20</td>\n",
       "      <td>uh uh uh got bring attention dirty better watc...</td>\n",
       "      <td>2000</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>#9 Dream</td>\n",
       "      <td>John Lennon</td>\n",
       "      <td>1974-12-21</td>\n",
       "      <td>long ago dream dream know yes know seemed real...</td>\n",
       "      <td>1970</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>#Beautiful</td>\n",
       "      <td>Mariah Carey Featuring Miguel</td>\n",
       "      <td>2013-05-25</td>\n",
       "      <td>ah ah beautiful ah ah beautiful hop back bike ...</td>\n",
       "      <td>2010</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>#SELFIE</td>\n",
       "      <td>The Chainsmokers</td>\n",
       "      <td>2014-03-15</td>\n",
       "      <td>jason table kept seeing look girl think make j...</td>\n",
       "      <td>2010</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>#thatPOWER</td>\n",
       "      <td>will.i.am Featuring Justin Bieber</td>\n",
       "      <td>2013-04-06</td>\n",
       "      <td>oh alive alive alive oh fly fly fly oh alive a...</td>\n",
       "      <td>2010</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0       title                             artist       date  \\\n",
       "0           2          #1                              Nelly 2001-10-20   \n",
       "1           4    #9 Dream                        John Lennon 1974-12-21   \n",
       "2           5  #Beautiful      Mariah Carey Featuring Miguel 2013-05-25   \n",
       "3           6     #SELFIE                   The Chainsmokers 2014-03-15   \n",
       "4           7  #thatPOWER  will.i.am Featuring Justin Bieber 2013-04-06   \n",
       "\n",
       "                                              lyrics  decade  topic  \n",
       "0  uh uh uh got bring attention dirty better watc...    2000      3  \n",
       "1  long ago dream dream know yes know seemed real...    1970      2  \n",
       "2  ah ah beautiful ah ah beautiful hop back bike ...    2010      2  \n",
       "3  jason table kept seeing look girl think make j...    2010      3  \n",
       "4  oh alive alive alive oh fly fly fly oh alive a...    2010      1  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['topic'] = df_final.argmax(axis = 1)\n",
    "\n",
    "df_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f5dd5336",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2791015135.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/var/folders/x6/9q2vjsz90nx_0lgx5gr8g33w0000gn/T/ipykernel_59365/2791015135.py\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    topic_label = {0:, 1:, 2:, 3:, 4:, }\u001b[0m\n\u001b[0m                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# creating a dictionary with key as topic numbers and value as topic names\n",
    "topic_label = {0:, 1:, 2:, 3:, 4:, }\n",
    "\n",
    "# mapping the dictionary with the dataframe to get the labels.\n",
    "df_new['topic_name'] = df_new['topic'].map(topic_label)\n",
    "\n",
    "# head of the dataframe\n",
    "df_new.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e78aaa9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7812459",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "00d0d2b0",
   "metadata": {},
   "source": [
    "### Other methods using Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef31dd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2126ece",
   "metadata": {},
   "source": [
    "### Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a788b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38380df2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c0303c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450a7c4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
