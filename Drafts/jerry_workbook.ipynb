{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6000f56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Typical Everydayers...\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "# Unicode, Regex, json for text digestion\n",
    "import unicodedata\n",
    "import re\n",
    "import json\n",
    "\n",
    "import datetime\n",
    "# Time formatting\n",
    "from time import strftime\n",
    "# Make deepcopy\n",
    "import copy\n",
    "\n",
    "# Modeling help...\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# nltk: natural language toolkit -> tokenization, stopwords\n",
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer as stemmer\n",
    "# nltk.download('stopwords')\n",
    "import nltk.sentiment\n",
    "sia = nltk.sentiment.SentimentIntensityAnalyzer()\n",
    "\n",
    "# Import prepare\n",
    "import draft_prepare as p\n",
    "\n",
    "# Quieeet!!! Y'all can't stop me now...\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac5ce7b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23762, 5)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('songs_0526.csv')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e551bcdb",
   "metadata": {},
   "source": [
    "### Preparing the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c23494d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = p.model_clean(df)\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47098db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = p.clean_df(df, extra_words = [], exclude_words = [])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c738f167",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = p.get_topics(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c1d84b16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>artist</th>\n",
       "      <th>date</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>decade</th>\n",
       "      <th>character_count</th>\n",
       "      <th>word_count</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>topic_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>#1</td>\n",
       "      <td>Nelly</td>\n",
       "      <td>2001-10-20</td>\n",
       "      <td>uh uh uh got bring attention dirty better watc...</td>\n",
       "      <td>2000</td>\n",
       "      <td>2014</td>\n",
       "      <td>363</td>\n",
       "      <td>0.9915</td>\n",
       "      <td>Transcendental</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>#9 Dream</td>\n",
       "      <td>John Lennon</td>\n",
       "      <td>1974-12-21</td>\n",
       "      <td>long ago dream dream know yes know seemed real...</td>\n",
       "      <td>1970</td>\n",
       "      <td>842</td>\n",
       "      <td>135</td>\n",
       "      <td>0.9169</td>\n",
       "      <td>Dance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>#Beautiful</td>\n",
       "      <td>Mariah Carey Featuring Miguel</td>\n",
       "      <td>2013-05-25</td>\n",
       "      <td>ah ah beautiful ah ah beautiful hop back bike ...</td>\n",
       "      <td>2010</td>\n",
       "      <td>768</td>\n",
       "      <td>129</td>\n",
       "      <td>0.9989</td>\n",
       "      <td>Nature</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>#SELFIE</td>\n",
       "      <td>The Chainsmokers</td>\n",
       "      <td>2014-03-15</td>\n",
       "      <td>jason table kept seeing look girl think make j...</td>\n",
       "      <td>2010</td>\n",
       "      <td>954</td>\n",
       "      <td>172</td>\n",
       "      <td>0.9218</td>\n",
       "      <td>Miracles</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>#thatPOWER</td>\n",
       "      <td>will.i.am Featuring Justin Bieber</td>\n",
       "      <td>2013-04-06</td>\n",
       "      <td>oh alive alive alive oh fly fly fly oh alive a...</td>\n",
       "      <td>2010</td>\n",
       "      <td>1238</td>\n",
       "      <td>208</td>\n",
       "      <td>0.9984</td>\n",
       "      <td>Transcendental</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0       title                             artist       date  \\\n",
       "0           2          #1                              Nelly 2001-10-20   \n",
       "1           4    #9 Dream                        John Lennon 1974-12-21   \n",
       "2           5  #Beautiful      Mariah Carey Featuring Miguel 2013-05-25   \n",
       "3           6     #SELFIE                   The Chainsmokers 2014-03-15   \n",
       "4           7  #thatPOWER  will.i.am Featuring Justin Bieber 2013-04-06   \n",
       "\n",
       "                                              lyrics  decade  character_count  \\\n",
       "0  uh uh uh got bring attention dirty better watc...    2000             2014   \n",
       "1  long ago dream dream know yes know seemed real...    1970              842   \n",
       "2  ah ah beautiful ah ah beautiful hop back bike ...    2010              768   \n",
       "3  jason table kept seeing look girl think make j...    2010              954   \n",
       "4  oh alive alive alive oh fly fly fly oh alive a...    2010             1238   \n",
       "\n",
       "   word_count  sentiment      topic_name  \n",
       "0         363     0.9915  Transcendental  \n",
       "1         135     0.9169           Dance  \n",
       "2         129     0.9989          Nature  \n",
       "3         172     0.9218        Miracles  \n",
       "4         208     0.9984  Transcendental  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eecdfb2e",
   "metadata": {},
   "source": [
    "## Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39fba184",
   "metadata": {},
   "source": [
    "#### Latent Dirichlet Allocation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "099cb479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create an instance\n",
    "# cv = CountVectorizer(max_df = .95, min_df = 2, stop_words = 'english')\n",
    "\n",
    "# # Fit and transform the lemmatized lyrics data\n",
    "# cv_fit = cv.fit_transform(df.lyrics)\n",
    "\n",
    "# print('\\nShape of the sparse matrix\\n')\n",
    "# cv_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "268209dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create the instance for LDA\n",
    "# lda = LatentDirichletAllocation(n_components = 20, random_state = 42)\n",
    "\n",
    "# # Fit the vectorizer with the LDA\n",
    "# lda.fit(cv_fit)\n",
    "\n",
    "# print('Number of topics:', len(lda.components_))\n",
    "# print('Number of columns of the LDA fit', len(lda.components_[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b7f66eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature = cv.get_feature_names()\n",
    "\n",
    "# print('Length of feature names:', len(feature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f4de244",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# for ind, topic in enumerate(lda.components_):\n",
    "#     print('Top 50 words in topic {}'.format(ind))\n",
    "#     print('-'*120)\n",
    "#     top_50 = topic.argsort()[-50:]\n",
    "#     print([feature[i] for i in top_50], '\\n\\n')\n",
    "#     print('-'*120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a21e70c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Final df transforming cv_fit\n",
    "# df_final = lda.transform(cv_fit)\n",
    "\n",
    "# # Make copy to save original df \n",
    "# df_new = copy.deepcopy(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "724dd675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('\\nChecking the probability distribution of one text data belonging to the topic.\\n')\n",
    "\n",
    "# print('Few words from 1st row:', df.lyrics[0][:88], '\\n')\n",
    "\n",
    "# print('Probability distribution:', df_final[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e223fa97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prob = df_final[0][df_final[0].argmax()].round(2)\n",
    "\n",
    "# print('Document belong to the topic', df_final[0].argmax(), 'with the probability of', prob)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8575fe88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['topic'] = df_final.argmax(axis = 1)\n",
    "\n",
    "# df_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eea07e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Creating a dictionary with key as topic numbers and value as topic names\n",
    "# topic_label = {0:'Love', 1:'Kind Goodbye', 2:'Appeasing', 3:'Club', 4:'Country Life', 5:'Resentful Goodbye', \n",
    "#                6:'Lost', 7:'Hard Times', 8:'Nature', 9:'Miracles', 10:'Money', 11:'Dance', 12:'Fun', \n",
    "#                13:'Dance', 14:'Weekend', 15:'Transcendental', 16:'Sex', 17:'Summer', 18:'Spanish', 19:'Affection'}\n",
    "\n",
    "# # Mapping the dictionary with the dataframe to get the labels.\n",
    "# df['topic_name'] = df['topic'].map(topic_label)\n",
    "\n",
    "# # Head of the dataframe\n",
    "# df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4338aa40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_topics(df):\n",
    "#     # Create an instance\n",
    "#     cv = CountVectorizer(max_df = .95, min_df = 2, stop_words = 'english')\n",
    "    \n",
    "#     # Fit and transform the lemmatized lyrics data\n",
    "#     cv_fit = cv.fit_transform(df.lyrics)\n",
    "\n",
    "#     # Create the instance for LDA\n",
    "#     lda = LatentDirichletAllocation(n_components = 20, random_state = 42)\n",
    "    \n",
    "#     # Fit the vectorizer with the LDA\n",
    "#     lda.fit(cv_fit)\n",
    "    \n",
    "#     # Pull feature names out and define as feature\n",
    "#     feature = cv.get_feature_names()\n",
    "    \n",
    "#     # Final df transforming cv_fit\n",
    "#     df_final = lda.transform(cv_fit)\n",
    "    \n",
    "# #     # Make copy to save original df \n",
    "# #     df_new = copy.deepcopy(df)\n",
    "    \n",
    "#     prob = df_final[0][df_final[0].argmax()].round(2)\n",
    "    \n",
    "#     # Assign the opics tp the dataframe\n",
    "#     df['topic'] = df_final.argmax(axis = 1)\n",
    "    \n",
    "#     # Creating a dictionary with key as topic numbers and value as topic names\n",
    "#     topic_label = {0:'Love', 1:'Kind Goodbye', 2:'Appeasing', 3:'Club', 4:'Country Life', 5:'Resentful Goodbye', \n",
    "#                    6:'Lost', 7:'Hard Times', 8:'Nature', 9:'Miracles', 10:'Money', 11:'Dance', 12:'Fun', \n",
    "#                    13:'Dance', 14:'Weekend', 15:'Transcendental', 16:'Sex', 17:'Summer', 18:'Spanish', 19:'Affection'}\n",
    "    \n",
    "#     # Mapping the dictionary with the dataframe to get the labels.\n",
    "#     df['topic_name'] = df['topic'].map(topic_label)\n",
    "# #     # Drop the unnecessary duplicate column\n",
    "# #     df = pd.concat([df, df_new['topic_name']], axis = 1)\n",
    "#     # Drop unnecessary column 'topic'\n",
    "#     df = df.drop(columns = ['topic'])\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5e1998db",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# get_topics(df)\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eba4f69",
   "metadata": {},
   "source": [
    "### Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dbcd6510",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['sentiment'] = df.lyrics.apply(lambda msg: sia.polarity_scores(msg)['compound'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14052577",
   "metadata": {},
   "source": [
    "## Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "64cb7be1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>artist</th>\n",
       "      <th>date</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>decade</th>\n",
       "      <th>character_count</th>\n",
       "      <th>word_count</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>topic_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1453</th>\n",
       "      <td>1724</td>\n",
       "      <td>BBO (Bad Bitches Only)</td>\n",
       "      <td>Migos Featuring 21 Savage</td>\n",
       "      <td>2018-02-10</td>\n",
       "      <td>buddah bless beat ap iced tennis chain iced wh...</td>\n",
       "      <td>2010</td>\n",
       "      <td>2122</td>\n",
       "      <td>378</td>\n",
       "      <td>-0.9999</td>\n",
       "      <td>Transcendental</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15627</th>\n",
       "      <td>18427</td>\n",
       "      <td>Rack City</td>\n",
       "      <td>Tyga</td>\n",
       "      <td>2011-12-10</td>\n",
       "      <td>rack rack city bitch rack rack rack city bitch...</td>\n",
       "      <td>2010</td>\n",
       "      <td>2108</td>\n",
       "      <td>372</td>\n",
       "      <td>-0.9998</td>\n",
       "      <td>Dance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17474</th>\n",
       "      <td>20592</td>\n",
       "      <td>Snake Skin</td>\n",
       "      <td>Trippie Redd</td>\n",
       "      <td>2019-08-24</td>\n",
       "      <td>ayy bitch bad snakeskin ayy foreign dash know ...</td>\n",
       "      <td>2010</td>\n",
       "      <td>1187</td>\n",
       "      <td>204</td>\n",
       "      <td>-0.9998</td>\n",
       "      <td>Miracles</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0                   title                     artist  \\\n",
       "1453         1724  BBO (Bad Bitches Only)  Migos Featuring 21 Savage   \n",
       "15627       18427               Rack City                       Tyga   \n",
       "17474       20592              Snake Skin               Trippie Redd   \n",
       "\n",
       "            date                                             lyrics  decade  \\\n",
       "1453  2018-02-10  buddah bless beat ap iced tennis chain iced wh...    2010   \n",
       "15627 2011-12-10  rack rack city bitch rack rack rack city bitch...    2010   \n",
       "17474 2019-08-24  ayy bitch bad snakeskin ayy foreign dash know ...    2010   \n",
       "\n",
       "       character_count  word_count  sentiment      topic_name  \n",
       "1453              2122         378    -0.9999  Transcendental  \n",
       "15627             2108         372    -0.9998           Dance  \n",
       "17474             1187         204    -0.9998        Miracles  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What song has the lowest sentiment?\n",
    "df.sort_values(by = ['sentiment'], ascending = True).head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e07e37fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>artist</th>\n",
       "      <th>date</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>decade</th>\n",
       "      <th>character_count</th>\n",
       "      <th>word_count</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>topic_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15734</th>\n",
       "      <td>18546</td>\n",
       "      <td>Ready</td>\n",
       "      <td>Cat Stevens</td>\n",
       "      <td>1974-12-07</td>\n",
       "      <td>love love ready love yes love love ready love ...</td>\n",
       "      <td>1970</td>\n",
       "      <td>696</td>\n",
       "      <td>130</td>\n",
       "      <td>0.9999</td>\n",
       "      <td>Weekend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15173</th>\n",
       "      <td>17882</td>\n",
       "      <td>Pills N Potions</td>\n",
       "      <td>Nicki Minaj</td>\n",
       "      <td>2014-06-07</td>\n",
       "      <td>pill potion overdosin angry still love pill po...</td>\n",
       "      <td>2010</td>\n",
       "      <td>1749</td>\n",
       "      <td>300</td>\n",
       "      <td>0.9999</td>\n",
       "      <td>Weekend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14050</th>\n",
       "      <td>16567</td>\n",
       "      <td>No Love</td>\n",
       "      <td>August Alsina</td>\n",
       "      <td>2014-10-04</td>\n",
       "      <td>said want stay together think ooh red light kn...</td>\n",
       "      <td>2010</td>\n",
       "      <td>1620</td>\n",
       "      <td>298</td>\n",
       "      <td>0.9999</td>\n",
       "      <td>Weekend</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0            title         artist       date  \\\n",
       "15734       18546            Ready    Cat Stevens 1974-12-07   \n",
       "15173       17882  Pills N Potions    Nicki Minaj 2014-06-07   \n",
       "14050       16567          No Love  August Alsina 2014-10-04   \n",
       "\n",
       "                                                  lyrics  decade  \\\n",
       "15734  love love ready love yes love love ready love ...    1970   \n",
       "15173  pill potion overdosin angry still love pill po...    2010   \n",
       "14050  said want stay together think ooh red light kn...    2010   \n",
       "\n",
       "       character_count  word_count  sentiment topic_name  \n",
       "15734              696         130     0.9999    Weekend  \n",
       "15173             1749         300     0.9999    Weekend  \n",
       "14050             1620         298     0.9999    Weekend  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What song has the highest sentiment?\n",
    "df.sort_values(by = ['sentiment'], ascending = False).head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0bfcf136",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_cloud(topic):\n",
    "    # plt.figure(figsize = (8,6))\n",
    "    topic_words = [feature[i] for i in lda.components_[topic].argsort()[-50:]]\n",
    "    cloud = WordCloud(stopwords = STOPWORDS, background_color = 'white',\n",
    "                      width=2500, height=1800).generate(\" \".join(topic_words))\n",
    "\n",
    "    print('\\nWordcloud for topic:', topic_name, '\\n')\n",
    "    plt.imshow(cloud)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ba7050",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53900f7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
