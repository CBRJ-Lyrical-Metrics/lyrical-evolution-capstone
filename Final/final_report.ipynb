{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a54c8f7-aa1f-4305-bbf3-62fb05b75fbc",
   "metadata": {},
   "source": [
    "## A LYRICAL EVOLUTION: \n",
    "\n",
    "#### An Investigation of the Cultural Lexicon of U.S. Popular Music from 1958 - Present\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c102d20-adbd-44b0-a0fc-0062cacb04eb",
   "metadata": {},
   "source": [
    "By: Jerry Nolf, Rachel Robbins-Mayhill, Ben Smith,  & Chris Teceno    |    Codeup   |   Innis Cohort   |   June 2022  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0330cf87-3d96-49e2-93f9-f638738c3096",
   "metadata": {},
   "source": [
    "<img src=\"dataset-cover.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0907b50f-dc30-4e34-8486-b873be7861be",
   "metadata": {},
   "source": [
    "*** **WARNING**: *** This project contains explicit content in the form of isolated words identified through Topic Modeling as features grouped within a topic. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5316282-edbd-4202-9736-911459ae923f",
   "metadata": {},
   "source": [
    "The findings of this project are available in presentation format by clicking on the [Final Slide Presentation](https://www.canva.com/design/DAFCXoeG7z0/jNCtQkQFqyOTWS5Ckg8Xuw/view?utm_content=DAFCXoeG7z0&utm_campaign=designshare&utm_medium=link2&utm_source=sharebutton)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819a8499-ba91-4a83-89c6-67dd43412f15",
   "metadata": {},
   "source": [
    "## Project Goal\n",
    "This project aimed to investigate the patterns of song lyrics across decades by applying Natural Language Processing techniques including Topic Modeling and Sentiment Analysis, while using a Kaggle data set of the Billboard Top 100 Songs from 1958 - Present and lyrics pulled from the Genius.com API. We believe the lyrics of popular songs could be used for historical analysis using exploratory methods and hypothesis testing to identify changing societal trends in relationships, sexuality, and vulgarity. Furthermore, we beleive we can predict the decade the song appeared on the Top 100 using features and machine learning methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae16ef11-eaa7-47c1-a858-84593c0dfa32",
   "metadata": {},
   "source": [
    "## Project Description\n",
    "\n",
    "Songs are powerful tokens: they can soothe, validate, ignite, confront, and educate us – among other things. Like time capsules, they are captured for eternity. The slang and language used are often indicative of the times, and you can probably recall exactly when a song was made based on what is mentioned. Arguably, music is a catalyst for societal and cultural evolution like no other art form. It has been causing controversy and societal upheaval for decades, and it seems with every generation there’s a new musical trend that has the older generations shaking their heads. \n",
    "\n",
    "For centuries, songs have been passed down through generations, being sung as oral histories. However, with advancements of the 20th century, technology has made the world of music a much smaller place and, thanks to cheap, widely-available audio equipment, songs are now distributed on a much larger scale, having a farther-reaching impact, and a more permanent place in history. \n",
    "\n",
    "This project aimed to combine the record of lyrical history and technological advancements to evaluate the changes in the societal lexicon over the last 60+ years. Using machine learning and natural language processing methodologies we investigated the topics prevalent in songs of the past, predicted the decade in which they were written, and conducted historical analysis through exploration to identify changing societal trends in relationships, sexuality, and vulgarity.\n",
    "\n",
    "<img src='Billboard.png' width=\"350\" height=\"350\" align=\"left\"/> To do this, we acquired a [Kaggle](https://www.kaggle.com/datasets/dhruvildave/billboard-the-hot-100-songs) data set of the Billboard Top 100 Songs from its inception in 1958 to present. We then utilized the [Genius.com](https://genius.com/) API and LyricGenius Library to conduct web scraping to pull the lyrics for the specified songs which became the corpus for this project. After acquiring and preparing the corpus, our team conducted time series analysis and natural language processing exploration utilizing methods such as sentiment analysis and topic modeling. We also employed multiclass classification methods to create multiple machine learning models. The end goal was to create an NLP model that accurately predicted the decade a song first appeared on the Billboard Top 100 chart, based on the words found in the lyrics of the song.\n",
    "\n",
    "We choose the Billboard Hot 100 song list as a focus because it is the music industry standard record chart in the United States for song popularity, published weekly by Billboard magazine. It provides a window into popular culture at a given time, by providing chart rankings of songs that were trending on sales, airplay, and now streaming for that week in the United States. It is arguably the best historical record of the impact of specific popular songs over time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd55c6df-51d2-412e-adce-1daa9ca98119",
   "metadata": {},
   "source": [
    "## Initial Thoughts & Hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe434ce1-b1f0-4415-8403-23081cd5e590",
   "metadata": {},
   "source": [
    "The initial hypothesis of this project was that we could use the top songs of each decade in conjunction with topic modeling and sentiment analysis to identify lyric features that would accurately predict the decade a song was on the Billboard Top 100 using machine learning. The thought behind this was that popular songs have been the historians of a unique lexicon, specific to their place in time. We believe the lyrics of popular songs could be analyzed through machine learning to identify societal trends in relationships, sexuality, and vulgarity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c142d940-b385-4db4-b002-1d76ab647b73",
   "metadata": {},
   "source": [
    "## Initial Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ddad9a-cfe1-4f58-8b8b-2b1cc20cd755",
   "metadata": {},
   "source": [
    "The focus of this project is on identifying the decade a song first appeared on the Billboard Top 100. Below are some of the initial questions this project looked to answer throughout the Data Science Pipeline.\n",
    " \n",
    "##### Data-Focused Questions\n",
    "- How does sentiment within lyrics change over time?\n",
    "- Is there a correclation between sentiment and the time a song was popular?\n",
    "- Is there a correlation between events in history and sentiment of lyrics?\n",
    "- What topics are most prevalent across the decades?\n",
    "- How do topics within lyrics change over time?\n",
    "- Is there a correlation between topics and the time a song was popular?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68b0078-9645-445d-9dec-996ac6aa654e",
   "metadata": {},
   "source": [
    "## Key Findings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0373f8-c854-40d9-a4ba-805f656e987e",
   "metadata": {},
   "source": [
    "Through exploratory analysis, we discovered US popular music has undergone a major cultural shift starting in the 1990's, where: \n",
    "\n",
    "- overall sentiment decreased \n",
    "- lyrics became more complex \n",
    "- topics shifted towards sex, money, & violence \n",
    "- ‘love’ was replaced with ‘like’\n",
    "\n",
    "Ultimately, our hypothesis that we could use the top songs of each decade to accurately predict the decade a song was on the Billboard Top 100 was true. Although, certain decades were predicted more accurately than others. Our best performing models were based heavily on TF/IDF with the top performing model being a Logistic Regression model with an F-1 score that was 220% over baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff086dd-8e1c-41b6-89a0-7f4795d490e3",
   "metadata": {},
   "source": [
    "=========================================================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f447ae0-d3ba-4e65-9e65-923a31f642cb",
   "metadata": {},
   "source": [
    "## I. ACQUIRE\n",
    "To acquire the data for this project, we utilized a [Kaggle](https://www.kaggle.com/datasets/dhruvildave/billboard-the-hot-100-songs) data set of the entire listing of Billboard Top 100 Songs from its inception in 1958 to present. \n",
    "\n",
    "The dataset provided:\n",
    "- date song was on the Billboard Top 100 \n",
    "- rank of song  \n",
    "- title\n",
    "- artist name\n",
    "- rank of song the previous week\n",
    "- rank of song at it's peak week\n",
    "- number of weeks song was on the Top 100  \n",
    "\n",
    "The original Kaggle dataset contained more than 300,000 entires. We selected only unique artists and songs, to ensure there were no duplicates, keeping only the earliest appearance on the chart to standardize the selections in the event of multiple appearances. Following song selection with the Kaggle dataset, we then obtained an API token to utilize the [Genius.com](https://genius.com/) API and [LyricGenius Library](https://pypi.org/project/lyricsgenius/) to conduct web scraping and pull the lyrics for the specified songs which became the corpus for this project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac72be3-1478-4f30-8653-a08976cab5a6",
   "metadata": {},
   "source": [
    "### Note about imports: \n",
    "Imports for this project are added in the sections in which they are required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a7a8a76-8a52-468b-84d5-13ad0a952e25",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid non-printable character U+200B (final_explore.py, line 176)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[1;32m\"/Users/jerrynolf/opt/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m3444\u001b[0m, in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "\u001b[0;36m  File \u001b[0;32m\"/var/folders/x6/9q2vjsz90nx_0lgx5gr8g33w0000gn/T/ipykernel_65247/1727540317.py\"\u001b[0;36m, line \u001b[0;32m7\u001b[0;36m, in \u001b[0;35m<module>\u001b[0;36m\u001b[0m\n\u001b[0;31m    import final_explore as explore\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m\"/Users/jerrynolf/codeup-data-science/lyrical-evolution-capstone/Final/final_explore.py\"\u001b[0;36m, line \u001b[0;32m176\u001b[0m\n\u001b[0;31m    ​\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid non-printable character U+200B\n"
     ]
    }
   ],
   "source": [
    "# import for acquisition\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "import final_acquire as acquire\n",
    "import final_prepare as prepare\n",
    "import final_explore as explore\n",
    "import final_model as model\n",
    "\n",
    "# import for data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Dict, List, Optional, Union, cast\n",
    "\n",
    "# import to ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292b3645-39b5-422e-8cf4-e884a2004574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# acquire data from .json saved and processed using functions found in wrangle.py\n",
    "df = pd.read_csv(\"songs_0526.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3880d05-efc6-46ab-972a-8c76a99d3a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain number of columns and rows for original dataframe\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd63d50-ff49-481a-8a6e-ebefdec84ba3",
   "metadata": {},
   "source": [
    "#### Original Filtered DataFrame Size: 23,762 rows, or documents, and 5 columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836ce5ea-d032-4f7d-8c2d-dc6b6375d8b8",
   "metadata": {},
   "source": [
    "=========================================================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0896e47-ee98-4192-9cb0-f79b187d5658",
   "metadata": {},
   "source": [
    "## II. PREPARE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9df564-01cc-463b-96a6-8536629a9e95",
   "metadata": {},
   "source": [
    "After data acquisition, the dataframe was analyzed and cleaned to facilitate functional exploration and clarify variable confusion. The preparation of this data can be replicated using the 'get_data' function saved within the prepare.py file inside the [Lyrical Evolution](https://github.com/CBRJ-Lyrical-Metrics/song-lyrics-capstone) repository on GitHub. The function takes in the original acquire dataframe and returns it with the changes noted below.\n",
    "\n",
    "**Steps Taken to Clean & Prepare Data:**\n",
    "\n",
    "- Cleaning: \n",
    "    - Make all text lowercase\n",
    "    - Normalize, encode, and decode to remove accented text and special characters\n",
    "    - Expand abbreviated contractions\n",
    "    - Lemmatize words to acquire base words\n",
    "    - Remove stopwords\n",
    "    - Convert date to DateTime format\n",
    "    - Remove song part identifiers ('lyrics' 'verse', 'chorus', 'hook', 'embed')\n",
    "    \n",
    "---   \n",
    "- Address missing values, data errors, unnecessary data, and unclear values:\n",
    "    - No null values\n",
    "    - Data Errors : The API returned lyrics that were not the expected song's lyrics \n",
    "        - Mannually checking some\n",
    "        Compared title, if they match after cleaning manipulation, \n",
    "---    \n",
    "- Create feature engineered columns:\n",
    "    - Decade \n",
    "    - Chorus Count\n",
    "    - Verse Count\n",
    "    - Verse/Chorus Ratio\n",
    "    - Word Count\n",
    "    - Unique Words per Song\n",
    "    - Unique Words per Decade\n",
    "    - Bigrams\n",
    "    - Trigrams\n",
    "    \n",
    "- Apply Natural Language Processing (NLP Methods:\n",
    "    - Sentiment Analysis\n",
    "    - Topic Modeling\n",
    "    \n",
    "---\n",
    "- Split corpus into train, validate, and test samples \n",
    "\n",
    "\n",
    "**Note on Splitting Data:**\n",
    "\n",
    "\n",
    "\n",
    "**Note on Missing Value Handling:**\n",
    "The missing value removal equated to removing ______ observations/documents, which was about ___   \\% of the data set. It still left _______ observations, a substantial number. If given more time with the data, it is recommended to investigate other ways to impute the missing data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7e776a-600d-45a1-9da9-7949bafdfec4",
   "metadata": {},
   "source": [
    "### Sentiment Analysis\n",
    "\n",
    "Natural Langauge Toolkit (NLTK) was used to prepare the corpus for sentiment analysis. NLTK assigns a score between -1 and +1 to each song based on whether the the sum of words and phrases in the song are considered to be positive or negative.\n",
    "After scores were assigned to each song based upon the lyrical content, sentiment score ranges were divided into 5 categories: very negative, somewhat negative, nuetral, somewhat positive, and very positive. Each song was then labled with the sentiment category by it's correspoinding sentiment score in preparation for exploration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7497566e-09c6-4abf-b224-a6ace8fa173b",
   "metadata": {},
   "source": [
    "### Topic Modeling\n",
    "\n",
    "Latent Dirichlet Allocation (or LDA) spearheaded the extraction of topics within the lyrics. This unsupervised machine learning method detected word and phrase patterns. It then clustered groups of words that could best be labeled as a topic. 20 major topics were originally produced, but 3 were overlapping in tone and were therefore manually combined with others, resulting in 17 final topics to explore. These topics will be outlined in more detail through the exploration section of this report."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84533370-3ee7-4b3b-b09b-eaae03b86f22",
   "metadata": {},
   "source": [
    "<img src='final_topics.png' width=\"900\"  align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937b3c41-a336-4a8c-9777-c13ff07c28ef",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee80dea-08b3-4064-adcf-513ab910b2d5",
   "metadata": {},
   "source": [
    "## Results of Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32c4894-1f7c-4684-8b22-81b119ad2ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import for prepare\n",
    "import final_prepare as prepare\n",
    "import unicodedata\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from time import strftime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c382de0-355d-40e6-9afe-137f3c86888b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the data preparation observations and tasks to clean the data using the prep_data function found in the prepare.py\n",
    "df = prepare.get_data()\n",
    "# view first few rows of dataframe\n",
    "# obtain the number of rows and columns for the updated/cleaned dataframe \n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22d1c7f-17a7-4564-ae60-445b2ccbce78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain number of columns and rows 'cleaned' dataframe\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35f06a2-40f7-49ea-b3b2-502d3a90b253",
   "metadata": {},
   "source": [
    "## Prepared DataFrame Size: 134 rows, or documents, and 13 columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad099ea-da58-4d5c-81d7-b391b4449579",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83fdb06a-f042-4b96-8fda-bf90f16f0227",
   "metadata": {},
   "source": [
    "### PREPARE - SPLIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa98112-1a01-4d46-bfe5-70db5d26156e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import for split\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c20745-cd3b-4686-a502-99981315e1d0",
   "metadata": {},
   "source": [
    "After preparing the corpus, it was split into 3 samples; train, validate, and test using:\n",
    "\n",
    "- Random State: ______\n",
    "- Test = 20% of the original dataset\n",
    "- The remaining 80% of the dataset is divided between valiidate and train\n",
    "    - Validate (.30*.80) = 24% of the original dataset\n",
    "    - Train (.70*.80) = 56% of the original dataset\n",
    "    \n",
    "The split of this data can be replicated using the split_data function saved within the prepare.py file inside the [_____](_________) repository on GitHub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687cb248-9647-41c5-9d50-30a4afa44e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into train, validate, and test using the split_data function found in the prepare.py\n",
    "train, validate, test = prepare.split_data(df)\n",
    "# obtain the number of rows and columns for the splits\n",
    "train.shape, validate.shape, test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c0e03e-2952-4eb6-9d44-54a5a04b8bf4",
   "metadata": {},
   "source": [
    "=========================================================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919c78fe-d835-4aa1-ab27-80c23b51faf5",
   "metadata": {},
   "source": [
    "## III. EXPLORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd7468a-7db3-4a6c-ba59-ca948831622c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import for data visualization\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.ticker import StrMethodFormatter\n",
    "from matplotlib import style\n",
    "from wordcloud import WordCloud\n",
    "import final_explore as explore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e368cfc-c8db-4375-9436-087118ad07ca",
   "metadata": {},
   "source": [
    "After acquiring and preparing the corpus, exploration was conducted. All univariate exploration was completed on the entire cleaned corpus in the workbook for this project. For the purpose of the final report, only the target variable will be displayed in order to reduce noise and provide focused context for the project. Following univariate exploration, the split sets (train, validate, and test samples) were utilized thorugh modeling, where only the train set was used for bivariate and multivariate exploration to prevent data leakage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b897a71-03f3-440f-a53c-7c5ca636dc48",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1854228a-7ff2-41dd-9e88-b55feba10cb3",
   "metadata": {},
   "source": [
    "### UNIVARIATE EXPLORATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1980cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep??????"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617865e3-742d-4be4-8f35-25cc1a5d5d8e",
   "metadata": {},
   "source": [
    "#### UNIVARIATE EXPLORATION of TARGET VARIABLE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3e7208-151f-4d23-89dd-c06b8b02a4b6",
   "metadata": {},
   "source": [
    "#### OBSERVATIONS: \n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4497c72-55d0-49eb-be38-154f2edf0a13",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c18e44d-ab8b-4cd3-af33-6388dd07862f",
   "metadata": {},
   "source": [
    "### EXPLORATION QUESTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa631c3e-d91c-4717-b4cc-01f09def4632",
   "metadata": {},
   "source": [
    "All bivariate exploration was conducted on the train corpus to prevent data leakage. The initial questions and univariate exploration guided the bivariate exploration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9502d911-af97-483e-a1d5-349aea49a78b",
   "metadata": {},
   "source": [
    "#### EXPLORE QUESTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888ede95-8e60-4d69-a6b0-a2b317559668",
   "metadata": {},
   "source": [
    "#### QUESTION 1: \n",
    "How has song sentiment changed over time?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd75bdbe-3c78-47eb-a03e-06a4c039a942",
   "metadata": {},
   "source": [
    "Examined the change in average sentiment score over tiem by looking at a rolling 5 year avereage and average by decade. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9faa39be-a1af-4ba8-89fb-21175c3d9c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "explore.sentiment_lineplot(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbdad8f7",
   "metadata": {},
   "source": [
    "#### ANSWER 1:\n",
    "Sentiment was fairly steady in the 60's and 70's, followed by a gradual downward trend which becomes sharper in the 2000's and 2010's. The downward trend is due to an increase in very negative sentiment and decrease in very positivesentiment while mid-range sentiment stays contstant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae6d3d5",
   "metadata": {},
   "source": [
    "#### Question 2:\n",
    "What topics are most prevalent across the decades?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31579a1-4752-44bf-9783-491f74de39a8",
   "metadata": {},
   "source": [
    " but through manual analysis, it was determined a few groupings were very similar. Combining these groupings into the same topic resulted in **17 finalized topics for our dataset**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f22a5b-4bf8-48fe-b613-1031a3ffcb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "explore.topic_popularity(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113ea4db",
   "metadata": {},
   "source": [
    "#### Answer 2:\n",
    "Breakups are by far the most popular topics in songs across all decades, followed by being or feeling lost, then affection, sex, and nature. The least popular over the years is Spanish-influence songs, holiday songs, and songs about jealousy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29ffc02",
   "metadata": {},
   "source": [
    "#### QUESTION 3: \n",
    "- How do relationship topics change over the decades?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d1d42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "explore.relationship_line(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af057b48",
   "metadata": {},
   "source": [
    "#### Observation:\n",
    "Most topics in our \"relationships\" group seem to be consistently present over time. However, sex and affection seem to stand out considering sex's rise and affection's decline in the 90's. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace94ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "explore.touch_swarm(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17adc54a",
   "metadata": {},
   "source": [
    "#### Answer 3:\n",
    "While most relationship topics appear constant, pulling out and viewing the presence of songs about sex versus affection over time seems to allow us to come to the conclusion that the categories have an inverse relationship when comparing them in this environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98012e5",
   "metadata": {},
   "source": [
    "#### QUESTION 4: \n",
    "- How do vice topics change over the decades?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48eeadea",
   "metadata": {},
   "outputs": [],
   "source": [
    "explore.vice_swarm(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69707286",
   "metadata": {},
   "source": [
    "#### ANSWER 4: \n",
    "After 1990 more explicit sexual themes became extremely popular in lyrics. Songs about violence and money then slowly followed until making a more impactful presence around 2015."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50827cd0",
   "metadata": {},
   "source": [
    "#### Question 5:\n",
    "How has the prevalence of the word 'like' changed over the decades?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143d676b",
   "metadata": {},
   "outputs": [],
   "source": [
    "explore.love_vs_like_lineplot(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b3a679",
   "metadata": {},
   "source": [
    "#### ANSWER 5: \n",
    "The word love's presence severely diminishedafter 1990. Around 2008, the word like became more present in lyrics and it continues to increase while love's decline contines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20e2e5f",
   "metadata": {},
   "source": [
    "#### Question 6: \n",
    "How does sentiment align with historical events?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf3884c",
   "metadata": {},
   "outputs": [],
   "source": [
    "explore.historical_lineplot(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c532f1b0",
   "metadata": {},
   "source": [
    "#### Answer 6:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6554fdcf",
   "metadata": {},
   "source": [
    "#### Question 7:\n",
    "How did unique word count change over time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782fcc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "explore.unique_words_lineplot(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a95962",
   "metadata": {},
   "source": [
    "#### Answer 7:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c456be1-eb72-47a7-b705-67edb07d26e4",
   "metadata": {},
   "source": [
    "=========================================================================================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15980c0-808c-448b-9a74-6fb25fff882e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "53b8ebc6-10c5-4182-9d63-0e4d761931d7",
   "metadata": {},
   "source": [
    "## IV. MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02fd964",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4d6649-e624-4553-a888-6d541bc4abed",
   "metadata": {},
   "source": [
    "### Focus of Model Metrics\n",
    "The target variable, Decade, is a categorical variable, therefore classification machine learning algorithms were used to fit to the training corpus and the models were evaluated on the validate corpus. The metrics used for model evaluation was accuracy, due to the multi-class classification approach. In other words, the model was optimized for identifying true positives, false positive, true negatives, and false negatives, therefore we focused on creating a model with the highest accuracy score from train to validate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121521ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the data\n",
    "df = prepare.get_data()\n",
    "# remove incomplete decades (1950, 2020)\n",
    "df = df[(df.decade != 1950) & (df.decade != 2020)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca5a63a-4615-42df-adb8-657de6c546fe",
   "metadata": {},
   "source": [
    "### Set X & y\n",
    "As mentioned above, two different approaches were taken to prepare the data for modeling. Feature engineering was done for exploratory analysis and even more for modeling. This however did not result in a significant improvement in the accuracy of the model. Therefore, the data was prepared for modeling by using TF-IDF vectorization which takes into account the word count in each song's lyrics vs word count in the entire corpus. Below is how this was performed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017b59b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make vectorizer\n",
    "tfidf = TfidfVectorizer()\n",
    "X = tfidf.fit_transform(df[\"lyrics\"])\n",
    "y = df[\"decade\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2447d7-1416-40b0-8640-d055c4149517",
   "metadata": {},
   "source": [
    "### Set Baseline\n",
    "A baseline prediction was set by using the mode for decade. This gave us a baseline accuracy of 20.6%. We will evaluate the accuracy of our models in comparrison to that baseline.\n",
    "### Consider Feature Engineering\n",
    "First lets look at the models with the lower accuracy, this is the df using feature engineering not including TF-IDF. \n",
    "The following were adjustable:\n",
    "- scale or not scale\n",
    "- use only unique bigrams as features or use all numeric features\n",
    "### Observation of models with feature engineering:\n",
    "### Consider TF-IDF\n",
    "#### The Type of Classification models built were \n",
    "- Decision Tree\n",
    "- Random Forest\n",
    "- Logistic Regression\n",
    "\n",
    "The models were run with many trials, adjusting parameters and algorithms to find the best performing model.  \n",
    "\n",
    "- All Logistic Regression models appeared to be overfit based upon their high performance on train accuracy compared to the significant drop off on validate accuracy.\n",
    "    - This is in part due to the use of TF-IDF which analyzes each word in the train corpus and does not remove attributes.\n",
    "- In general all models outperformed baseline, which had  ___ accuracy on train and ___ accuracy on validate.\n",
    "- The Logistic Regression Model that performed best had a c of 1000 and solver of 'lbfgs', with train accuracy of 98% and validate accuracy of 61% performing 19% better than baseline with validate. It was then applied to the un-seen test data with an accuracy of 56%.\n",
    "---\n",
    "### MODEL - DECISION TREE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfe81b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncommit to run the following code\n",
    "# results = model.run_decision_tree_models(df)\n",
    "# results.drop(columns='test_accuracy').head(1) # show baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05f4dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncommit to run the following code\n",
    "# results.drop(columns='test_accuracy').sort_values('validate_accuracy', ascending=False).head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60988f74",
   "metadata": {},
   "source": [
    "The Decision Tree model that performed the best on train & validate set had max_depth of 10, with 41% accuracy on train, and 31% accuracy on validate, so that model will be isolated below in the event it is the best performing model to be applied to the test (unseen) dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd5e113",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270052c1-bab2-47e8-b59a-5b71a6a86f7e",
   "metadata": {},
   "source": [
    "### Model - RANDOM FOREST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f34d609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncommit to run the following code\n",
    "# results2 = model.run_random_forest_models(df)\n",
    "# results2.drop(columns='test_accuracy').sort_values('validate_accuracy', ascending=False).head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10eab43c-070b-4722-aa80-f2e2cf34e497",
   "metadata": {},
   "source": [
    "The Random Forest model that performed the best on train & validate set had max_depth of 100 and min_sample_leaf of 2, with 93%  accuracy on train, and 40% accuracy on validate, so that model will be isolated below in the event it is the best performing model to be applied to the test (unseen) dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6066b3b-404f-4820-ae54-77ecd1370c61",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d8c6d5-67c9-4131-86ac-7c232895b562",
   "metadata": {},
   "source": [
    "### Model - LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678c6e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncommit to run the following code\n",
    "# results3 = model.run_logistic_reg_models(df)\n",
    "# results3.drop(columns='test_accuracy').sort_values('validate_accuracy', ascending=False).head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637f18e5",
   "metadata": {},
   "source": [
    "Evaluating the model with the validate data set was done in the function above for comparrison. The Logistic Regression Model that performed best had a c-statistic of 1000 with a train accuracy of 69% and validate accuracy of 45% performing 222% better than baseline on unseen (validate) data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725496a4",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fedcf9d2",
   "metadata": {},
   "source": [
    "### Best Performing Model Applied to Test Data (Unseen Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4460fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncommit to run the following code\n",
    "# results3.sort_values('validate_accuracy', ascending=False).head(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38284848-c794-43c9-8fa2-7c38be8437e5",
   "metadata": {},
   "source": [
    "=========================================================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f62750-5420-47d4-92e5-67b1759ff80d",
   "metadata": {},
   "source": [
    "## V. CONCLUSION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d09473c",
   "metadata": {},
   "source": [
    "This project aimed to investigate the patterns of song lyrics across decades using Time Series Analysis and Natural Language Processing techniques including Topic Modeling, Sentiment Analysis, and Term Frequency using a Kaggle data set of the Billboard Top 100 Songs from 1958 - 2021 and lyrics pulled from the Genius.com API. We believed the lyrics of popular songs could be used for historical analysis using exploratory methods and hypothesis testing to identify changing societal trends in relationships, technology, sexuality, and vulgarity. Furthermore, we beleive we can predict the decade the song appeared on the Top 100 using features and machine learning methods.\n",
    "\n",
    "Through exploration and modeling, we determined _____________________.\n",
    "\n",
    "This information could be usiful in various contexts:\n",
    "- Anthropologic and Sociologic Academic Analysis\n",
    "- Marketing Analysis for companies associated with the music industry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3df586d",
   "metadata": {},
   "source": [
    "### RECOMMENDATIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0905b7",
   "metadata": {},
   "source": [
    "### NEXT STEPS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d98812",
   "metadata": {},
   "source": [
    "TBD with more exploration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ca57c2-01a1-450b-99b8-4604562e3865",
   "metadata": {},
   "source": [
    "=========================================================================================================================================================="
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3067ead486e059ec00ffe7555bdb889e6e264a24dc711bf108106cc7baee8d5d"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
