{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a54c8f7-aa1f-4305-bbf3-62fb05b75fbc",
   "metadata": {},
   "source": [
    "# A LYRICAL EVOLUTION: \n",
    "\n",
    "### An Investigation of the Cultural Lexicon & Historical Relevance of U.S. Popular Music from 1958 - 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49eb0c4-37d7-45cc-979b-57baaf3d0d2e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c102d20-adbd-44b0-a0fc-0062cacb04eb",
   "metadata": {},
   "source": [
    "**An NLP based Capstone Project & Final Report Created By:**\n",
    "\n",
    "Ben Smith, Chris Teceno, Jerry Nolf & Rachel Robbins-Mayhill\n",
    "Codeup   |   Innis Cohort   |   June 2022  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0330cf87-3d96-49e2-93f9-f638738c3096",
   "metadata": {},
   "source": [
    "<img src=\"dataset-cover.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819a8499-ba91-4a83-89c6-67dd43412f15",
   "metadata": {},
   "source": [
    "## Project Goal\n",
    "This project aims to investigate the patterns of song lyrics across decades using Natural Language Processing techniques including Topic Modeling, Sentiment Analysis, and Term Frequency. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae16ef11-eaa7-47c1-a858-84593c0dfa32",
   "metadata": {},
   "source": [
    "## Project Description\n",
    "\n",
    "Songs are powerful tokens: they can soothe, validate, ignite, confront, and educate us – among other things. Like time capsules, they are now captured for eternity. The slang and language used are often indicative of the times, and you can probably recall exactly when a song was made based on what is mentioned. There’s nothing quite like a song to provide a picture of what was going on culturally at a specified time. Arguably, music is a catalyst for societal and cultural evolution like no other artform. It has been causing controversy and societal upheaval for decades, and it seems with every generation there’s a new musical trend that has the older generations shaking their heads and clutching their jewelry. \n",
    "\n",
    "\n",
    "Traditionally, for centuries, songs have been passed down through generations, being sung like oral histories. However, with advancements of the 20th century, technology has made the world of music a much smaller place and, thanks to cheap, widely-available audio equipment, songs could suddenly be distributed on a much larger scale, having farther reaching impact, and a more permanent place in history. This project aims to take those technological advancements a step further in regards to the historical impact of song lyrics. By using machine learning and natural language processing methodologies,  investigate songs of the past, and categorize them in the decade i\n",
    "\n",
    "\n",
    ", songs can be analyzed by machines to provide a window to their cultural revalence and societal impact. \n",
    "\n",
    "The Billboard Hot 100 is the music industry standard record chart in the United States for songs, published weekly by Billboard magazine. Chart rankings are based on sales, radio play, and online streaming in the United States. It is arguablly the best historical record of the  <img src='Billboard.png' width=\"350\" height=\"350\" align=\"left\"/> impact of specific popular songs over time. \n",
    "\n",
    "Every week, Billboard releases \"The Hot 100\" chart of songs that were trending on sales and airplay for that week. This project used a dataset from [Kaggle](https://www.kaggle.com/datasets/dhruvildave/billboard-the-hot-100-songs) that is a collection of all \"The Hot 100\" charts released since its inception in 1958."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd55c6df-51d2-412e-adce-1daa9ca98119",
   "metadata": {},
   "source": [
    "## Initial Thoughts & Hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe434ce1-b1f0-4415-8403-23081cd5e590",
   "metadata": {},
   "source": [
    "The initial hypothesis of this project was that we could use the top songs of each decade in conjunction with topic modeling to identify unique words or topics which could be used as a feature or a set of features to accurately predict the decade a song was on the Billboard Top 100 using machine learning. The thought behind this was that popular songs have been the historians of a unique lexicon, specific to their place in time. We believe the lyrics of popular songs could be analyzed through machine learning to identify societal trends in relationships, technology, sexuality, and vulgarity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c142d940-b385-4db4-b002-1d76ab647b73",
   "metadata": {},
   "source": [
    "## Initial Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ddad9a-cfe1-4f58-8b8b-2b1cc20cd755",
   "metadata": {},
   "source": [
    "The focus of this project is on identifying the decade a song first appeared on the Billboard Top 100. Below are some of the initial questions this project looked to answer throughout the Data Science Pipeline.\n",
    " \n",
    "##### Data-Focused Questions\n",
    "- What are the most frequently occuring words?\n",
    "- What are the most frequently occuring bigrams (pairs of words) by each decade?\n",
    "- What decade did the song first appear in the top 100?\n",
    "- What topics are most unique to each decade?\n",
    "- Is there a correlation between sentiment and decade?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68b0078-9645-445d-9dec-996ac6aa654e",
   "metadata": {},
   "source": [
    "## Key Findings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0373f8-c854-40d9-a4ba-805f656e987e",
   "metadata": {},
   "source": [
    "The key findings for this presentation are available in slide format by clicking on the [Final Slide Presentation](URL)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff086dd-8e1c-41b6-89a0-7f4795d490e3",
   "metadata": {},
   "source": [
    "=========================================================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f447ae0-d3ba-4e65-9e65-923a31f642cb",
   "metadata": {},
   "source": [
    "## I. ACQUIRE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac72be3-1478-4f30-8653-a08976cab5a6",
   "metadata": {},
   "source": [
    "### Note about imports: \n",
    "Imports for this project are added in the sections in which they are required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a7a8a76-8a52-468b-84d5-13ad0a952e25",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'final_acquire'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/jx/x5_xzwy107g6d0zd021r2ph40000gn/T/ipykernel_50453/2776771300.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mfinal_acquire\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfinal_prepare\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfinal_explore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'final_acquire'"
     ]
    }
   ],
   "source": [
    "# import for acquisition\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "import final_acquire\n",
    "import final_prepare\n",
    "import final_explore\n",
    "import final_model\n",
    "from env import github_token, github_username\n",
    "\n",
    "# import for data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Dict, List, Optional, Union, cast\n",
    "\n",
    "# import to ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292b3645-39b5-422e-8cf4-e884a2004574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# acquire data from .json saved and processed using functions found in wrangle.py\n",
    "df = pd.read_json(\"data.json\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3880d05-efc6-46ab-972a-8c76a99d3a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain number of columns and rows for original dataframe\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd63d50-ff49-481a-8a6e-ebefdec84ba3",
   "metadata": {},
   "source": [
    "### The Original DataFrame Size: ____ rows, or documents, and ____ columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836ce5ea-d032-4f7d-8c2d-dc6b6375d8b8",
   "metadata": {},
   "source": [
    "=========================================================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0896e47-ee98-4192-9cb0-f79b187d5658",
   "metadata": {},
   "source": [
    "## II. PREPARE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9df564-01cc-463b-96a6-8536629a9e95",
   "metadata": {},
   "source": [
    "After data acquisition, the table was analyzed and cleaned to facilitate functional exploration and clarify variable confusion. The preparation of this data can be replicated using the prep_data  function saved within the prepare.py file inside the 'NLP-Project' repository on GitHub. The function takes in the original data.json dataframe and returns it with the changes noted below.\n",
    "\n",
    "**Steps Taken to Clean & Prepare Data:**\n",
    "\n",
    "- Basic Cleaning: \n",
    "    - Make all text lowercase\n",
    "    - Normalize, encode, and decode to remove accented text and special characters\n",
    "    - Tokenize strings to break words and punctuation into discrete units\n",
    "    - Stem and Lemmatize words to acquire base words\n",
    "    - Remove stopwords\n",
    "    - Rename columns\n",
    "---   \n",
    "- Address missing values, data errors, unnecessary data, and unclear values:\n",
    "    - Replace Jupyter Notebook values with Python after manually verifying most Jupyter Notebook entires used the Python programming language \n",
    "    - Drop missing values to prevent impediments in exploration and modeling: 9 documents/observations that had null values in the language column \n",
    "    - Drop all rows where README length was 0\n",
    "    - Total dropped documents = 32\n",
    "---    \n",
    "- Create feature engineered columns:\n",
    "    - unique words\n",
    "    - character count\n",
    "    - word count\n",
    "    - unique word count\n",
    "    - most common word count (2nd, 3rd, 4th, 5th most common)\n",
    "    - unique bigram count\n",
    "    - count of bigrams unique to each language in train set(this is done by creating a new column for each language)\n",
    "    \n",
    "---\n",
    "- Split corpus into train, validate, and test samples\n",
    "\n",
    "**Note on Missing Value Handling:**\n",
    "The missing value removal equated to removing 9 observations/documents, which was about 9\\% of the data set. It still left a substantial number of observations above the minimum expectation of 100. If given more time with the data, it is recommended to investigate other ways to impute the missing data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937b3c41-a336-4a8c-9777-c13ff07c28ef",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee80dea-08b3-4064-adcf-513ab910b2d5",
   "metadata": {},
   "source": [
    "## Results of Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32c4894-1f7c-4684-8b22-81b119ad2ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import for prepare\n",
    "import prepare\n",
    "import unicodedata\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from time import strftime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c382de0-355d-40e6-9afe-137f3c86888b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the data preparation observations and tasks to clean the data using the prep_data function found in the prepare.py\n",
    "df = prepare.prep_data(df)\n",
    "# view first few rows of dataframe\n",
    "# obtain the number of rows and columns for the updated/cleaned dataframe \n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35f06a2-40f7-49ea-b3b2-502d3a90b253",
   "metadata": {},
   "source": [
    "## Prepared DataFrame Size: 134 rows, or documents, and 13 columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad099ea-da58-4d5c-81d7-b391b4449579",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83fdb06a-f042-4b96-8fda-bf90f16f0227",
   "metadata": {},
   "source": [
    "### PREPARE - SPLIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa98112-1a01-4d46-bfe5-70db5d26156e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import for split\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c20745-cd3b-4686-a502-99981315e1d0",
   "metadata": {},
   "source": [
    "After preparing the corpus, it was split into 3 samples; train, validate, and test using:\n",
    "\n",
    "- Random State: ______\n",
    "- Test = 20% of the original dataset\n",
    "- The remaining 80% of the dataset is divided between valiidate and train\n",
    "    - Validate (.30*.80) = 24% of the original dataset\n",
    "    - Train (.70*.80) = 56% of the original dataset\n",
    "    \n",
    "The split of this data can be replicated using the split_data function saved within the prepare.py file inside the [_____](_________) repository on GitHub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687cb248-9647-41c5-9d50-30a4afa44e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into train, validate, and test using the split_data function found in the prepare.py\n",
    "train, validate, test = prepare.split_data(df)\n",
    "# obtain the number of rows and columns for the splits\n",
    "train.shape, validate.shape, test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c0e03e-2952-4eb6-9d44-54a5a04b8bf4",
   "metadata": {},
   "source": [
    "=========================================================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919c78fe-d835-4aa1-ab27-80c23b51faf5",
   "metadata": {},
   "source": [
    "## III. EXPLORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd7468a-7db3-4a6c-ba59-ca948831622c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import for data visualization\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.ticker import StrMethodFormatter\n",
    "from matplotlib import style\n",
    "from wordcloud import WordCloud\n",
    "import explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ece60b-5838-41ee-ac9b-577d3db57fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Universal Visualization Formatting\n",
    "\n",
    "# determine figure size\n",
    "plt.rc('figure', figsize=(20, 8))\n",
    "# determine font size\n",
    "plt.rc('font', size=15)\n",
    "# determine style\n",
    "plt.style.use('seaborn-deep')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e368cfc-c8db-4375-9436-087118ad07ca",
   "metadata": {},
   "source": [
    "After acquiring and preparing the corpus, exploration was conducted. All univariate exploration was completed on the entire cleaned corpus in the workbook for this project. For the purpose of the final report, only the target variable will be displayed in order to reduce noise and provide focused context for the project. Following univariate exploration, the split sets (train, validate, and test samples) were utilized thorugh modeling, where only the train set was used for bivariate and multivariate exploration to prevent data leakage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b897a71-03f3-440f-a53c-7c5ca636dc48",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1854228a-7ff2-41dd-9e88-b55feba10cb3",
   "metadata": {},
   "source": [
    "### UNIVARIATE EXPLORATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617865e3-742d-4be4-8f35-25cc1a5d5d8e",
   "metadata": {},
   "source": [
    "#### UNIVARIATE EXPLORATION of TARGET VARIABLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f5ff97-3cea-44a4-935a-7d2cf7ac42db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create visualization\n",
    "df.language.value_counts().plot(kind='pie', y='Language', autopct=\"%1.1f%%\")\n",
    "# remove y axis label\n",
    "plt.ylabel(None)\n",
    "#add title\n",
    "plt.title('Top 4 Programming Langauges Across Corpus by Percentage')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3e7208-151f-4d23-89dd-c06b8b02a4b6",
   "metadata": {},
   "source": [
    "#### OBSERVATIONS: \n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4497c72-55d0-49eb-be38-154f2edf0a13",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c18e44d-ab8b-4cd3-af33-6388dd07862f",
   "metadata": {},
   "source": [
    "### EXPLORATION QUESTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa631c3e-d91c-4717-b4cc-01f09def4632",
   "metadata": {},
   "source": [
    "All bivariate exploration was conducted on the train corpus to prevent data leakage. The initial questions and univariate exploration guided the bivariate exploration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9502d911-af97-483e-a1d5-349aea49a78b",
   "metadata": {},
   "source": [
    "#### EXPLORE QUESTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888ede95-8e60-4d69-a6b0-a2b317559668",
   "metadata": {},
   "source": [
    "### QUESTION 1: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9faa39be-a1af-4ba8-89fb-21175c3d9c15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb2dc44-bf0e-483a-be7f-48cdf293df70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d93106b-1655-45ca-95f8-912ab18e5910",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f22a5b-4bf8-48fe-b613-1031a3ffcb57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506a35ce-6efe-49a5-9e1b-2fa9f52d04e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3c456be1-eb72-47a7-b705-67edb07d26e4",
   "metadata": {},
   "source": [
    "=========================================================================================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15980c0-808c-448b-9a74-6fb25fff882e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "53b8ebc6-10c5-4182-9d63-0e4d761931d7",
   "metadata": {},
   "source": [
    "## IV. MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4d6649-e624-4553-a888-6d541bc4abed",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "38284848-c794-43c9-8fa2-7c38be8437e5",
   "metadata": {},
   "source": [
    "=========================================================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f62750-5420-47d4-92e5-67b1759ff80d",
   "metadata": {},
   "source": [
    "## V. CONCLUSION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ca57c2-01a1-450b-99b8-4604562e3865",
   "metadata": {},
   "source": [
    "=========================================================================================================================================================="
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
