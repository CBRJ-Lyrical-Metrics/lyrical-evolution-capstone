{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a54c8f7-aa1f-4305-bbf3-62fb05b75fbc",
   "metadata": {},
   "source": [
    "## A LYRICAL EVOLUTION: \n",
    "\n",
    "#### An Investigation of the Cultural Lexicon of U.S. Popular Music from 1958 - Present"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49eb0c4-37d7-45cc-979b-57baaf3d0d2e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c102d20-adbd-44b0-a0fc-0062cacb04eb",
   "metadata": {},
   "source": [
    "**A Time Series Analysis and Natural Language Processing Based Capstone Project Created By:**\n",
    "\n",
    "Jerry Nolf, Rachel Robbins-Mayhill, Ben Smith,  & Chris Teceno                      \n",
    "\n",
    "\n",
    "Codeup   |   Innis Cohort   |   June 2022  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0330cf87-3d96-49e2-93f9-f638738c3096",
   "metadata": {},
   "source": [
    "<img src=\"dataset-cover.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819a8499-ba91-4a83-89c6-67dd43412f15",
   "metadata": {},
   "source": [
    "## Project Goal\n",
    "This project aimed to investigate the patterns of song lyrics across decades by applying Natural Language Processing techniques including Topic Modeling and Sentiment Analysis, while using a Kaggle data set of the Billboard Top 100 Songs from 1958 - Present and lyrics pulled from the Genius.com API. We believe the lyrics of popular songs could be used for historical analysis using exploratory methods and hypothesis testing to identify changing societal trends in relationships, technology, sexuality, and vulgarity. Furthermore, we beleive we can predict the decade the song appeared on the Top 100 using features and machine learning methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae16ef11-eaa7-47c1-a858-84593c0dfa32",
   "metadata": {},
   "source": [
    "## Project Description\n",
    "\n",
    "Songs are powerful tokens: they can soothe, validate, ignite, confront, and educate us – among other things. Like time capsules, they are captured for eternity. The slang and language used are often indicative of the times, and you can probably recall exactly when a song was made based on what is mentioned. Arguably, music is a catalyst for societal and cultural evolution like no other art form. It has been causing controversy and societal upheaval for decades, and it seems with every generation there’s a new musical trend that has the older generations shaking their heads. \n",
    "\n",
    "For centuries, songs have been passed down through generations, being sung as oral histories. However, with advancements of the 20th century, technology has made the world of music a much smaller place and, thanks to cheap, widely-available audio equipment, songs are now distributed on a much larger scale, having a farther-reaching impact, and a more permanent place in history. \n",
    "\n",
    "This project aimed to combine the record of lyrical history and technological advancements to evaluate the changes in the societal lexicon over the last 60+ years. Using machine learning and natural language processing methodologies we investigated the topics prevalent in songs of the past, predicted the decade in which they were written, and conducted historical analysis through exploration to identify changing societal trends in relationships, sexuality, and vulgarity.\n",
    "\n",
    "<img src='Billboard.png' width=\"350\" height=\"350\" align=\"left\"/> To do this, we acquired a [Kaggle](https://www.kaggle.com/datasets/dhruvildave/billboard-the-hot-100-songs) data set of the Billboard Top 100 Songs from its inception in 1958 to present. We then utilized the [Genius.com](https://genius.com/) API and LyricGenius Library to conduct web scraping to pull the lyrics for the specified songs which became the corpus for this project. After acquiring and preparing the corpus, our team conducted time series analysis and natural language processing exploration utilizing methods such as sentiment analysis and topic modeling. We also employed multiclass classification methods to create multiple machine learning models. The end goal was to create an NLP model that accurately predicted the decade a song first appeared on the Billboard Top 100 chart, based on the words and word combinations found in the lyrics of the song.\n",
    "\n",
    "We choose the Billboard Hot 100 song list as a focus because it is the music industry standard record chart in the United States for song popularity, published weekly by Billboard magazine. It provides a window into popular culture at a given time, by providing chart rankings of songs that were trending on sales, airplay, and now streaming for that week in the United States. It is arguably the best historical record of the impact of specific popular songs over time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd55c6df-51d2-412e-adce-1daa9ca98119",
   "metadata": {},
   "source": [
    "## Initial Thoughts & Hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe434ce1-b1f0-4415-8403-23081cd5e590",
   "metadata": {},
   "source": [
    "The initial hypothesis of this project was that we could use the top songs of each decade in conjunction with topic modeling to identify lyric features that would accurately predict the decade a song was on the Billboard Top 100 using machine learning. The thought behind this was that popular songs have been the historians of a unique lexicon, specific to their place in time. We believe the lyrics of popular songs could be analyzed through machine learning to identify societal trends in relationships, sexuality, and vulgarity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c142d940-b385-4db4-b002-1d76ab647b73",
   "metadata": {},
   "source": [
    "## Initial Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ddad9a-cfe1-4f58-8b8b-2b1cc20cd755",
   "metadata": {},
   "source": [
    "The focus of this project is on identifying the decade a song first appeared on the Billboard Top 100. Below are some of the initial questions this project looked to answer throughout the Data Science Pipeline.\n",
    " \n",
    "##### Data-Focused Questions\n",
    "- How does sentiment change over time?\n",
    "- Is there a correlation between historical events and sentiment?\n",
    "- What topics are most prevalent across the decades?\n",
    "- How do topics change over time?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68b0078-9645-445d-9dec-996ac6aa654e",
   "metadata": {},
   "source": [
    "## Key Findings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0373f8-c854-40d9-a4ba-805f656e987e",
   "metadata": {},
   "source": [
    "Through exploratory analysis, we discovered US popular music has undergone a major cultural shift starting in the 90's, where: \n",
    "\n",
    "- overall sentiment decreased \n",
    "- lyrics became more complex \n",
    "- topics shifted towards sex, money, & violence \n",
    "- ‘love’ was replaced with ‘like’\n",
    "\n",
    "Ultimately, our hypothesis that we could use the top songs of each decade to accurately predict the decade a song was on the Billboard Top 100 was true. Although, certain decades were predicted more accurately than others. Our best performing models were based heavily on TF/IDF with the top performing model being a Logistic Regression model performing 220% over baseline.\n",
    "\n",
    "The key findings for this presentation are available in slide format by clicking on the [Final Slide Presentation](https://www.canva.com/design/DAFCXoeG7z0/jNCtQkQFqyOTWS5Ckg8Xuw/view?utm_content=DAFCXoeG7z0&utm_campaign=designshare&utm_medium=link2&utm_source=sharebutton)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff086dd-8e1c-41b6-89a0-7f4795d490e3",
   "metadata": {},
   "source": [
    "=========================================================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f447ae0-d3ba-4e65-9e65-923a31f642cb",
   "metadata": {},
   "source": [
    "## I. ACQUIRE\n",
    "To acquire the data for this project, we utilized a [Kaggle](https://www.kaggle.com/datasets/dhruvildave/billboard-the-hot-100-songs) data set of the Billboard Top 100 Songs from its inception in 1958 to present. \n",
    "\n",
    "The dataset provided:\n",
    "- date song was on the Billboard Top 100 \n",
    "- rank of song  \n",
    "- title\n",
    "- artist name\n",
    "- rank of song the previous week\n",
    "- rank of song at it's peak week\n",
    "- number of weeks song was on the Top 100  \n",
    "\n",
    "We selected only unique artists and songs, to ensure there were no duplicates, keeping only the earliest appearance on the chart to standardize the selections in the event of multiple appearances. Following song selection with the Kaggle dataset, we then obtained an API token to utilize the [Genius.com](https://genius.com/) API and [LyricGenius Library](https://pypi.org/project/lyricsgenius/) to conduct web scraping to pull the lyrics for the specified songs which became the corpus for this project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac72be3-1478-4f30-8653-a08976cab5a6",
   "metadata": {},
   "source": [
    "### Note about imports: \n",
    "Imports for this project are added in the sections in which they are required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a7a8a76-8a52-468b-84d5-13ad0a952e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import for acquisition\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "import final_acquire as acquire\n",
    "import final_prepare as prepare\n",
    "import final_explore as explore\n",
    "import final_model as model\n",
    "\n",
    "\n",
    "# import for data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Dict, List, Optional, Union, cast\n",
    "\n",
    "# import to ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "292b3645-39b5-422e-8cf4-e884a2004574",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'prepped_songs.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/jx/x5_xzwy107g6d0zd021r2ph40000gn/T/ipykernel_11702/4196548759.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# acquire data from .json saved and processed using functions found in wrangle.py\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"prepped_songs.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \"\"\"\n\u001b[0;32m--> 222\u001b[0;31m         self.handles = get_handle(\n\u001b[0m\u001b[1;32m    223\u001b[0m             \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.9/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    700\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    703\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'prepped_songs.csv'"
     ]
    }
   ],
   "source": [
    "# acquire data from .json saved and processed using functions found in wrangle.py\n",
    "df = pd.read_csv(\"prepped_songs.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3880d05-efc6-46ab-972a-8c76a99d3a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain number of columns and rows for original dataframe\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd63d50-ff49-481a-8a6e-ebefdec84ba3",
   "metadata": {},
   "source": [
    "### The Original DataFrame Size: ____ rows, or documents, and ____ columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836ce5ea-d032-4f7d-8c2d-dc6b6375d8b8",
   "metadata": {},
   "source": [
    "=========================================================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0896e47-ee98-4192-9cb0-f79b187d5658",
   "metadata": {},
   "source": [
    "## II. PREPARE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9df564-01cc-463b-96a6-8536629a9e95",
   "metadata": {},
   "source": [
    "After data acquisition, the dataframe was analyzed and cleaned to facilitate functional exploration and clarify variable confusion. The preparation of this data can be replicated using the ______ function saved within the prepare.py file inside the [Lyrical Evolution](https://github.com/CBRJ-Lyrical-Metrics/song-lyrics-capstone) repository on GitHub. The function takes in the original corpus dataframe and returns it with the changes noted below.\n",
    "\n",
    "**Steps Taken to Clean & Prepare Data:**\n",
    "\n",
    "- Basic Cleaning: \n",
    "    - Make all text lowercase\n",
    "    - Normalize, encode, and decode to remove accented text and special characters\n",
    "    - Tokenize strings to break words and punctuation into discrete units\n",
    "    - Lemmatize words to acquire base words\n",
    "    - Remove stopwords\n",
    "    - Rename columns\n",
    "    \n",
    "___\n",
    " \n",
    "- Address missing values, data errors, unnecessary data, and unclear values:\n",
    "    - \n",
    "    - Drop missing values to prevent impediments in exploration and modeling: ______ documents/observations that had null values in the ______ column \n",
    "    - Drop all rows where ________\n",
    "    - Total dropped documents = _______\n",
    "---    \n",
    "- Create feature engineered columns:\n",
    "    - unique words\n",
    "    - character count\n",
    "    - word count\n",
    "    - unique word count\n",
    "    - most common word count (2nd, 3rd, 4th, 5th most common)\n",
    "    - unique bigram count\n",
    "    - count of bigrams unique to each language in train set(this is done by creating a new column for each language)\n",
    "    \n",
    "---\n",
    "- Split corpus into train, validate, and test samples\n",
    "\n",
    "**Note on Missing Value Handling:**\n",
    "The missing value removal equated to removing 9 observations/documents, which was about 9\\% of the data set. It still left a substantial number of observations above the minimum expectation of 100. If given more time with the data, it is recommended to investigate other ways to impute the missing data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937b3c41-a336-4a8c-9777-c13ff07c28ef",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee80dea-08b3-4064-adcf-513ab910b2d5",
   "metadata": {},
   "source": [
    "## Results of Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32c4894-1f7c-4684-8b22-81b119ad2ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import for prepare\n",
    "import final_prepare as prepare\n",
    "import unicodedata\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from time import strftime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c382de0-355d-40e6-9afe-137f3c86888b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the data preparation observations and tasks to clean the data using the prep_data function found in the prepare.py\n",
    "df = prepare.get_data(df)\n",
    "# view first few rows of dataframe\n",
    "# obtain the number of rows and columns for the updated/cleaned dataframe \n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35f06a2-40f7-49ea-b3b2-502d3a90b253",
   "metadata": {},
   "source": [
    "## Prepared DataFrame Size: 134 rows, or documents, and 13 columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad099ea-da58-4d5c-81d7-b391b4449579",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83fdb06a-f042-4b96-8fda-bf90f16f0227",
   "metadata": {},
   "source": [
    "### PREPARE - SPLIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa98112-1a01-4d46-bfe5-70db5d26156e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import for split\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c20745-cd3b-4686-a502-99981315e1d0",
   "metadata": {},
   "source": [
    "After preparing the corpus, it was split into 3 samples; train, validate, and test using:\n",
    "\n",
    "- Random State: ______\n",
    "- Test = 20% of the original dataset\n",
    "- The remaining 80% of the dataset is divided between valiidate and train\n",
    "    - Validate (.30*.80) = 24% of the original dataset\n",
    "    - Train (.70*.80) = 56% of the original dataset\n",
    "    \n",
    "The split of this data can be replicated using the split_data function saved within the prepare.py file inside the [_____](_________) repository on GitHub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687cb248-9647-41c5-9d50-30a4afa44e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into train, validate, and test using the split_data function found in the prepare.py\n",
    "train, validate, test = prepare.split_data(df)\n",
    "# obtain the number of rows and columns for the splits\n",
    "train.shape, validate.shape, test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c0e03e-2952-4eb6-9d44-54a5a04b8bf4",
   "metadata": {},
   "source": [
    "=========================================================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919c78fe-d835-4aa1-ab27-80c23b51faf5",
   "metadata": {},
   "source": [
    "## III. EXPLORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd7468a-7db3-4a6c-ba59-ca948831622c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import for data visualization\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.ticker import StrMethodFormatter\n",
    "from matplotlib import style\n",
    "from wordcloud import WordCloud\n",
    "import final_explore as explore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e368cfc-c8db-4375-9436-087118ad07ca",
   "metadata": {},
   "source": [
    "After acquiring and preparing the corpus, exploration was conducted. All univariate exploration was completed on the entire cleaned corpus in the workbook for this project. For the purpose of the final report, only the target variable will be displayed in order to reduce noise and provide focused context for the project. Following univariate exploration, the split sets (train, validate, and test samples) were utilized thorugh modeling, where only the train set was used for bivariate and multivariate exploration to prevent data leakage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b897a71-03f3-440f-a53c-7c5ca636dc48",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1854228a-7ff2-41dd-9e88-b55feba10cb3",
   "metadata": {},
   "source": [
    "### UNIVARIATE EXPLORATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1980cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep??????"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617865e3-742d-4be4-8f35-25cc1a5d5d8e",
   "metadata": {},
   "source": [
    "#### UNIVARIATE EXPLORATION of TARGET VARIABLE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3e7208-151f-4d23-89dd-c06b8b02a4b6",
   "metadata": {},
   "source": [
    "#### OBSERVATIONS: \n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4497c72-55d0-49eb-be38-154f2edf0a13",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c18e44d-ab8b-4cd3-af33-6388dd07862f",
   "metadata": {},
   "source": [
    "### EXPLORATION QUESTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa631c3e-d91c-4717-b4cc-01f09def4632",
   "metadata": {},
   "source": [
    "All bivariate exploration was conducted on the train corpus to prevent data leakage. The initial questions and univariate exploration guided the bivariate exploration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9502d911-af97-483e-a1d5-349aea49a78b",
   "metadata": {},
   "source": [
    "#### EXPLORE QUESTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888ede95-8e60-4d69-a6b0-a2b317559668",
   "metadata": {},
   "source": [
    "#### QUESTION 1: \n",
    "How has song sentiment changed over time?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9faa39be-a1af-4ba8-89fb-21175c3d9c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "explore.sentiment_lineplot(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb2dc44-bf0e-483a-be7f-48cdf293df70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hypothesis Testing - Mann-Whitney"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbdad8f7",
   "metadata": {},
   "source": [
    "#### ANSWER 1:\n",
    "Sentiment was fairly steady in the 60's and 70's, followed by a gradual downward trend which becomes sharper in the 2000's and 2010's. The downward trend is due to an increase in very negative sentiment and decrease in very positivesentiment while mid-range sentiment stays contstant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae6d3d5",
   "metadata": {},
   "source": [
    "#### Question 2:\n",
    "What topics are most prevalent across the decades?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f22a5b-4bf8-48fe-b613-1031a3ffcb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "explore.topic_popularity(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113ea4db",
   "metadata": {},
   "source": [
    "#### Answer 2:\n",
    "Breakups are by far the most popular topics in songs across all decades, followed by being or feeling lost, then affection, sex, and nature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29ffc02",
   "metadata": {},
   "source": [
    "#### QUESTION 3: \n",
    "- How do relationship topics change over the decades?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d1d42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "explore.relationship_line(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af057b48",
   "metadata": {},
   "source": [
    "Observation:\n",
    "There appears to be an inverse relationship between affection and sex, with the topic of affection decreaseing in prevalence over time, and the topic of sex increasing in usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce0328d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Hyptohesis Testing - Correlation or Kruskal-Wallis?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace94ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "explore.touch_swarm(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17adc54a",
   "metadata": {},
   "source": [
    "#### Answer 3:\n",
    "While most relationship topics appear constant, affection and sex have an inverse relationship."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98012e5",
   "metadata": {},
   "source": [
    "#### QUESTION 4: \n",
    "- How do vice topics change over the decades?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48eeadea",
   "metadata": {},
   "outputs": [],
   "source": [
    "explore.vice_swarm(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd98b9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Hypothesis Testing - "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69707286",
   "metadata": {},
   "source": [
    "#### ANSWER 4: \n",
    "After 1990 sex became extremely popular in lyrics, then around 2015 violence and money exploded as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50827cd0",
   "metadata": {},
   "source": [
    "#### Question 5:\n",
    "How has the prevalence of the word 'like' changed over the decades?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143d676b",
   "metadata": {},
   "outputs": [],
   "source": [
    "explore.love_vs_like_lineplot(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12fac52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hypothesis Testing - "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b3a679",
   "metadata": {},
   "source": [
    "#### ANSWER 5: \n",
    "Love went from most common word in the early decades, to lower in the top 5, then out of the top 5 and replaced with like."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c456be1-eb72-47a7-b705-67edb07d26e4",
   "metadata": {},
   "source": [
    "=========================================================================================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15980c0-808c-448b-9a74-6fb25fff882e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "53b8ebc6-10c5-4182-9d63-0e4d761931d7",
   "metadata": {},
   "source": [
    "## IV. MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4d6649-e624-4553-a888-6d541bc4abed",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "38284848-c794-43c9-8fa2-7c38be8437e5",
   "metadata": {},
   "source": [
    "=========================================================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f62750-5420-47d4-92e5-67b1759ff80d",
   "metadata": {},
   "source": [
    "## V. CONCLUSION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ca57c2-01a1-450b-99b8-4604562e3865",
   "metadata": {},
   "source": [
    "=========================================================================================================================================================="
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
