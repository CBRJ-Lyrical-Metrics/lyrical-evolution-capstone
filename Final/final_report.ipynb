{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a54c8f7-aa1f-4305-bbf3-62fb05b75fbc",
   "metadata": {},
   "source": [
    "## A LYRICAL EVOLUTION: \n",
    "\n",
    "#### An Investigation of the Cultural Lexicon of U.S. Popular Music from 1958 - Present\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c102d20-adbd-44b0-a0fc-0062cacb04eb",
   "metadata": {},
   "source": [
    "By: Jerry Nolf, Rachel Robbins-Mayhill, Ben Smith,  & Chris Teceno    |    Codeup   |   Innis Cohort   |   June 2022  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5316282-edbd-4202-9736-911459ae923f",
   "metadata": {},
   "source": [
    "The findings of this project are available in presentation format by clicking on the [Final Slide Presentation](https://www.canva.com/design/DAFCXoeG7z0/jNCtQkQFqyOTWS5Ckg8Xuw/view?utm_content=DAFCXoeG7z0&utm_campaign=designshare&utm_medium=link2&utm_source=sharebutton)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0330cf87-3d96-49e2-93f9-f638738c3096",
   "metadata": {},
   "source": [
    "<img src=\"dataset-cover.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0907b50f-dc30-4e34-8486-b873be7861be",
   "metadata": {},
   "source": [
    "*** **WARNING**: *** \n",
    "\n",
    "This project contains explicit content in the form of isolated words identified through Topic Modeling as features grouped within a topic. Read with caution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819a8499-ba91-4a83-89c6-67dd43412f15",
   "metadata": {},
   "source": [
    "## Project Goal\n",
    "This project aimed to investigate the patterns of song lyrics across decades by applying Natural Language Processing techniques including Topic Modeling and Sentiment Analysis, while using a Kaggle data set of the Billboard Top 100 Songs from 1958 - Present and lyrics pulled from the Genius.com API. We believe the lyrics of popular songs could be used for historical analysis using exploratory methods and hypothesis testing to identify changing societal trends in relationships, sexuality, and vulgarity. Furthermore, we believe we can predict the decade the song appeared on the Top 100 using features and machine learning methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae16ef11-eaa7-47c1-a858-84593c0dfa32",
   "metadata": {},
   "source": [
    "## Project Description\n",
    "\n",
    "Songs are powerful tokens: they can soothe, validate, ignite, confront, and educate us – among other things. Like time capsules, they are captured for eternity. The slang and language used are often indicative of the times, and you can probably recall exactly when a song was made based on what is mentioned. Arguably, music is a catalyst for societal and cultural evolution like no other art form. It has been causing controversy and societal upheaval for decades, and it seems with every generation there’s a new musical trend that has the older generations shaking their heads. \n",
    "\n",
    "For centuries, songs have been passed down through generations, being sung as oral histories. However, with advancements of the 20th century, technology has made the world of music a much smaller place and, thanks to cheap, widely-available audio equipment, songs are now distributed on a much larger scale, having a farther-reaching impact, and a more permanent place in history. \n",
    "\n",
    "This project aimed to combine the record of lyrical history and technological advancements to evaluate the changes in the societal lexicon over the last 60+ years. Using machine learning and natural language processing methodologies we investigated the topics prevalent in songs of the past, predicted the decade in which they were written, and conducted historical analysis through exploration to identify changing societal trends in relationships, sexuality, and vulgarity.\n",
    "\n",
    "<img src='Billboard.png' width=\"350\" height=\"350\" align=\"left\"/> To do this, we acquired a [Kaggle](https://www.kaggle.com/datasets/dhruvildave/billboard-the-hot-100-songs) data set of the Billboard Top 100 Songs from its inception in 1958 to present. We then utilized the [Genius.com](https://genius.com/) API and LyricGenius Library to conduct web scraping to pull the lyrics for the specified songs which became the corpus for this project. After acquiring and preparing the corpus, our team conducted time series analysis and natural language processing exploration utilizing methods such as sentiment analysis and topic modeling. We also employed multiclass classification methods to create multiple machine learning models. The end goal was to create an NLP model that accurately predicted the decade a song first appeared on the Billboard Top 100 chart, based on the words found in the lyrics of the song.\n",
    "\n",
    "We choose the Billboard Hot 100 song list as a focus because it is the music industry standard record chart in the United States for song popularity, published weekly by Billboard magazine. It provides a window into popular culture at a given time, by providing chart rankings of songs that were trending on sales, airplay, and now streaming for that week in the United States. It is arguably the best historical record of the impact of specific popular songs over time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd55c6df-51d2-412e-adce-1daa9ca98119",
   "metadata": {},
   "source": [
    "## Initial Thoughts & Hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe434ce1-b1f0-4415-8403-23081cd5e590",
   "metadata": {},
   "source": [
    "The initial hypothesis of this project was that we could use the top songs of each decade in conjunction with topic modeling and sentiment analysis to identify lyric features that would accurately predict the decade a song was on the Billboard Top 100 using machine learning. The thought behind this was that popular songs have been the historians of a unique lexicon, specific to their place in time. We believe the lyrics of popular songs could be analyzed through machine learning to identify societal trends in relationships, sexuality, and vulgarity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c142d940-b385-4db4-b002-1d76ab647b73",
   "metadata": {},
   "source": [
    "## Initial Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ddad9a-cfe1-4f58-8b8b-2b1cc20cd755",
   "metadata": {},
   "source": [
    "The focus of this project is on identifying the decade a song first appeared on the Billboard Top 100. Below are some of the initial questions this project looked to answer throughout the Data Science Pipeline.\n",
    " \n",
    "##### Data-Focused Questions\n",
    "- How does sentiment within lyrics change over time?\n",
    "- Is there a correlation between events in history and sentiment of lyrics?\n",
    "- What topics are most prevalent across the decades?\n",
    "- How do topics within lyrics change over time?\n",
    "- Is there a correlation between topics and the time a song was popular?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68b0078-9645-445d-9dec-996ac6aa654e",
   "metadata": {},
   "source": [
    "## Key Findings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0373f8-c854-40d9-a4ba-805f656e987e",
   "metadata": {},
   "source": [
    "Through exploratory analysis, we discovered US popular music has undergone a major cultural shift starting in the 1990's, where: \n",
    "\n",
    "- overall sentiment decreased \n",
    "- lyrics became more complex \n",
    "- topics shifted towards sex, money, & violence \n",
    "- ‘love’ was replaced with ‘like’\n",
    "\n",
    "Ultimately, our hypothesis that we could use the top songs of each decade to accurately predict the decade a song was on the Billboard Top 100 was true. Although, certain decades were predicted more accurately than others. Our best performing models were based heavily on TF/IDF with the top performing model being a Logistic Regression model with an F-1 score that was 220% over baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff086dd-8e1c-41b6-89a0-7f4795d490e3",
   "metadata": {},
   "source": [
    "=========================================================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f447ae0-d3ba-4e65-9e65-923a31f642cb",
   "metadata": {},
   "source": [
    "## I. ACQUIRE\n",
    "To acquire the data for this project, we utilized a [Kaggle](https://www.kaggle.com/datasets/dhruvildave/billboard-the-hot-100-songs) data set of the entire listing of Billboard Top 100 Songs from its inception in 1958 to present. \n",
    "\n",
    "The dataset provided:\n",
    "- date song was on the Billboard Top 100 \n",
    "- rank of song  \n",
    "- title\n",
    "- artist name\n",
    "- rank of song the previous week\n",
    "- rank of song at its peak week\n",
    "- number of weeks song was on the Top 100  \n",
    "\n",
    "The original Kaggle dataset contained more than 300,000 entries. We selected only unique artists and songs, to ensure there were no duplicates, keeping only the earliest appearance on the chart to standardize the selections in the event of multiple appearances. Following song selection with the Kaggle dataset, we then obtained an API token to utilize the [Genius.com](https://genius.com/) API and [LyricGenius Library](https://pypi.org/project/lyricsgenius/) to conduct web scraping and pull the lyrics for the specified songs which became the corpus for this project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac72be3-1478-4f30-8653-a08976cab5a6",
   "metadata": {},
   "source": [
    "### Note about imports: \n",
    "Imports for this project are added in the sections in which they are required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a7a8a76-8a52-468b-84d5-13ad0a952e25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features added ******************\n",
      "transforming cv_fit ****\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/zb/3lg9b5xn3831bhkh23bd5bs00000gn/T/ipykernel_3220/1727540317.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfinal_prepare\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mprepare\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfinal_explore\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexplore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mfinal_model\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# import for data manipulation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/codeup-data-science/song-lyrics-capstone/Final/final_model.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;31m# get the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;31m# remove incomplete decades (1950, 2020)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecade\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1950\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecade\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m2020\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/codeup-data-science/song-lyrics-capstone/Final/final_prepare.py\u001b[0m in \u001b[0;36mget_data\u001b[0;34m(file, use_cache)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_count\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# this is 8 songs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;31m# add topics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_topics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Saving to csv in local directory...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/codeup-data-science/song-lyrics-capstone/Final/final_prepare.py\u001b[0m in \u001b[0;36mget_topics\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m    444\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"topic_name\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"topic\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m     \u001b[0;31m# Drop unnecessary column 'topic'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"topic\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"topics added                        \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/anaconda3/lib/python3.9/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4904\u001b[0m                 \u001b[0mweight\u001b[0m  \u001b[0;36m1.0\u001b[0m     \u001b[0;36m0.8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4905\u001b[0m         \"\"\"\n\u001b[0;32m-> 4906\u001b[0;31m         return super().drop(\n\u001b[0m\u001b[1;32m   4907\u001b[0m             \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4908\u001b[0m             \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/anaconda3/lib/python3.9/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4148\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4149\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4150\u001b[0;31m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4152\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/anaconda3/lib/python3.9/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_drop_axis\u001b[0;34m(self, labels, axis, level, errors)\u001b[0m\n\u001b[1;32m   4184\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4185\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4186\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0maxis_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnew_axis\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4188\u001b[0m         \u001b[0;31m# Case for non-unique axis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[0mkind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParameter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPOSITIONAL_OR_KEYWORD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/anaconda3/lib/python3.9/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mreindex\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   4770\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"axis\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4771\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"labels\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4772\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4773\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4774\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mdeprecate_nonkeyword_arguments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallowed_args\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"self\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"labels\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/anaconda3/lib/python3.9/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mreindex\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   4798\u001b[0m             )\n\u001b[1;32m   4799\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4800\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_consolidate_inplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4801\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4802\u001b[0m         \u001b[0;31m# if all axes that are requested to reindex are equal, then only copy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/anaconda3/lib/python3.9/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_consolidate_inplace\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   5563\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconsolidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5564\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5565\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_protect_consolidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5567\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mfinal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/anaconda3/lib/python3.9/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_protect_consolidate\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m   5551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5552\u001b[0m         \u001b[0mblocks_before\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5553\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5554\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mblocks_before\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5555\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_clear_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/anaconda3/lib/python3.9/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mf\u001b[0;34m()\u001b[0m\n\u001b[1;32m   5561\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5562\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5563\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconsolidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5564\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5565\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_protect_consolidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/anaconda3/lib/python3.9/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mconsolidate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    617\u001b[0m         \u001b[0mbm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_integrity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    618\u001b[0m         \u001b[0mbm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_consolidated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 619\u001b[0;31m         \u001b[0mbm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_consolidate_inplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    620\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/anaconda3/lib/python3.9/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36m_consolidate_inplace\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_consolidate_inplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    623\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_consolidated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 624\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_consolidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    625\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_consolidated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_known_consolidated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/anaconda3/lib/python3.9/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36m_consolidate\u001b[0;34m(blocks)\u001b[0m\n\u001b[1;32m   1972\u001b[0m     \u001b[0mnew_blocks\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mBlock\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1973\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0m_can_consolidate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup_blocks\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrouper\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1974\u001b[0;31m         merged_blocks = _merge_blocks(\n\u001b[0m\u001b[1;32m   1975\u001b[0m             \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup_blocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcan_consolidate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_can_consolidate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1976\u001b[0m         )\n",
      "\u001b[0;32m/opt/homebrew/anaconda3/lib/python3.9/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36m_merge_blocks\u001b[0;34m(blocks, dtype, can_consolidate)\u001b[0m\n\u001b[1;32m   1999\u001b[0m             \u001b[0;31m# Sequence[Union[int, float, complex, str, bytes, generic]],\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2000\u001b[0m             \u001b[0;31m# Sequence[Sequence[Any]], SupportsArray]]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2001\u001b[0;31m             \u001b[0mnew_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mblocks\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2002\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2003\u001b[0m             \u001b[0mbvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mblk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mblk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mblocks\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mvstack\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/anaconda3/lib/python3.9/site-packages/numpy/core/shape_base.py\u001b[0m in \u001b[0;36mvstack\u001b[0;34m(tup)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m         \u001b[0marrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0marrs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# import for acquisition\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "import final_acquire as acquire\n",
    "import final_prepare as prepare\n",
    "import final_explore as explore\n",
    "import final_model as model\n",
    "\n",
    "# import for data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Dict, List, Optional, Union, cast\n",
    "\n",
    "# import to ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292b3645-39b5-422e-8cf4-e884a2004574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# acquire data from .json saved and processed using functions found in wrangle.py\n",
    "df = pd.read_csv(\"songs_0526.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3880d05-efc6-46ab-972a-8c76a99d3a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain number of columns and rows for original dataframe\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd63d50-ff49-481a-8a6e-ebefdec84ba3",
   "metadata": {},
   "source": [
    "#### Original Filtered DataFrame Size: \n",
    "- 23,762 rows, or documents, and 5 columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836ce5ea-d032-4f7d-8c2d-dc6b6375d8b8",
   "metadata": {},
   "source": [
    "=========================================================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0896e47-ee98-4192-9cb0-f79b187d5658",
   "metadata": {},
   "source": [
    "## II. PREPARE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9df564-01cc-463b-96a6-8536629a9e95",
   "metadata": {},
   "source": [
    "After data acquisition, the dataframe was analyzed and cleaned to facilitate functional exploration and clarify variable confusion. The preparation of this data can be replicated using the 'get_data' function saved within the prepare.py file inside the [Lyrical Evolution](https://github.com/CBRJ-Lyrical-Metrics/song-lyrics-capstone) repository on GitHub. The function takes in the original acquire dataframe and returns it with the changes noted below.\n",
    "\n",
    "**Steps Taken to Clean & Prepare Data:**\n",
    "\n",
    "- Cleaning: \n",
    "    - Make all text lowercase\n",
    "    - Normalize, encode, and decode to remove accented text and special characters\n",
    "    - Expand abbreviated contractions\n",
    "    - Lemmatize words to acquire base words\n",
    "    - Remove stopwords\n",
    "    - Convert date to DateTime format\n",
    "    - Remove song part identifiers ('lyrics' 'verse', 'chorus', 'hook', 'embed')\n",
    "    \n",
    "---   \n",
    "- Address missing values, data errors, unnecessary data, and unclear values:\n",
    "    - No null values\n",
    "    - Data Errors : The API returned lyrics that were not the expected song's lyrics \n",
    "        - Manually checking some\n",
    "        Compared title, if they match after cleaning manipulation, \n",
    "---    \n",
    "- Create feature engineered columns:\n",
    "    - Decade \n",
    "    - Chorus Count\n",
    "    - Verse Count\n",
    "    - Verse/Chorus Ratio\n",
    "    - Word Count\n",
    "    - Unique Words per Song\n",
    "    - Unique Words per Decade\n",
    "    - Bigrams\n",
    "    - Trigrams\n",
    "    \n",
    "- Apply Natural Language Processing (NLP Methods:\n",
    "    - Sentiment Analysis\n",
    "    - Topic Modeling\n",
    "    \n",
    "---\n",
    "- Split corpus into train, validate, and test samples \n",
    "\n",
    "\n",
    "**Note on Missing Value Handling:**\n",
    "________@BEN___Do you want to address any of the lyric alignment issues within prepare?_________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7e776a-600d-45a1-9da9-7949bafdfec4",
   "metadata": {},
   "source": [
    "### Sentiment Analysis\n",
    "\n",
    "Natural Language Toolkit (NLTK) was used to prepare the corpus for sentiment analysis. NLTK assigns a score between -1 and +1 to each song based on whether the the sum of words and phrases in the song are considered to be positive or negative.\n",
    "After scores were assigned to each song based upon the lyrical content, sentiment score ranges were divided into 5 categories: very negative, somewhat negative, nuetral, somewhat positive, and very positive. Each song was then labeled with the sentiment category by its corresponding sentiment score in preparation for exploration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7497566e-09c6-4abf-b224-a6ace8fa173b",
   "metadata": {},
   "source": [
    "### Topic Modeling\n",
    "\n",
    "Latent Dirichlet Allocation (or LDA) spearheaded the extraction of topics within the lyrics. This unsupervised machine learning method detected word and phrase patterns. It then clustered groups of words that could best be labeled as a topic. 20 major topics were originally produced, but 3 were overlapping in tone and were therefore manually combined with others, resulting in 17 final topics to explore. These topics will be outlined in more detail through the exploration section of this report."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84533370-3ee7-4b3b-b09b-eaae03b86f22",
   "metadata": {},
   "source": [
    "<img src='final_topics.png' width=\"900\"  align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937b3c41-a336-4a8c-9777-c13ff07c28ef",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee80dea-08b3-4064-adcf-513ab910b2d5",
   "metadata": {},
   "source": [
    "## Results of Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32c4894-1f7c-4684-8b22-81b119ad2ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import for prepare\n",
    "import final_prepare as prepare\n",
    "import unicodedata\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from time import strftime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c382de0-355d-40e6-9afe-137f3c86888b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the data preparation observations and tasks to clean the data using the prep_data function found in the prepare.py\n",
    "df = prepare.get_data()\n",
    "# view first few rows of dataframe\n",
    "# obtain the number of rows and columns for the updated/cleaned dataframe \n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35f06a2-40f7-49ea-b3b2-502d3a90b253",
   "metadata": {},
   "source": [
    "## Prepared DataFrame Size: \n",
    "- 23,762 rows, or documents, and 23 columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c0e03e-2952-4eb6-9d44-54a5a04b8bf4",
   "metadata": {
    "tags": []
   },
   "source": [
    "=========================================================================================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd7468a-7db3-4a6c-ba59-ca948831622c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import for data visualization\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.ticker import StrMethodFormatter\n",
    "from matplotlib import style\n",
    "from wordcloud import WordCloud\n",
    "import final_explore as explore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b897a71-03f3-440f-a53c-7c5ca636dc48",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919c78fe-d835-4aa1-ab27-80c23b51faf5",
   "metadata": {},
   "source": [
    "## III. EXPLORE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e368cfc-c8db-4375-9436-087118ad07ca",
   "metadata": {},
   "source": [
    "After acquiring and preparing the corpus, exploration was conducted. Due to the intent to explore sentiment and topics, exploration was completed on the entire cleaned corpus. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c18e44d-ab8b-4cd3-af33-6388dd07862f",
   "metadata": {},
   "source": [
    "### EXPLORATION QUESTIONS\n",
    "\n",
    "The initial questions for this project revolved around sentiment and topic prevalence and were used as the foundation of exploration.\n",
    "\n",
    "- How does sentiment within lyrics change over time?\n",
    "- What topics are most prevalent across the Billboard Hot 100?\n",
    "- How do topics within lyrics change over time?\n",
    "- What is the relationship between Sex and Affection?\n",
    "\n",
    "\n",
    "- Is there a correlation between events in history and sentiment of lyrics?\n",
    "- Is there a correlation between topics and the time a song was popular?\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888ede95-8e60-4d69-a6b0-a2b317559668",
   "metadata": {},
   "source": [
    "#### QUESTION 1: \n",
    "#### **How does sentiment within lyrics change over time?**\n",
    "\n",
    "To examine the change in average sentiment score over time, we used a rolling 5 year average and average by decade. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9faa39be-a1af-4ba8-89fb-21175c3d9c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create visualization using the sentiment_lineplot function found in final_explore.py \n",
    "explore.sentiment_lineplot(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9da3e5f-8589-4521-ac5b-f0e78436254e",
   "metadata": {},
   "source": [
    "#### ANSWER 1:\n",
    "Sentiment was steady in the 1950's and 1960's, followed by a gradual **downward trend** through the 1970's and 1980's. The downward trend became **more sharp begining in the 1990's**. The 2000's and 2010's saw the most significant drop in sentiment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0bff0b-142a-4381-9283-c4d12421c8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "explore.sentiment_stacked_bar(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0481608d-069a-4f81-80d3-8af4eed50caf",
   "metadata": {},
   "source": [
    "### ????????????????NEED EVIDENCE OF THIS????????????????\n",
    "@BEN **The downward trend is due to an increase in very negative sentiment and decrease in very positive sentiment while mid-range sentiment stays constant.** \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae6d3d5",
   "metadata": {},
   "source": [
    "#### QUESTION 2:\n",
    "#### **What topics are most prevalent across the Billboard Hot 100?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f22a5b-4bf8-48fe-b613-1031a3ffcb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create visualization using the topic_popularity function found in final_explore.py \n",
    "explore.topic_popularity(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113ea4db",
   "metadata": {},
   "source": [
    "#### ANSWER 2:\n",
    "By count, **breakup references are the most frequently occurring topic** within the Billboard Top 100 Song catalog, with the topics of lost, affection, sex, and nature rounding out the top 5 most prevalent topics within the corpus. The 5 least prevalent topics are spanish, holiday, jealousy, heartache, and transcendental.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29ffc02",
   "metadata": {},
   "source": [
    "#### QUESTION 3: \n",
    "#### **How do topics within lyrics change over time?**\n",
    "\n",
    "To answer this question we grouped topics that aligned to others in theme to evaluate changing trends. The first of the groups was labled **Relationship Topics** and the second, **Vice Topics**. We then explored each grouping.\n",
    "\n",
    "<img src='topic_groupings.png' width=\"450\"  align=\"center\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f64cfb8-6b26-4caf-95d9-e0abc847805b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73bc13c-a03c-43ba-b4d4-813703e8aba7",
   "metadata": {},
   "source": [
    "#### Q3A: How do **Relationship Topics** change over the decades?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d1d42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create visualization using the relationship_line function found in final_explore.py \n",
    "explore.relationship_line(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af057b48",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "Within the **Relationship Topics** grouping, breakups, heartache, jealousy, and love stay somewhat consistent in prevalence within the lyrical corpus across the decades. Affection steadily decreases from being the 2nd most prevalent topic for 30 years, from the 1960's until the 1990's. **In the '90's the topic of sex suddenly rises** from the least prevalent to the second most prevalent topic, then overtakes breakups for the most prevalent topic around 2018. \n",
    "\n",
    "**Note**: For the purpose of this project the topic of sex is defined as more forward and explicit encounters with a partner while affection is more sensual encounters such as touching, hugging, or kissing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa30dd7-b44a-4721-951b-8beb73246a94",
   "metadata": {},
   "source": [
    "Noticing the significant changes to the topics of Affection and Sex over time, a closer look was conducted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ceeeadf-df9b-4c09-a6d9-05285b010102",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1404659-7c1b-4c7e-a21a-71a1cce8d51f",
   "metadata": {},
   "source": [
    "#### Q3A: What is the relationship between Sex and Affection?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace94ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create visualization using the touch_swarm function found in final_explore.py \n",
    "explore.touch_swarm(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6ac9c7-3b73-4e57-9d71-26093a8c1ff0",
   "metadata": {},
   "source": [
    "**Observation:**\n",
    "Sex and Affection have an **inverse relationship**, with the **shift occurring in the 1990's**. Affection was consistently represented across the lyrical corpus for nearly 40 years, from the late 1950's through the late '90's, while the topic of Sex exploded in usage in the early '90s. Shortly after the increased prevalence of the Sex topic, the affection topic declined in prevalence. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb97f6bb-eb8c-4ec0-ab18-5133f9cfc78a",
   "metadata": {},
   "source": [
    "#### Q3B: How do **Vice Topics** change over the decades?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17adc54a",
   "metadata": {},
   "source": [
    "#### ANSWER 3:\n",
    "While most relationship topics appear constant, affection and sex have an inverse relationship.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98012e5",
   "metadata": {},
   "source": [
    "#### QUESTION 4: \n",
    "**How do vice topics change over the decades?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48eeadea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create visualization using the vice_swarm function found in final_explore.py \n",
    "explore.vice_swarm(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd98b9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Hypothesis Testing - "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69707286",
   "metadata": {},
   "source": [
    "#### ANSWER 4: \n",
    "After 1990 sex became extremely popular in lyrics, then around 2015 violence and money exploded as well.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50827cd0",
   "metadata": {},
   "source": [
    "#### QUESTION 5:\n",
    "**How has the prevalence of the word 'like' changed over the decades?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143d676b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create visualization using the love_vs_like_lineplot function found in final_explore.py \n",
    "explore.love_vs_like_lineplot(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12fac52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hypothesis Testing - "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b3a679",
   "metadata": {},
   "source": [
    "#### ANSWER 5: \n",
    "Love went from most common word in the early decades, to lower in the top 5, then out of the top 5 and replaced with like."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20e2e5f",
   "metadata": {},
   "source": [
    "#### Question 6: \n",
    "How does sentiment align with historical events?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf3884c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create visualization using the historical_lineplot function found in final_explore.py \n",
    "explore.historical_lineplot(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c532f1b0",
   "metadata": {},
   "source": [
    "#### Answer 6:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6554fdcf",
   "metadata": {},
   "source": [
    "#### QUESTION 7:\n",
    "#### **How did unique word count change over time?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782fcc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create visualization using the unique_words_lineplot function found in final_explore.py \n",
    "explore.unique_words_lineplot(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a95962",
   "metadata": {},
   "source": [
    "#### ANSWER 7:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74491e4-2da9-4f32-a606-c7b1b9dd8f6c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### EXPLORATION SUMMARY\n",
    "Exploration revealed significant changes to sentiment and topic prevalence across the decades, indicating **a cultural shift starting in the 90's where:** \n",
    "- Overall sentiment decreased\n",
    "- Lyrics became more complex\n",
    "- Topics shifted towards Sex, Money & Violence\n",
    "- And ‘Love’ was replaced with ‘Like’\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "This information was utilized going forward into modeling to adjust hyperparameters based upon impactful features, primarily _____________ ___________. ____________.\n",
    "\n",
    "==================================================== Overview of Exploratory Questions & Answers ==================================================== \n",
    "- 1. **How does sentiment within lyrics change over time?**\n",
    "\n",
    "Sentiment was fairly steady in the 1950's and 1960's, followed by a gradual **downward trend** through the 1970's and 1980's. The downward trend became **more sharp begining in the 1990's**. The 2000's and 2010's saw the most significant drop in sentiment. \n",
    "\n",
    "- 2. **What topics are most prevalent across the Billboard Hot 100?**\n",
    "\n",
    "By count, **breakup references are the most frequently occurring topic** within the Billboard Top 100 Song catalog, with the topics of lost, affection, sex, and nature rounding out the top 5 most prevalent topics within the corpus. The 5 least prevalent topics are spanish, holiday, jealousy, heartache, and transcendental.\n",
    "\n",
    "\n",
    "- 3. **How do topics within lyrics change over time?**\n",
    "    - **Q3A: How do **Relationship Topics** change over the decades?**\n",
    "\n",
    "Within the **Relationship Topics** grouping, breakups, heartache, jealousy, and love stay somewhat consistent in prevalence within the lyrical corpus across the decades. Affection steadily decreases from being the 2nd most prevalent topic for 30 years, from the 1960's until the 1990's. **In the '90's the topic of sex suddenly rises** from the least prevalent to the second most prevalent topic, then overtakes breakups for the most prevalent topic around 2018. \n",
    "\n",
    "\n",
    "    -**Q3B: What is the relationship between Sex and Affection?**\n",
    "\n",
    "Sex and Affection have an **inverse relationship**, with the **shift occurring in the 1990's**. Affection was consistently represented across the lyrical corpus for nearly 40 years, from the late 1950's through the late '90's, while the topic of Sex exploded in usage in the early '90s. Shortly after the increased prevalence of the Sex topic, the affection topic declined in prevalence. \n",
    "\n",
    "    - **Q3C: How do **Vice Topics** change over the decades?**\n",
    "\n",
    "\n",
    "- 4. How do topics within lyrics change over time?\n",
    "\n",
    "\n",
    "- 5. Is there a correlation between topics and the time a song was popular?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c456be1-eb72-47a7-b705-67edb07d26e4",
   "metadata": {},
   "source": [
    "=========================================================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b8ebc6-10c5-4182-9d63-0e4d761931d7",
   "metadata": {},
   "source": [
    "## IV. MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83fdb06a-f042-4b96-8fda-bf90f16f0227",
   "metadata": {},
   "source": [
    "### DATA SPLIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa98112-1a01-4d46-bfe5-70db5d26156e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import for split\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c20745-cd3b-4686-a502-99981315e1d0",
   "metadata": {},
   "source": [
    "Following exploration, the data was split into train, validate, and test samples using:\n",
    "\n",
    "- Random State: 42\n",
    "- Test = 20% of the original dataset\n",
    "- The remaining 80% of the dataset is divided between valiidate and train\n",
    "    - Validate (.30*.80) = 24% of the original dataset\n",
    "    - Train (.70*.80) = 56% of the original dataset\n",
    "    \n",
    "The split of this data can be replicated using the splitdata function saved within the prepare.py file inside the [Lyrical Evolution](https://github.com/CBRJ-Lyrical-Metrics/song-lyrics-capstone) repository on GitHub. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687cb248-9647-41c5-9d50-30a4afa44e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into train, validate, and test using the split_data function found in the prepare.py\n",
    "train, validate, test = prepare.split_data(df)\n",
    "# obtain the number of rows and columns for the splits\n",
    "train.shape, validate.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02fd964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF/IDF import\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4d6649-e624-4553-a888-6d541bc4abed",
   "metadata": {},
   "source": [
    "### Focus of Model Metrics\n",
    "The target variable, Decade, is a categorical variable, therefore classification machine learning algorithms were used to fit to the training corpus and the models were evaluated on the validate corpus. The metrics used for model evaluation was accuracy, due to the multi-class classification approach. In other words, the model was optimized for identifying true positives, false positive, true negatives, and false negatives, therefore we focused on creating a model with the highest accuracy score from train to validate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121521ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the data\n",
    "df = prepare.get_data()\n",
    "# remove incomplete decades (1950, 2020)\n",
    "df = df[(df.decade != 1950) & (df.decade != 2020)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca5a63a-4615-42df-adb8-657de6c546fe",
   "metadata": {},
   "source": [
    "### Set X & y\n",
    "As mentioned above, two different approaches were taken to prepare the data for modeling. Feature engineering was done for exploratory analysis and even more for modeling. This however did not result in a significant improvement in the accuracy of the model. Therefore, the data was prepared for modeling by using TF-IDF vectorization which takes into account the word count in each song's lyrics vs word count in the entire corpus. Below is how this was performed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017b59b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make vectorizer\n",
    "tfidf = TfidfVectorizer()\n",
    "X = tfidf.fit_transform(df[\"lyrics\"])\n",
    "y = df[\"decade\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2447d7-1416-40b0-8640-d055c4149517",
   "metadata": {},
   "source": [
    "### Set Baseline\n",
    "A baseline prediction was set by using the mode for decade. This gave us a baseline accuracy of 20.6%. We will evaluate the accuracy of our models in comparrison to that baseline.\n",
    "### Condsider Feature Engineering\n",
    "First lets look at the models with the lower accuracy, this is the df using feature engineering not including TF-IDF. \n",
    "The following were adjustable:\n",
    "- scale or not scale\n",
    "- use only unique bigrams as features or use all numeric features\n",
    "### Observation of models with feature engineering:\n",
    "### Consider TF-IDF\n",
    "#### The Type of Classification models built were \n",
    "- Decision Tree\n",
    "- Random Forest\n",
    "- Logistic Regression\n",
    "\n",
    "The models were run with many trials, adjusting parameters and algorithms to find the best performing model.  \n",
    "\n",
    "- All Logistic Regression models appeared to be overfit based upon their high performance on train accuracy compared to the significant drop off on validate accuracy.\n",
    "    - This is in part due to the use of TF-IDF which analyzes each word in the train corpus and does not remove attributes.\n",
    "- In general all models outperformed baseline, which had  ___ accuracy on train and ___ accuracy on validate.\n",
    "- The Logistic Regression Model that performed best had a c of 1000 and solver of 'lbfgs', with train accuracy of 98% and validate accuracy of 61% performing 19% better than baseline with validate. It was then applied to the un-seen test data with an accuracy of 56%.\n",
    "---\n",
    "### MODEL - DECISION TREE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfe81b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncommit to run the following code\n",
    "# results = model.run_decision_tree_models(df)\n",
    "# results.drop(columns='test_accuracy').head(1) # show baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05f4dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncommit to run the following code\n",
    "# results.drop(columns='test_accuracy').sort_values('validate_accuracy', ascending=False).head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60988f74",
   "metadata": {},
   "source": [
    "The Decision Tree model that performed the best on train & validate set had max_depth of 10, with 41% accuracy on train, and 31% accuracy on validate, so that model will be isolated below in the event it is the best performing model to be applied to the test (unseen) dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd5e113",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270052c1-bab2-47e8-b59a-5b71a6a86f7e",
   "metadata": {},
   "source": [
    "### Model - RANDOM FOREST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f34d609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncommit to run the following code\n",
    "# results2 = model.run_random_forest_models(df)\n",
    "# results2.drop(columns='test_accuracy').sort_values('validate_accuracy', ascending=False).head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10eab43c-070b-4722-aa80-f2e2cf34e497",
   "metadata": {},
   "source": [
    "The Random Forest model that performed the best on train & validate set had max_depth of 100 and min_sample_leaf of 2, with 93%  accuracy on train, and 40% accuracy on validate, so that model will be isolated below in the event it is the best performing model to be applied to the test (unseen) dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6066b3b-404f-4820-ae54-77ecd1370c61",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d8c6d5-67c9-4131-86ac-7c232895b562",
   "metadata": {},
   "source": [
    "### Model - LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678c6e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncommit to run the following code\n",
    "# results3 = model.run_logistic_reg_models(df)\n",
    "# results3.drop(columns='test_accuracy').sort_values('validate_accuracy', ascending=False).head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637f18e5",
   "metadata": {},
   "source": [
    "Evaluating the model with the validate data set was done in the function above for comparrison. The Logistic Regression Model that performed best had a c-statistic of 1000 with a train accuracy of 69% and validate accuracy of 45% performing 222% better than baseline on unseen (validate) data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725496a4",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fedcf9d2",
   "metadata": {},
   "source": [
    "### Best Performing Model Applied to Test Data (Unseen Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4460fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncommit to run the following code\n",
    "# results3.sort_values('validate_accuracy', ascending=False).head(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38284848-c794-43c9-8fa2-7c38be8437e5",
   "metadata": {},
   "source": [
    "=========================================================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f62750-5420-47d4-92e5-67b1759ff80d",
   "metadata": {},
   "source": [
    "## V. CONCLUSION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d09473c",
   "metadata": {},
   "source": [
    "This project aimed to investigate the patterns of Billboard Top 100 song lyrics across decades using Time Series Analysis and Natural Language Processing techniques including Topic Modeling and Sentiment Analysis.\n",
    "\n",
    "Through exploration we identified **a cultural shift starting in the 90's where:** \n",
    "- Overall sentiment decreased\n",
    "- Lyrics became more complex\n",
    "- Topics shifted towards sex, money & violence\n",
    "- And ‘Love’ was replaced with ‘Like’\n",
    "\n",
    "Through modeling we were able to confirm our hypothesis that we could use the lyrics of the top songs of each decade to accurately predict the decade a song was on the Billboard Top 100. Although, certain decades were predicted more accurately than others. Our best performing models were based heavily on TF/IDF with the top performing model being a Logistic Regression model with an F-1 score that was 220% over baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3df586d",
   "metadata": {},
   "source": [
    "### RECOMMENDATIONS\n",
    "\n",
    "There are many avenues of application for both the findings and the Machine Learning processes of this project. Some of the major applications being in the areas of Journalism, Business, Academics, and Politics.\n",
    "\n",
    "**Journalism**\n",
    "- Reporting on changing cultural trends within lyrics.\n",
    "- Music production reporting tracking changes in lyrical content for further content creation.\n",
    "\n",
    "**Business**\n",
    "- Marketing Analysis for companies associated with the music industry.\n",
    "- Developing inclusive HR policies in business that adapt to changing popular societal trends.\n",
    "- Predicting changes to the general market based upon sentiment brought on by prominent events.\n",
    "\n",
    "\n",
    "**Academics**\n",
    "- Anthropologic and Sociologic academic analysis.\n",
    "- Using the NLP modeling process to predict the date of historical texts. \n",
    "- Researching the sentiment of social media posts over time and evaluating the impact of generalized sentiment.\n",
    "\n",
    "\n",
    "**Politics**\n",
    "- Applying data insights for use in political agendas or legislation for or against identified societal trends. \n",
    "- Making better informed decisions related to censorship. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0905b7",
   "metadata": {},
   "source": [
    "### NEXT STEPS\n",
    "If given more time, we would like to:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ca57c2-01a1-450b-99b8-4604562e3865",
   "metadata": {},
   "source": [
    "=========================================================================================================================================================="
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3067ead486e059ec00ffe7555bdb889e6e264a24dc711bf108106cc7baee8d5d"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
